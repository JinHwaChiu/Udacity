{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks in Keras\n",
    "Luckily, every time we need to use a neural network, we won't need to code the activation function, gradient descent, etc. There are lots of packages for this, which we recommend you to check out, including the following:\n",
    "\n",
    "- Keras\n",
    "- TensorFlow\n",
    "- Caffe\n",
    "- Theano\n",
    "- Scikit-learn\n",
    "\n",
    "In this course, we will learn Keras. Keras makes coding deep neural networks simpler. To demonstrate just how easy it is, you're going to build a simple fully-connected network in a few dozen lines of code.\n",
    "\n",
    "We’ll be connecting the concepts that you’ve learned in the previous lessons to the methods that Keras provides.\n",
    "\n",
    "The general idea for this example is that you'll first load the data, then define the network, and then finally train the network.\n",
    "\n",
    "Building a Neural Network in Keras\n",
    "Here are some core concepts you need to know for working with Keras.\n",
    "\n",
    "Sequential Model\n",
    "```python\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    Create the Sequential model\n",
    "    model = Sequential()\n",
    "```\n",
    "\n",
    "The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like compile(), fit(), and evaluate() that are used to train and run the model. We'll cover these functions soon, but first let's start looking at the layers of the model.\n",
    "\n",
    "Layers\n",
    "The Keras Layer class provides a common interface for a variety of standard neural network layers. There are fully connected layers, max pool layers, activation layers, and more. You can add a layer to a model using the model's add() method. For example, a simple model with a single hidden layer might look like this:\n",
    "\n",
    "```python\n",
    "\n",
    "    import numpy as np\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Activation\n",
    "    # X has shape (num_rows, num_cols), where the training data are stored\n",
    "    # as row vectors\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "    # y must have an output vector for each input vector\n",
    "    y = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "    # Create the Sequential model\n",
    "    model = Sequential()\n",
    "    # 1st Layer - Add an input layer of 32 nodes with the same input shape as\n",
    "    # the training samples in X\n",
    "    model.add(Dense(32, input_dim=X.shape[1]))\n",
    "    # Add a softmax activation layer\n",
    "    model.add(Activation('softmax'))\n",
    "    # 2nd Layer - Add a fully connected output layer\n",
    "    model.add(Dense(1))\n",
    "    # Add a sigmoid activation layer\n",
    "    model.add(Activation('sigmoid'))\n",
    "   \n",
    "```\n",
    "\n",
    "Keras requires the input shape to be specified in the first layer, but it will automatically infer the shape of all other layers. This means you only have to explicitly set the input dimensions for the first layer.\n",
    "\n",
    "The first (hidden) layer from above, **model.add(Dense(32, input_dim=X.shape[1]))**, creates 32 nodes which each expect to receive 2-element vectors as inputs. Each layer takes the outputs from the previous layer as inputs and pipes through to the next layer. This chain of passing output to the next layer continues until the last layer, which is the output of the model. We can see that the output has dimension 1.\n",
    "\n",
    "The activation \"layers\" in Keras are equivalent to specifying an activation function in the Dense layers **(e.g., model.add(Dense(128)); model.add(Activation('softmax')) is computationally equivalent to model.add(Dense(128, activation=\"softmax\")))),**\n",
    "\n",
    "but it is common to explicitly separate the activation layers because it allows direct access to the outputs of each layer before the activation is applied (which is useful in some model architectures).\n",
    "\n",
    "Once we have our model built, we need to compile it before it can be run. \n",
    "\n",
    "Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data.\n",
    "\n",
    "We'll specify the loss function to be binary_crossentropy which can be used when there are only two classes, and specify adam as the optimizer (which is a reasonable default when speed is a priority).\n",
    "\n",
    "And finally, we can specify what metrics we want to evaluate the model with. Here we'll use accuracy.\n",
    "\n",
    "> model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "We can see the resulting model architecture with the following command:\n",
    "\n",
    "> model.summary()\n",
    "\n",
    "The model is trained with the fit() method, through the following command that specifies the number of training epochs and the message level (how much information we want displayed on the screen during training).\n",
    "\n",
    "model.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "Note: In Keras 1, nb_epoch sets the number of epochs, but in Keras 2 this changes to the keyword epochs.\n",
    "\n",
    "Finally, we can use the following command to evaluate the model:\n",
    "\n",
    "> model.evaluate()\n",
    "\n",
    "Pretty simple, right? Let's put it into practice.\n",
    "\n",
    "##### Quiz:\n",
    "\n",
    "Let's start with the simplest example. In this quiz you will build a simple multi-layer feedforward neural network to solve the XOR problem.\n",
    "\n",
    "1. Set the first layer to a Dense() layer with an output width of 8 nodes and the input_dim set to the size of the training samples (in this case 2).\n",
    "2. Add a tanh activation function.\n",
    "3. Set the output layer width to 1, since the output has only two classes. (We can use 0 for one class an 1 for the other)\n",
    "4. Use a sigmoid activation function after the output layer.\n",
    "5. Run the model for 50 epochs.\n",
    "\n",
    "This should give you an accuracy of 50%. That's ok, but certainly not great. Out of 4 input points, we're correctly classifying only 2 of them. Let's try to change some parameters around to improve. For example, you can increase the number of epochs. You'll pass this quiz if you get 75% accuracy. Can you reach 100%?\n",
    "\n",
    "To get started, review the Keras documentation about models and layers. The Keras example of a **<a href=https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py>Multi-Layer Perceptron network </a>**  is similar to what you need to do here. Use that as a guide, but keep in mind that there will be a number of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 162\n",
      "Trainable params: 162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MLUSER\\AppData\\Local\\conda\\conda\\envs\\python_tensorflow\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s\n",
      "\n",
      "Accuracy:  0.75\n",
      "\n",
      "Predictions:\n",
      "4/4 [==============================] - 0s\n",
      "[[  5.55743694e-01   2.03497093e-02]\n",
      " [  9.99901891e-01   8.94698083e-01]\n",
      " [  9.55545911e-05   2.01363675e-03]\n",
      " [  5.42226493e-01   3.91011089e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Using TensorFlow 1.0.0; use tf.python_io in later versions\n",
    "tf.python_io = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]],dtype='float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# One-hot encoding the output\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "xor.add(Dense(32, input_dim=2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "xor.add(Dense(2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "xor.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11313152/11490434 [============================>.] - ETA: 0s60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 33s - loss: 0.2528 - acc: 0.9220 - val_loss: 0.1242 - val_acc: 0.9611\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 32s - loss: 0.1045 - acc: 0.9682 - val_loss: 0.0893 - val_acc: 0.9705\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 35s - loss: 0.0776 - acc: 0.9775 - val_loss: 0.0823 - val_acc: 0.9761\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 26s - loss: 0.0623 - acc: 0.9815 - val_loss: 0.0719 - val_acc: 0.9807\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 26s - loss: 0.0520 - acc: 0.9847 - val_loss: 0.0793 - val_acc: 0.9792\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 30s - loss: 0.0441 - acc: 0.9867 - val_loss: 0.0811 - val_acc: 0.9814\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 30s - loss: 0.0393 - acc: 0.9885 - val_loss: 0.0768 - val_acc: 0.9821\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 29s - loss: 0.0356 - acc: 0.9894 - val_loss: 0.0872 - val_acc: 0.9812\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 30s - loss: 0.0342 - acc: 0.9904 - val_loss: 0.0725 - val_acc: 0.9837\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 28s - loss: 0.0299 - acc: 0.9914 - val_loss: 0.0819 - val_acc: 0.9843\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 28s - loss: 0.0269 - acc: 0.9922 - val_loss: 0.0950 - val_acc: 0.9817\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 27s - loss: 0.0263 - acc: 0.9927 - val_loss: 0.1031 - val_acc: 0.9829\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 27s - loss: 0.0273 - acc: 0.9924 - val_loss: 0.1095 - val_acc: 0.9800\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 26s - loss: 0.0227 - acc: 0.9934 - val_loss: 0.1080 - val_acc: 0.9829\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 27s - loss: 0.0225 - acc: 0.9939 - val_loss: 0.0939 - val_acc: 0.9848\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 26s - loss: 0.0224 - acc: 0.9942 - val_loss: 0.1046 - val_acc: 0.9841\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 27s - loss: 0.0204 - acc: 0.9944 - val_loss: 0.1047 - val_acc: 0.9835\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 27s - loss: 0.0203 - acc: 0.9951 - val_loss: 0.1088 - val_acc: 0.9843\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 29s - loss: 0.0219 - acc: 0.9947 - val_loss: 0.1053 - val_acc: 0.9832\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 29s - loss: 0.0195 - acc: 0.9951 - val_loss: 0.1049 - val_acc: 0.9833\n",
      "Test loss: 0.104871679578\n",
      "Test accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
