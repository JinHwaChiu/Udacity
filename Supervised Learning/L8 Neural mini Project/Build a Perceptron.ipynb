{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will add in code that decides whether\n",
    "#a perceptron will fire based\n",
    "# on the threshold. Your code will go in lines 32 and 34. \n",
    "#\n",
    "# ----------\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # The strength with which the perceptron fires.\n",
    "        strength = np.dot(self.weights, inputs)\n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "        if strength <= self.threshold :\n",
    "            self.result = 0\n",
    "        else:\n",
    "            self.result = 1\n",
    "        return self.result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here, and the rest of the mini-project, that signal strength equal to the threshold results in a 0 being output (rather than 1).\n",
    "\n",
    "It is required that the dot product be strictly greater than the threshold, rather than greater than or equal to the threshold, to pass the assertion tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Threshold Meditation\n",
    "> The main advantage of having a threshold be set to a perceptron is being able control when a perceptron should fire and when it shouldn't. This gives us control on the sensitivity of our neurons thereby helping us influence the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Neural networks are built out of components like perceptron units. what do inputs to networks of perceptron look like ?\n",
    "> A matrix of numerical values with classification for each row \n",
    "- A single perceptron is very much like linear regression. Therefore it should take the same kinds of inputs. However __the outputs of perceptrons will generally be classifications, not numerical__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: What information can we get as the output of a neural network ?\n",
    "\n",
    "- A directed graph, the neural network itself\n",
    "- A single scalar valued number\n",
    "- The classification\n",
    "- A vector valued output for any vector input\n",
    "\n",
    "\n",
    "In general, neural nets are much more flexible than thresholded perceptron networks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "#\n",
    "# YOUR CODE WILL GO IN LINES 51 AND 59.\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights.astype(float) \n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # For each data point:\n",
    "        for data_point in xrange(len(values)):\n",
    "            # TODO: Obtain the neuron's prediction for the data_point --> values[data_point]\n",
    "            prediction = self.activate(values[data_point])\n",
    "            # Get the prediction accuracy calculated as \n",
    "            # (expected value - predicted value)\n",
    "            # expected value = train[data_point], \n",
    "            # predicted value = prediction\n",
    "            \n",
    "            error = train[data_point] - prediction\n",
    "            # TODO: update self.weights based on the multiplication of:\n",
    "            # - prediction accuracy(error)\n",
    "            # - learning rate(eta)\n",
    "            # - input value(values[data_point])\n",
    "            \n",
    "            weight_update = eta * error * values[data_point]\n",
    "            self.weights += weight_update\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25\n"
     ]
    }
   ],
   "source": [
    "#weights of hidden layer\n",
    "w1 = np.array([1,1,-5])\n",
    "w2 = np.array([3,-4,2])\n",
    "#weights of output layer\n",
    "w3 = np.array([2,-1])\n",
    "#inputs\n",
    "inputs = np.array([1,2,3])\n",
    "hidden_layer_input = np.array([np.dot(w1,inputs),np.dot(w2,inputs)])\n",
    "output =  np.dot(hidden_layer_input,w3)\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-enterprise.discourse.org/udacity/uploads/default/original/3X/9/4/942a1b4ed344d005e2ba810380f049e8cfdf23fb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Representational Power\n",
    "![](https://lh3.googleusercontent.com/T0jgfmNN7gajZIPRV9ldJ-aQ0eTcBtRmMsViTotZCpitg6sb8eXt5YG_zsOsZBiuA6CbXDkPKAvmB_Il6bMV=s0#w=960&h=540)\n",
    "![](https://cdn-enterprise.discourse.org/udacity/uploads/default/original/4X/a/d/6/ad6110ecac8bb940630a0348a4d97a690e958514.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, -1]\n"
     ]
    }
   ],
   "source": [
    "#[[input,input],[[3,2],[-1,4],[3,-5]],[[1,2,-1]]]\n",
    "\n",
    "x = np.array([[3,2],[-1,4],[3,-5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function Quiz\n",
    "> 3\n",
    "![](https://lh3.googleusercontent.com/q0oy8Ws2HrLXTJFcmtZ6PpYF7eBq0zIHWd6kehIdyIeFAgM63W_r7DL47lW6MidFrddyX9y4qD4ABwFZNN4=s0#w=960&h=540)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Vs Sigmoid\n",
    "> 2\n",
    "![](https://lh3.googleusercontent.com/YI3V5BzcavIPkjTX1FtrCXGTmkexF6jK7uE_UGQFIf_4Jeh9xQpx_nHUItXxvN1ovhsXLmlLb7N6cYCDqw=s0#w=960&h=540)\n",
    "\n",
    "Sigmoid networks can be different in many ways, but single units will end up classifying just like perceptrons in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Learning\n",
    "> 4\n",
    "![](https://lh3.googleusercontent.com/722t2viFgMvj6R1p_EOU65VO-G-I1Ut0BrkfrmZmEzfq0HsNiteTvCyBFgnyoGfbYlzY-EYEq4i7s-e2zgo=s0#w=960&h=540)\n",
    "\n",
    "\n",
    "Right! We want to deal with small, gradual changes of continuous functions. This is exactly where we should use calculus!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Issues\n",
    "> 1,2,3,4\n",
    "![](https://lh3.googleusercontent.com/XZ6e_B5Ak-wZR3O-q1C-LjKdNbKLxiJE7GRbMcnoNAqICx8pMH6HX5Pwbe7S8apbAyJijqkyjH0TPnP_1A=s0#w=960&h=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
