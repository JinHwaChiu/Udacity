{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models in scikit learn\n",
    "--\n",
    "In this section, we'll still be working with the dataset of the previous sections.\n",
    "\n",
    "![Alt text](https://d17h27t6h515a5.cloudfront.net/topher/2017/June/594c6b79_points/points.png)\n",
    "\n",
    "In the last section, we learned most of the most important classification algorithms in Machine Learning, including the following:\n",
    "\n",
    "- Logistic Regression\n",
    "- Neural Networks\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "\n",
    "Now, we'll have the chance to use them in real data! In sklearn, this is very easy, all we do is define our classifier, and then use the following line to fit the classifier to the data (which we call X, y):\n",
    "\n",
    "```python\n",
    "classifier.fit(X,y)\n",
    "```\n",
    "Here are the main classifiers we define, together with the package we must import:\n",
    "\n",
    "##### Logistic Regression\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "```\n",
    "\n",
    "###### Neural Networks\n",
    "(note: This is only available on versions 0.18 or higher of scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier = MLPClassifier()\n",
    "```\n",
    "##### Decision Trees\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier = GradientBoostingClassifier()\n",
    "```\n",
    "\n",
    "##### Support Vector Machines\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC()\n",
    "```\n",
    "\n",
    "##### Example: Logistic Regression\n",
    "Let's do an end-to-end example on how to read data and train our classifier. Let's say we carry our X and y from the previous section. Then, the following commands will train the Logistic Regression classifier:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X,y)\n",
    "This gives us the following boundary:\n",
    "```\n",
    "\n",
    "![Alt text](https://d17h27t6h515a5.cloudfront.net/topher/2017/June/594c0430_linear-boundary/linear-boundary.png)\n",
    "\n",
    "##### Quiz: Train your own model\n",
    "\n",
    "![Alt text](https://d17h27t6h515a5.cloudfront.net/topher/2017/June/594c04d8_circle-data/circle-data.png)\n",
    "\n",
    "Now, it's your turn to shine! In the quiz below, we'll work with the following dataset:\n",
    "\n",
    "Your goal is to use one of the classifiers above, between Logistic Regression, Decision Trees, or Support Vector Machines (sorry, Neural Networks are still not available in this version of sklearn, but we will be upgrading soon!), to see which one will fit the data better. Click on Test Run to see the graphical output of your classifier, and in the quiz underneath this, enter the classifier that you think fit the data better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Parameters\n",
    "So it looks like 2 out of 3 algorithms worked well last time, right? This are the graphs you probably got:\n",
    "\n",
    "![Alt text](https://d17h27t6h515a5.cloudfront.net/topher/2017/June/594d5e2e_curves/curves.png)\n",
    "\n",
    "It seems that Logistic Regression didn't do so well, as it's a linear algorithm. Decision Trees managed to bound the data well (question: Why does the area bounded by a decision tree look like that?), and the SVM also did pretty well. Now, let's try a slightly harder dataset, as follows:\n",
    "\n",
    "![Alt text](https://d17h27t6h515a5.cloudfront.net/topher/2017/June/594d5ffe_eggsdata/eggsdata.png)\n",
    "\n",
    "Let's try to fit this data with an SVM Classifier, as follows:\n",
    "```python \n",
    " classifier = SVC()\n",
    " classifier.fit(X,y)\n",
    " ```\n",
    "If we do this, it will fail (you'll have the chance to try below). However, it seems that maybe we're not exploring all the power of an SVM Classifier. For starters, are we using the right kernel? We can use, for example, a polynomial kernel of degree 2, as follows:\n",
    "\n",
    "```python \n",
    "classifier = SVC(kernel = 'poly', degree = 2)\n",
    "```\n",
    "\n",
    "Let's try it ourselves, let's play with some of these parameters. We'll learn more about these later, but here are some values you can play with (These are parameters that we'll learn later. For now, we can use them as a black box.):\n",
    "\n",
    "- kernel: linear, poly, rbf.\n",
    "- degree (integer): This is the degree of the polynomial kernel, if that's the kernel you picked.\n",
    "- gamma (integer): The gamma parameter.\n",
    "- C (integer): The C parameter.\n",
    "In the quiz below, you can play with these parameters. Try to tune them in such a way that they bound the desired area!\n",
    "\n",
    "Note: The quiz is not graded, but if you want to see a solution that works, hit Submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
