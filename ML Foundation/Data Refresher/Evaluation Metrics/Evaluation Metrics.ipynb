{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "In this lesson we'll look at a small selection of common performance metrics and evaluate some algorithms with them on the Titanic dataset you used earlier.\n",
    "\n",
    "There are a few important things to keep in mind here:\n",
    "\n",
    "There is a big difference in performance based on whether a train/test split is used.\n",
    "In general, performance on all metrics is correlated. But some algorithms may end up doing better or worse in different situations.\n",
    "The practical coding of any metric looks almost exactly the same! The difficulty comes in how to make the choice, not in how to implement it.\n",
    "The topics covered in this lesson are:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Confusion Matrix\n",
    "- F1 score\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "If you are familiar with these concepts you can skip ahead, but we do recommend completing this lesson as a refresher nonetheless.\n",
    "\n",
    "#### Classification and Regression\n",
    "__Classification__ is about deciding __which categories__ new instances belong to. For example we can organize objects based on whether they are square or round, or we might have data about different passengers on the Titanic like in project 0, and want to know whether or not each passenger survived. Then when we see new objects we can use their features to guess which class they belong to.\n",
    "\n",
    "In __regression__, we want to make __a prediction on continuous data__. For example we might have a list of different people's height, age, and gender and wish to predict their weight. Or perhaps, like in the final project of this course, we have some housing data and wish to make a prediction about the value of a single home.\n",
    "\n",
    "The problem at hand will determine how we choose to evaluate a model.\n",
    "\n",
    "#### Classification Metrics\n",
    "For classification we are dealing with models that make discrete predictions.\n",
    "\n",
    "That is to say these models decide which of a given set of categories a certain data point belongs to. Using a set of data kept for testing, we can measure on this testing set which points were accurately classified, and which were not.\n",
    "\n",
    "Popular classification metrics are:\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "##### Recall: (真實)\n",
    "   - True Positive / (True Positive + False Negative). Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.\n",
    "   \n",
    "##### Precision:(精準)\n",
    "   - True Positive / (True Positive + False Positive). Out of all the items labeled as positive, how many truly belong to the positive class.- Confusion Matrix\n",
    "   \n",
    "##### F1 score\n",
    "   - Now that you've seen precision and recall, another metric you might consider using is the F1 score. F1 score combines precision and recall relative to a specific positive class. The F1 score can be interpreted as a weighted average of the precision and recall, where an __F1 score reaches its best value at 1 and worst at 0:__\n",
    "```python\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "```\n",
    "\n",
    "For more information about F1 score how to use it in sklearn, check out the documentation here.(http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "\n",
    "##### Accuracy\n",
    "The most basic and common classification metric is accuracy. Accuracy here is described as the proportion of items classified or labeled correctly.\n",
    "\n",
    "For instance if a classroom has 14 boys and 16 girls, can a facial recognition software correctly identify all boys and all girls? If the software can identify 10 boys and 8 girls, then the software is 60% accurate.\n",
    "\n",
    "- accuracy = number of correctly identified instances / all instances\n",
    "\n",
    "Accuracy is the default metric used in the .score() method for classifiers in sklearn. You can read more in the documentation here.\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Metrics overview\n",
    "As mentioned earlier, for regression problems we are dealing with a model that makes continuous predictions. In this case we care about how close the prediction is.\n",
    "\n",
    "For example, with height or weight predictions it is unreasonable to expect a model to 100% accurately predict someone's weight down to a fraction of a pound! But we do care how consistently the model can make a close prediction — perhaps within 3 to 4 pounds.\n",
    "\n",
    "We'll discuss a few measurements of accuracy for regression tasks. Since the Titanic dataset is a classification task, we'll try out another one of sklearn's pre-loaded datasets, the diabetes dataset, for the rest of the lesson.\n",
    "\n",
    "##### Popular regression metrics are:\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "- R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error\n",
    "One way to measure error is by using absolute error to find the predicted distance from the true value. The mean absolute error takes the total absolute error of each example and averages the error based on the number of data points. By adding up all the absolute values of errors of a model we can avoid canceling out errors from being too high or below the true values and get an overall error metric to evaluate the model on.\n",
    "\n",
    "For more information about mean absolute error and how to use it in sklearn, check out the documentation here.(http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)[source]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Squared Error\n",
    "\n",
    "Mean squared is the most common metric to measure model performance. In contrast with absolute error, the residual error (the difference between predicted and the true value) is squared.\n",
    "\n",
    "Some benefits of squaring the residual error is that error terms are positive, it emphasizes larger errors over smaller errors, and is differentiable. Being differentiable allows us to use calculus to find minimum or maximum values, often resulting in being more computationally efficient.\n",
    "\n",
    "For more information about mean squared error and how to use it in sklearn, check out the documentation here.(http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression Scoring Functions\n",
    "In addition to error metrics, scikit-learn contains two scoring metrics which scale continuously from 0 to 1, with values of 0 being bad and 1 being perfect performance.\n",
    "\n",
    "These are the metrics that you'll use in the project at the end of the course. They have the advantage of looking similar to classification metrics, with numbers closer to 1.0 being good scores and bad scores tending to be near 0.\n",
    "\n",
    "One of these is the __R2 score__, which computes the coefficient of determination of predictions for true values. This is the default scoring method for regression learners in scikit-learn.\n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)\n",
    "The other is the __explained variance score__.\n",
    "\n",
    "While we will not dive deep into explained variance score and R2 score in this lecture , one important point to remember is that, in general, metrics for regression are such that \"higher is better\"; that is, higher scores indicate better performance. When using __error__ metrics, such as mean squared error or mean absolute error, we will need to overwrite this preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 score\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
