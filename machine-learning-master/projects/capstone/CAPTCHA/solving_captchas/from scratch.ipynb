{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HSV即色相、饱和度、明度（英语：Hue, Saturation, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACSCAYAAABVCTF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0EVEPPF/Z9EMDKnF5nYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1be9355dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = cv2.imread(\"2A2X.png\")\n",
    "plt.imshow(image)\n",
    "#轉換顏色空間\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "#thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)[1]\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1be9a04fbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v8BfApML4JWn4IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1be99f244a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = os.path.join(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\")\n",
    "p = os.path.join(save_path, \"2A2X.png\")\n",
    "cv2.imwrite(p, thresh)\n",
    "image = cv2.imread(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\\\\2A2X.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "- BORDER_CONSTANT: 使用常数填充边界 (i.e. 黑色或者 0)\n",
    "- BORDER_REPLICATE: 复制原图中最临近的行或者列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " http://monkeycoding.com/?p=600\n",
    " \n",
    " threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)\n",
    "\n",
    "src：輸入圖，只能輸入單通道，8位元或32位元浮點數影像。\n",
    "\n",
    "dst：輸出圖，尺寸大小、深度會和輸入圖相同。\n",
    "\n",
    "thresh：閾值。\n",
    "\n",
    "maxval：二值化結果的最大值。\n",
    "\n",
    "type：二值化操作型態，共有THRESH_BINARY、THRESH_BINARY_INV、THRESH_TRUNC、THRESH_TOZERO、THRESH_TOZERO_INV五種。\n",
    "\n",
    "type從上述五種結合CV_THRESH_OTSU，類似寫成：THRESH_BINARY | CV_THRESH_OTSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#thresh = cv2.threshold(gray, 0, 255,  cv2.THRESH_OTSU)[1]\n",
    "#print (thresh[1])\n",
    "#plt.imshow(thresh)\n",
    "type(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the contours (continuous blobs of pixels) the image\n",
    "# https://docs.opencv.org/3.3.1/d4/d73/tutorial_py_contours_begin.html\n",
    "contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Hack for compatibility with different OpenCV versions\n",
    "contours = contours[0] if imutils.is_cv2() else contours[1]\n",
    "\n",
    "save_path = os.path.join(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\")\n",
    "p = os.path.join(save_path, \"2A2X.png\")\n",
    "#cv2.imwrite(p,contours)\n",
    "#image = cv2.imread(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\\\\2A2X.png\")\n",
    "\n",
    "\n",
    "\n",
    "letter_image_regions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the contour to detect letters that\n",
    "        # are conjoined into one chunk\n",
    "    if w / h > 1.25:\n",
    "            # This contour is too wide to be a single letter!\n",
    "            # Split it in half into two letter regions!\n",
    "        half_width = int(w / 2)\n",
    "        letter_image_regions.append((x, y, half_width, h))\n",
    "        letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "    else:\n",
    "            # This is a normal letter by itself\n",
    "        letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    # If we found more or less than 4 letters in the captcha, our letter extraction\n",
    "    # didn't work correcly. Skip the image instead of saving bad training data!\n",
    "if len(letter_image_regions) != 4:\n",
    "    pass\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(letter_image_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29058 samples, validate on 9686 samples\n",
      "Epoch 1/10\n",
      "29058/29058 [==============================] - 135s - loss: 0.2520 - acc: 0.9365 - val_loss: 0.0230 - val_acc: 0.9940\n",
      "Epoch 2/10\n",
      "29058/29058 [==============================] - 128s - loss: 0.0150 - acc: 0.9962 - val_loss: 0.0216 - val_acc: 0.9922\n",
      "Epoch 3/10\n",
      "29058/29058 [==============================] - 123s - loss: 0.0085 - acc: 0.9977 - val_loss: 0.0121 - val_acc: 0.9965\n",
      "Epoch 4/10\n",
      "29058/29058 [==============================] - 107s - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0101 - val_acc: 0.9968\n",
      "Epoch 5/10\n",
      "29058/29058 [==============================] - 107s - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0108 - val_acc: 0.9968\n",
      "Epoch 6/10\n",
      "29058/29058 [==============================] - 107s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0079 - val_acc: 0.9975\n",
      "Epoch 7/10\n",
      "29058/29058 [==============================] - 106s - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0101 - val_acc: 0.9974\n",
      "Epoch 8/10\n",
      "29058/29058 [==============================] - 111s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0136 - val_acc: 0.9970\n",
      "Epoch 9/10\n",
      "29058/29058 [==============================] - 94s - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0050 - val_acc: 0.9985\n",
      "Epoch 10/10\n",
      "29058/29058 [==============================] - 94s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0167 - val_acc: 0.9971\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit\n",
    "\n",
    "\n",
    "LETTER_IMAGES_FOLDER = \"extracted_letter_images\"\n",
    "MODEL_FILENAME = \"captcha_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(20, 20, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense(32, activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHA text is: QYHY\n",
      "CAPTCHA text is: R9NY\n",
      "CAPTCHA text is: 7M6R\n",
      "CAPTCHA text is: BYN4\n",
      "CAPTCHA text is: MYGD\n",
      "CAPTCHA text is: 4JMM\n",
      "CAPTCHA text is: RCRC\n",
      "CAPTCHA text is: EJWN\n",
      "CAPTCHA text is: 7WTA\n",
      "CAPTCHA text is: R494\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from helpers import resize_to_fit\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"captcha_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "CAPTCHA_IMAGE_FOLDER = \"generated_captcha_images\"\n",
    "\n",
    "\n",
    "# Load up the model labels (so we can translate model predictions to actual letters)\n",
    "with open(MODEL_LABELS_FILENAME, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Load the trained neural network\n",
    "model = load_model(MODEL_FILENAME)\n",
    "\n",
    "# Grab some random CAPTCHA images to test against.\n",
    "# In the real world, you'd replace this section with code to grab a real\n",
    "# CAPTCHA image from a live website.\n",
    "captcha_image_files = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "captcha_image_files = np.random.choice(captcha_image_files, size=(10,), replace=False)\n",
    "\n",
    "# loop over the image paths\n",
    "for image_file in captcha_image_files:\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Add some extra padding around the image\n",
    "    image = cv2.copyMakeBorder(image, 20, 20, 20, 20, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # threshold the image (convert it to pure black and white)\n",
    "    thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # find the contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] if imutils.is_cv2() else contours[1]\n",
    "\n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we can loop through each of the four contours and extract the letter\n",
    "    # inside of each one\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the contour to detect letters that\n",
    "        # are conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # This contour is too wide to be a single letter!\n",
    "            # Split it in half into two letter regions!\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # This is a normal letter by itself\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    # If we found more or less than 4 letters in the captcha, our letter extraction\n",
    "    # didn't work correcly. Skip the image instead of saving bad training data!\n",
    "    if len(letter_image_regions) != 4:\n",
    "        continue\n",
    "\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "\n",
    "    # Create an output image and a list to hold our predicted letters\n",
    "    output = cv2.merge([image] * 3)\n",
    "    predictions = []\n",
    "\n",
    "    # loop over the lektters\n",
    "    for letter_bounding_box in letter_image_regions:\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "        # Re-size the letter image to 20x20 pixels to match training data\n",
    "        letter_image = resize_to_fit(letter_image, 20, 20)\n",
    "\n",
    "        # Turn the single image into a 4d list of images to make Keras happy\n",
    "        letter_image = np.expand_dims(letter_image, axis=2)\n",
    "        letter_image = np.expand_dims(letter_image, axis=0)\n",
    "\n",
    "        # Ask the neural network to make a prediction\n",
    "        prediction = model.predict(letter_image)\n",
    "\n",
    "        # Convert the one-hot-encoded prediction back to a normal letter\n",
    "        letter = lb.inverse_transform(prediction)[0]\n",
    "        predictions.append(letter)\n",
    "\n",
    "        # draw the prediction on the output image\n",
    "        cv2.rectangle(output, (x - 2, y - 2), (x + w + 4, y + h + 4), (0, 255, 0), 1)\n",
    "        cv2.putText(output, letter, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)\n",
    "\n",
    "    # Print the captcha's text\n",
    "    captcha_text = \"\".join(predictions)\n",
    "    print(\"CAPTCHA text is: {}\".format(captcha_text))\n",
    "\n",
    "    # Show the annotated image\n",
    "    cv2.imshow(\"Output\", output)\n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
