{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HSV即色相、饱和度、明度（英语：Hue, Saturation, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACSCAYAAABVCTF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0EVEPPF/Z9EMDKnF5nYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a24f849518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = cv2.imread(\"2A2X.png\")\n",
    "plt.imshow(image)\n",
    "#轉換顏色空間\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "#thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)[1]\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2506bf4a8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v8BfApML4JWn4IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a250135dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = os.path.join(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\")\n",
    "p = os.path.join(save_path, \"2A2X.png\")\n",
    "cv2.imwrite(p, thresh)\n",
    "image = cv2.imread(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\\\\2A2X.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "- BORDER_CONSTANT: 使用常数填充边界 (i.e. 黑色或者 0)\n",
    "- BORDER_REPLICATE: 复制原图中最临近的行或者列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " http://monkeycoding.com/?p=600\n",
    " \n",
    " threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)\n",
    "\n",
    "src：輸入圖，只能輸入單通道，8位元或32位元浮點數影像。\n",
    "\n",
    "dst：輸出圖，尺寸大小、深度會和輸入圖相同。\n",
    "\n",
    "thresh：閾值。\n",
    "\n",
    "maxval：二值化結果的最大值。\n",
    "\n",
    "type：二值化操作型態，共有THRESH_BINARY、THRESH_BINARY_INV、THRESH_TRUNC、THRESH_TOZERO、THRESH_TOZERO_INV五種。\n",
    "\n",
    "type從上述五種結合CV_THRESH_OTSU，類似寫成：THRESH_BINARY | CV_THRESH_OTSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#thresh = cv2.threshold(gray, 0, 255,  cv2.THRESH_OTSU)[1]\n",
    "#print (thresh[1])\n",
    "#plt.imshow(thresh)\n",
    "type(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the contours (continuous blobs of pixels) the image\n",
    "# https://docs.opencv.org/3.3.1/d4/d73/tutorial_py_contours_begin.html\n",
    "contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Hack for compatibility with different OpenCV versions\n",
    "contours = contours[0] if imutils.is_cv2() else contours[1]\n",
    "\n",
    "save_path = os.path.join(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\")\n",
    "p = os.path.join(save_path, \"2A2X.png\")\n",
    "#cv2.imwrite(p,contours)\n",
    "#image = cv2.imread(\"C:\\\\Users\\\\MLUSER\\\\Documents\\\\GitHub\\\\2A2X.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_image_regions = []\n",
    "for contour in contours:\n",
    "    # Get the rectangle that contains the contour\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "    # print (x,y,w,h)\n",
    "    # Compare the width and height of the contour to detect letters that\n",
    "    # are conjoined into one chunk\n",
    "    if w / h > 1.25:\n",
    "        # This contour is too wide to be a single letter!\n",
    "        # Split it in half into two letter regions!\n",
    "        half_width = int(w / 2)\n",
    "        letter_image_regions.append((x, y, half_width, h))\n",
    "        letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "    else:\n",
    "        # This is a normal letter by itself\n",
    "        letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    # If we found more or less than 4 letters in the captcha, our letter extraction\n",
    "    # didn't work correcly. Skip the image instead of saving bad training data!\n",
    "if len(letter_image_regions) != 4:\n",
    "    pass\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2507422b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAADKCAYAAAC7UQfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a250223940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(letter_image_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit\n",
    "\n",
    "\n",
    "LETTER_IMAGES_FOLDER = \"extracted_letter_images\"\n",
    "MODEL_FILENAME = \"captcha_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(20, 20, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense(32, activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, MaxPooling2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 20, 20, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1000)              401000    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                16416     \n",
      "=================================================================\n",
      "Total params: 929,928\n",
      "Trainable params: 929,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential()\\n\\nmodel.add(Flatten(input_shape=(20, 20, 1)))\\nmodel.add(Dense(1000, activation=\\'relu\\'))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(512, activation=\\'relu\\'))\\nmodel.add(Dropout(0.2))\\n# Output layer with 32 nodes (one for each possible letter/number we predict)\\nmodel.add(Dense(32, activation=\"softmax\"))\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "LETTER_IMAGES_FOLDER = \"extracted_letter_images\"\n",
    "MODEL_FILENAME = \"captcha_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "\n",
    "inp = Input(shape=(20,20,1))\n",
    "flat = Flatten()(ip)\n",
    "Dense1 = Dense(1000, activation='relu')(flat)\n",
    "Drop1 = Dropout(0.2)(Dense1)\n",
    "Dense2 = Dense(512, activation='relu')(Drop1)\n",
    "Drop2 = Dropout(0.2)(Dense2)\n",
    "Dense3 = Dense(32, activation='softmax')(Drop2)      \n",
    "model = Model(inp,Dense3)\n",
    "\n",
    "model.summary()\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(20, 20, 1)))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense(32, activation=\"softmax\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29058 samples, validate on 9686 samples\n",
      "Epoch 1/10\n",
      "29058/29058 [==============================] - 66s - loss: 0.4724 - acc: 0.8749 - val_loss: 0.0909 - val_acc: 0.9822\n",
      "Epoch 2/10\n",
      "29058/29058 [==============================] - 64s - loss: 0.0933 - acc: 0.9776 - val_loss: 0.0456 - val_acc: 0.9896\n",
      "Epoch 3/10\n",
      "29058/29058 [==============================] - 69s - loss: 0.0731 - acc: 0.9798 - val_loss: 0.0597 - val_acc: 0.9859\n",
      "Epoch 4/10\n",
      "29058/29058 [==============================] - 69s - loss: 0.0651 - acc: 0.9815 - val_loss: 0.0390 - val_acc: 0.9898\n",
      "Epoch 5/10\n",
      "29058/29058 [==============================] - 62s - loss: 0.0568 - acc: 0.9835 - val_loss: 0.0320 - val_acc: 0.9916\n",
      "Epoch 6/10\n",
      "29058/29058 [==============================] - 61s - loss: 0.0545 - acc: 0.9832 - val_loss: 0.0311 - val_acc: 0.9925\n",
      "Epoch 7/10\n",
      "29058/29058 [==============================] - 61s - loss: 0.0397 - acc: 0.9883 - val_loss: 0.0260 - val_acc: 0.9929\n",
      "Epoch 8/10\n",
      "29058/29058 [==============================] - 72s - loss: 0.0496 - acc: 0.9844 - val_loss: 0.0273 - val_acc: 0.9921\n",
      "Epoch 9/10\n",
      "29058/29058 [==============================] - 67s - loss: 0.0437 - acc: 0.9866 - val_loss: 0.0280 - val_acc: 0.9925\n",
      "Epoch 10/10\n",
      "29058/29058 [==============================] - 63s - loss: 0.0412 - acc: 0.9872 - val_loss: 0.0219 - val_acc: 0.9945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHA text is: K3WB\n",
      "CAPTCHA text is: FZQP\n",
      "CAPTCHA text is: FNS2\n",
      "CAPTCHA text is: KZT3\n",
      "CAPTCHA text is: PN22\n",
      "CAPTCHA text is: V2NZ\n",
      "CAPTCHA text is: 59H8\n",
      "CAPTCHA text is: KBFS\n",
      "CAPTCHA text is: M3LP\n",
      "CAPTCHA text is: 8YDS\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from helpers import resize_to_fit\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"captcha_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME = \"model_labels.dat\"\n",
    "CAPTCHA_IMAGE_FOLDER = \"generated_captcha_images\"\n",
    "\n",
    "\n",
    "# Load up the model labels (so we can translate model predictions to actual letters)\n",
    "with open(MODEL_LABELS_FILENAME, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Load the trained neural network\n",
    "model = load_model(MODEL_FILENAME)\n",
    "\n",
    "# Grab some random CAPTCHA images to test against.\n",
    "# In the real world, you'd replace this section with code to grab a real\n",
    "# CAPTCHA image from a live website.\n",
    "captcha_image_files = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "captcha_image_files = np.random.choice(captcha_image_files, size=(10,), replace=False)\n",
    "\n",
    "# loop over the image paths\n",
    "for image_file in captcha_image_files:\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Add some extra padding around the image\n",
    "    image = cv2.copyMakeBorder(image, 20, 20, 20, 20, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # threshold the image (convert it to pure black and white)\n",
    "    thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # find the contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] if imutils.is_cv2() else contours[1]\n",
    "\n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we can loop through each of the four contours and extract the letter\n",
    "    # inside of each one\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the contour to detect letters that\n",
    "        # are conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # This contour is too wide to be a single letter!\n",
    "            # Split it in half into two letter regions!\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # This is a normal letter by itself\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    # If we found more or less than 4 letters in the captcha, our letter extraction\n",
    "    # didn't work correcly. Skip the image instead of saving bad training data!\n",
    "    if len(letter_image_regions) != 4:\n",
    "        continue\n",
    "\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "\n",
    "    # Create an output image and a list to hold our predicted letters\n",
    "    output = cv2.merge([image] * 3)\n",
    "    predictions = []\n",
    "\n",
    "    # loop over the lektters\n",
    "    for letter_bounding_box in letter_image_regions:\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "        # Re-size the letter image to 20x20 pixels to match training data\n",
    "        letter_image = resize_to_fit(letter_image, 20, 20)\n",
    "\n",
    "        # Turn the single image into a 4d list of images to make Keras happy\n",
    "        letter_image = np.expand_dims(letter_image, axis=2)\n",
    "        letter_image = np.expand_dims(letter_image, axis=0)\n",
    "\n",
    "        # Ask the neural network to make a prediction\n",
    "        prediction = model.predict(letter_image)\n",
    "\n",
    "        # Convert the one-hot-encoded prediction back to a normal letter\n",
    "        letter = lb.inverse_transform(prediction)[0]\n",
    "        predictions.append(letter)\n",
    "\n",
    "        # draw the prediction on the output image\n",
    "        cv2.rectangle(output, (x - 2, y - 2), (x + w + 4, y + h + 4), (0, 255, 0), 1)\n",
    "        cv2.putText(output, letter, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)\n",
    "\n",
    "    # Print the captcha's text \n",
    "    captcha_text = \"\".join(predictions)\n",
    "    print(\"CAPTCHA text is: {}\".format(captcha_text))\n",
    "\n",
    "    # Show the annotated image\n",
    "    cv2.imshow(\"Output\", output)\n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
