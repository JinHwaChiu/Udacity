{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1jOUggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e634dc6c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detectora\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[5])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    "Humans face detection accuracy: 0.99\n",
    "Dogs face detection accuracy: 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans face detection accuracy: 0.99\n",
      "Dogs face detection accuracy: 0.11\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "humans_pred = []\n",
    "dogs_pred = []\n",
    "for file in human_files_short: \n",
    "    humans_pred.append(int(face_detector(file)))\n",
    "    humans_score = np.mean(humans_pred)\n",
    "\n",
    "for file in dog_files_short:\n",
    "    dogs_pred.append(int(face_detector(file)))\n",
    "    dogs_score = np.mean(dogs_pred)\n",
    "\n",
    "print('Humans face detection accuracy:', humans_score)\n",
    "print('Dogs face detection accuracy:', dogs_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "I think we can use images without eyes or mouths substitute for clear images dur to private concerns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of the humans were detected as dogs\n",
      "100% of the dogs were detected as dogs\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "human_count = 0\n",
    "for human_img in human_files_short:\n",
    "    if dog_detector(human_img):\n",
    "        human_count += 1\n",
    "dog_count = 0\n",
    "for dog_img in dog_files_short:\n",
    "    if dog_detector(dog_img):\n",
    "        dog_count += 1\n",
    "print (str(human_count)+\"% of the humans were detected as dogs\")\n",
    "print (str(dog_count)+\"% of the dogs were detected as dogs\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [01:29<00:00, 74.99it/s]\n",
      "100%|██████████| 835/835 [00:10<00:00, 78.94it/s]\n",
      "100%|██████████| 836/836 [00:09<00:00, 85.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               25088500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               66633     \n",
      "=================================================================\n",
      "Total params: 25,165,677\n",
      "Trainable params: 25,165,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040/6680 [=================>............] - ETA: 1562s - loss: 4.9044 - acc: 0.0000e+ - ETA: 1205s - loss: 9.6315 - acc: 0.0250   - ETA: 1073s - loss: 10.8494 - acc: 0.016 - ETA: 1022s - loss: 10.1853 - acc: 0.012 - ETA: 987s - loss: 9.3014 - acc: 0.0100  - ETA: 964s - loss: 8.6171 - acc: 0.008 - ETA: 965s - loss: 8.0881 - acc: 0.007 - ETA: 945s - loss: 7.7338 - acc: 0.006 - ETA: 926s - loss: 7.4358 - acc: 0.005 - ETA: 920s - loss: 7.1831 - acc: 0.005 - ETA: 908s - loss: 6.9653 - acc: 0.009 - ETA: 897s - loss: 6.7900 - acc: 0.008 - ETA: 906s - loss: 6.6561 - acc: 0.007 - ETA: 896s - loss: 6.5255 - acc: 0.007 - ETA: 896s - loss: 6.4171 - acc: 0.006 - ETA: 901s - loss: 6.3169 - acc: 0.006 - ETA: 900s - loss: 6.2413 - acc: 0.005 - ETA: 900s - loss: 6.1678 - acc: 0.005 - ETA: 904s - loss: 6.0993 - acc: 0.007 - ETA: 906s - loss: 6.0387 - acc: 0.007 - ETA: 907s - loss: 5.9830 - acc: 0.007 - ETA: 904s - loss: 5.9361 - acc: 0.006 - ETA: 907s - loss: 5.8909 - acc: 0.006 - ETA: 912s - loss: 5.8504 - acc: 0.006 - ETA: 905s - loss: 5.8114 - acc: 0.006 - ETA: 901s - loss: 5.7757 - acc: 0.005 - ETA: 892s - loss: 5.7417 - acc: 0.005 - ETA: 884s - loss: 5.7093 - acc: 0.005 - ETA: 884s - loss: 5.6835 - acc: 0.005 - ETA: 875s - loss: 5.6568 - acc: 0.005 - ETA: 871s - loss: 5.6331 - acc: 0.004 - ETA: 863s - loss: 5.6095 - acc: 0.004 - ETA: 857s - loss: 5.5878 - acc: 0.006 - ETA: 851s - loss: 5.5661 - acc: 0.005 - ETA: 849s - loss: 5.5462 - acc: 0.007 - ETA: 848s - loss: 5.5287 - acc: 0.006 - ETA: 842s - loss: 5.5123 - acc: 0.006 - ETA: 835s - loss: 5.4959 - acc: 0.007 - ETA: 831s - loss: 5.4809 - acc: 0.007 - ETA: 825s - loss: 5.4662 - acc: 0.007 - ETA: 822s - loss: 5.4517 - acc: 0.007 - ETA: 821s - loss: 5.4382 - acc: 0.008 - ETA: 817s - loss: 5.4254 - acc: 0.010 - ETA: 812s - loss: 5.4132 - acc: 0.010 - ETA: 808s - loss: 5.4005 - acc: 0.010 - ETA: 803s - loss: 5.3913 - acc: 0.009 - ETA: 800s - loss: 5.3803 - acc: 0.010 - ETA: 798s - loss: 5.3701 - acc: 0.011 - ETA: 793s - loss: 5.3598 - acc: 0.011 - ETA: 789s - loss: 5.3505 - acc: 0.011 - ETA: 785s - loss: 5.3418 - acc: 0.011 - ETA: 780s - loss: 5.3326 - acc: 0.012 - ETA: 777s - loss: 5.3249 - acc: 0.012 - ETA: 774s - loss: 5.3167 - acc: 0.012 - ETA: 772s - loss: 5.3092 - acc: 0.011 - ETA: 767s - loss: 5.3016 - acc: 0.011 - ETA: 764s - loss: 5.2946 - acc: 0.011 - ETA: 760s - loss: 5.2868 - acc: 0.011 - ETA: 760s - loss: 5.2797 - acc: 0.011 - ETA: 761s - loss: 5.2718 - acc: 0.013 - ETA: 762s - loss: 5.2633 - acc: 0.013 - ETA: 763s - loss: 5.2588 - acc: 0.013 - ETA: 761s - loss: 5.2583 - acc: 0.013 - ETA: 760s - loss: 5.2522 - acc: 0.013 - ETA: 757s - loss: 5.2456 - acc: 0.013 - ETA: 758s - loss: 5.2413 - acc: 0.012 - ETA: 758s - loss: 5.2345 - acc: 0.013 - ETA: 758s - loss: 5.2291 - acc: 0.013 - ETA: 757s - loss: 5.2247 - acc: 0.013 - ETA: 753s - loss: 5.2202 - acc: 0.013 - ETA: 749s - loss: 5.2149 - acc: 0.013 - ETA: 744s - loss: 5.2080 - acc: 0.013 - ETA: 739s - loss: 5.2080 - acc: 0.013 - ETA: 735s - loss: 5.2032 - acc: 0.013 - ETA: 732s - loss: 5.1989 - acc: 0.013 - ETA: 729s - loss: 5.1953 - acc: 0.013 - ETA: 726s - loss: 5.1914 - acc: 0.013 - ETA: 722s - loss: 5.1877 - acc: 0.013 - ETA: 718s - loss: 5.1838 - acc: 0.013 - ETA: 714s - loss: 5.1798 - acc: 0.013 - ETA: 712s - loss: 5.1757 - acc: 0.013 - ETA: 709s - loss: 5.1723 - acc: 0.012 - ETA: 706s - loss: 5.1680 - acc: 0.012 - ETA: 702s - loss: 5.1644 - acc: 0.012 - ETA: 699s - loss: 5.1607 - acc: 0.012 - ETA: 696s - loss: 5.1596 - acc: 0.012 - ETA: 694s - loss: 5.1567 - acc: 0.012 - ETA: 691s - loss: 5.1542 - acc: 0.011 - ETA: 688s - loss: 5.1508 - acc: 0.011 - ETA: 686s - loss: 5.1481 - acc: 0.011 - ETA: 685s - loss: 5.1452 - acc: 0.011 - ETA: 684s - loss: 5.1417 - acc: 0.012 - ETA: 682s - loss: 5.1398 - acc: 0.011 - ETA: 678s - loss: 5.1368 - acc: 0.011 - ETA: 675s - loss: 5.1339 - acc: 0.011 - ETA: 672s - loss: 5.1311 - acc: 0.011 - ETA: 669s - loss: 5.1286 - acc: 0.011 - ETA: 667s - loss: 5.1255 - acc: 0.011 - ETA: 666s - loss: 5.1229 - acc: 0.011 - ETA: 664s - loss: 5.1217 - acc: 0.011 - ETA: 660s - loss: 5.1201 - acc: 0.010 - ETA: 657s - loss: 5.1176 - acc: 0.010 - ETA: 654s - loss: 5.1148 - acc: 0.011 - ETA: 653s - loss: 5.1119 - acc: 0.011 - ETA: 650s - loss: 5.1096 - acc: 0.011 - ETA: 649s - loss: 5.1070 - acc: 0.010 - ETA: 645s - loss: 5.1044 - acc: 0.010 - ETA: 644s - loss: 5.1026 - acc: 0.010 - ETA: 640s - loss: 5.1013 - acc: 0.011 - ETA: 637s - loss: 5.0993 - acc: 0.011 - ETA: 635s - loss: 5.0972 - acc: 0.011 - ETA: 631s - loss: 5.0950 - acc: 0.011 - ETA: 627s - loss: 5.0921 - acc: 0.011 - ETA: 625s - loss: 5.0919 - acc: 0.011 - ETA: 621s - loss: 5.0901 - acc: 0.011 - ETA: 617s - loss: 5.0879 - acc: 0.011 - ETA: 614s - loss: 5.0858 - acc: 0.011 - ETA: 610s - loss: 5.0847 - acc: 0.011 - ETA: 607s - loss: 5.0829 - acc: 0.011 - ETA: 604s - loss: 5.0817 - acc: 0.011 - ETA: 601s - loss: 5.0800 - acc: 0.011 - ETA: 597s - loss: 5.0780 - acc: 0.011 - ETA: 593s - loss: 5.0769 - acc: 0.011 - ETA: 590s - loss: 5.0752 - acc: 0.010 - ETA: 586s - loss: 5.0739 - acc: 0.010 - ETA: 583s - loss: 5.0725 - acc: 0.010 - ETA: 580s - loss: 5.0712 - acc: 0.010 - ETA: 577s - loss: 5.0697 - acc: 0.010 - ETA: 574s - loss: 5.0681 - acc: 0.010 - ETA: 571s - loss: 5.0671 - acc: 0.010 - ETA: 568s - loss: 5.0655 - acc: 0.010 - ETA: 565s - loss: 5.0643 - acc: 0.010 - ETA: 563s - loss: 5.0631 - acc: 0.010 - ETA: 560s - loss: 5.0614 - acc: 0.010 - ETA: 557s - loss: 5.0598 - acc: 0.010 - ETA: 554s - loss: 5.0606 - acc: 0.010 - ETA: 551s - loss: 5.0597 - acc: 0.010 - ETA: 548s - loss: 5.0586 - acc: 0.010 - ETA: 545s - loss: 5.0571 - acc: 0.010 - ETA: 542s - loss: 5.0556 - acc: 0.010 - ETA: 539s - loss: 5.0549 - acc: 0.009 - ETA: 536s - loss: 5.0535 - acc: 0.009 - ETA: 533s - loss: 5.0522 - acc: 0.010 - ETA: 530s - loss: 5.0510 - acc: 0.010 - ETA: 528s - loss: 5.0513 - acc: 0.010 - ETA: 526s - loss: 5.0499 - acc: 0.010 - ETA: 524s - loss: 5.0484 - acc: 0.010 - ETA: 521s - loss: 5.0467 - acc: 0.010 - ETA: 518s - loss: 5.0455 - acc: 0.010 - ETA: 515s - loss: 5.0440 - acc: 0.010 - ETA: 512s - loss: 5.0425 - acc: 0.009 - ETA: 510s - loss: 5.0413 - acc: 0.009 - ETA: 506s - loss: 5.0407 - acc: 0.009 - ETA: 503s - loss: 5.0396 - acc: 0.010 - ETA: 500s - loss: 5.0385 - acc: 0.010 - ETA: 498s - loss: 5.0377 - acc: 0.010 - ETA: 495s - loss: 5.0361 - acc: 0.010 - ETA: 492s - loss: 5.0342 - acc: 0.010 - ETA: 489s - loss: 5.0331 - acc: 0.010 - ETA: 486s - loss: 5.0350 - acc: 0.010 - ETA: 483s - loss: 5.0339 - acc: 0.010 - ETA: 481s - loss: 5.0322 - acc: 0.010 - ETA: 478s - loss: 5.0315 - acc: 0.010 - ETA: 475s - loss: 5.0305 - acc: 0.010 - ETA: 472s - loss: 5.0293 - acc: 0.010 - ETA: 470s - loss: 5.0280 - acc: 0.010 - ETA: 467s - loss: 5.0271 - acc: 0.010 - ETA: 464s - loss: 5.0263 - acc: 0.010 - ETA: 461s - loss: 5.0252 - acc: 0.010 - ETA: 458s - loss: 5.0244 - acc: 0.010 - ETA: 455s - loss: 5.0229 - acc: 0.010 - ETA: 452s - loss: 5.0241 - acc: 0.010 - ETA: 449s - loss: 5.0238 - acc: 0.010 - ETA: 446s - loss: 5.0233 - acc: 0.010 - ETA: 443s - loss: 5.0219 - acc: 0.010 - ETA: 440s - loss: 5.0210 - acc: 0.010 - ETA: 437s - loss: 5.0198 - acc: 0.010 - ETA: 434s - loss: 5.0189 - acc: 0.010 - ETA: 431s - loss: 5.0181 - acc: 0.010 - ETA: 428s - loss: 5.0179 - acc: 0.010 - ETA: 425s - loss: 5.0166 - acc: 0.010 - ETA: 422s - loss: 5.0155 - acc: 0.010 - ETA: 419s - loss: 5.0149 - acc: 0.010 - ETA: 416s - loss: 5.0143 - acc: 0.010 - ETA: 413s - loss: 5.0133 - acc: 0.010 - ETA: 410s - loss: 5.0123 - acc: 0.010 - ETA: 408s - loss: 5.0120 - acc: 0.010 - ETA: 405s - loss: 5.0108 - acc: 0.010 - ETA: 403s - loss: 5.0093 - acc: 0.010 - ETA: 400s - loss: 5.0081 - acc: 0.010 - ETA: 397s - loss: 5.0073 - acc: 0.010 - ETA: 395s - loss: 5.0063 - acc: 0.010 - ETA: 392s - loss: 5.0049 - acc: 0.010 - ETA: 389s - loss: 5.0042 - acc: 0.010 - ETA: 386s - loss: 5.0033 - acc: 0.010 - ETA: 383s - loss: 5.0029 - acc: 0.010 - ETA: 380s - loss: 5.0019 - acc: 0.010 - ETA: 378s - loss: 5.0018 - acc: 0.010 - ETA: 375s - loss: 5.0011 - acc: 0.010 - ETA: 372s - loss: 5.0006 - acc: 0.010 - ETA: 369s - loss: 4.9997 - acc: 0.010 - ETA: 366s - loss: 4.9993 - acc: 0.0101\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 364s - loss: 4.9986 - acc: 0.010 - ETA: 361s - loss: 4.9980 - acc: 0.010 - ETA: 358s - loss: 4.9972 - acc: 0.010 - ETA: 355s - loss: 4.9962 - acc: 0.010 - ETA: 352s - loss: 4.9954 - acc: 0.010 - ETA: 349s - loss: 4.9945 - acc: 0.010 - ETA: 346s - loss: 4.9938 - acc: 0.010 - ETA: 343s - loss: 4.9926 - acc: 0.010 - ETA: 340s - loss: 4.9922 - acc: 0.010 - ETA: 337s - loss: 4.9916 - acc: 0.010 - ETA: 335s - loss: 4.9909 - acc: 0.010 - ETA: 332s - loss: 4.9902 - acc: 0.011 - ETA: 329s - loss: 4.9897 - acc: 0.010 - ETA: 326s - loss: 4.9892 - acc: 0.011 - ETA: 323s - loss: 4.9883 - acc: 0.011 - ETA: 321s - loss: 4.9877 - acc: 0.011 - ETA: 318s - loss: 4.9867 - acc: 0.011 - ETA: 315s - loss: 4.9861 - acc: 0.010 - ETA: 313s - loss: 4.9854 - acc: 0.010 - ETA: 310s - loss: 4.9837 - acc: 0.011 - ETA: 307s - loss: 4.9854 - acc: 0.011 - ETA: 304s - loss: 4.9850 - acc: 0.011 - ETA: 302s - loss: 4.9846 - acc: 0.011 - ETA: 299s - loss: 4.9836 - acc: 0.011 - ETA: 296s - loss: 4.9827 - acc: 0.011 - ETA: 293s - loss: 4.9811 - acc: 0.011 - ETA: 290s - loss: 4.9813 - acc: 0.010 - ETA: 287s - loss: 4.9810 - acc: 0.010 - ETA: 285s - loss: 4.9803 - acc: 0.010 - ETA: 282s - loss: 4.9794 - acc: 0.010 - ETA: 279s - loss: 4.9786 - acc: 0.010 - ETA: 276s - loss: 4.9779 - acc: 0.010 - ETA: 274s - loss: 4.9779 - acc: 0.010 - ETA: 271s - loss: 4.9763 - acc: 0.010 - ETA: 268s - loss: 4.9756 - acc: 0.010 - ETA: 265s - loss: 4.9748 - acc: 0.010 - ETA: 262s - loss: 4.9746 - acc: 0.010 - ETA: 260s - loss: 4.9735 - acc: 0.010 - ETA: 257s - loss: 4.9729 - acc: 0.010 - ETA: 254s - loss: 4.9715 - acc: 0.010 - ETA: 251s - loss: 4.9709 - acc: 0.010 - ETA: 248s - loss: 4.9694 - acc: 0.010 - ETA: 246s - loss: 4.9677 - acc: 0.010 - ETA: 243s - loss: 4.9664 - acc: 0.010 - ETA: 240s - loss: 4.9657 - acc: 0.010 - ETA: 237s - loss: 4.9646 - acc: 0.010 - ETA: 234s - loss: 4.9637 - acc: 0.010 - ETA: 232s - loss: 4.9630 - acc: 0.010 - ETA: 229s - loss: 4.9627 - acc: 0.010 - ETA: 226s - loss: 4.9626 - acc: 0.010 - ETA: 223s - loss: 4.9624 - acc: 0.010 - ETA: 221s - loss: 4.9612 - acc: 0.011 - ETA: 218s - loss: 4.9614 - acc: 0.011 - ETA: 215s - loss: 4.9608 - acc: 0.011 - ETA: 212s - loss: 4.9606 - acc: 0.011 - ETA: 210s - loss: 4.9596 - acc: 0.011 - ETA: 207s - loss: 4.9589 - acc: 0.011 - ETA: 204s - loss: 4.9574 - acc: 0.011 - ETA: 201s - loss: 4.9565 - acc: 0.011 - ETA: 198s - loss: 4.9553 - acc: 0.011 - ETA: 196s - loss: 4.9563 - acc: 0.011 - ETA: 193s - loss: 4.9550 - acc: 0.011 - ETA: 190s - loss: 4.9538 - acc: 0.011 - ETA: 187s - loss: 4.9532 - acc: 0.011 - ETA: 185s - loss: 4.9521 - acc: 0.011 - ETA: 182s - loss: 4.9511 - acc: 0.011 - ETA: 179s - loss: 4.9509 - acc: 0.011 - ETA: 176s - loss: 4.9509 - acc: 0.011 - ETA: 174s - loss: 4.9503 - acc: 0.011 - ETA: 171s - loss: 4.9493 - acc: 0.011 - ETA: 168s - loss: 4.9485 - acc: 0.011 - ETA: 165s - loss: 4.9472 - acc: 0.011 - ETA: 162s - loss: 4.9471 - acc: 0.011 - ETA: 160s - loss: 4.9467 - acc: 0.011 - ETA: 157s - loss: 4.9456 - acc: 0.011 - ETA: 154s - loss: 4.9453 - acc: 0.011 - ETA: 151s - loss: 4.9442 - acc: 0.011 - ETA: 149s - loss: 4.9436 - acc: 0.011 - ETA: 146s - loss: 4.9430 - acc: 0.011 - ETA: 143s - loss: 4.9425 - acc: 0.011 - ETA: 140s - loss: 4.9417 - acc: 0.011 - ETA: 137s - loss: 4.9420 - acc: 0.011 - ETA: 135s - loss: 4.9417 - acc: 0.011 - ETA: 132s - loss: 4.9405 - acc: 0.012 - ETA: 129s - loss: 4.9395 - acc: 0.012 - ETA: 126s - loss: 4.9381 - acc: 0.012 - ETA: 124s - loss: 4.9395 - acc: 0.011 - ETA: 121s - loss: 4.9390 - acc: 0.012 - ETA: 118s - loss: 4.9387 - acc: 0.012 - ETA: 115s - loss: 4.9384 - acc: 0.012 - ETA: 113s - loss: 4.9387 - acc: 0.012 - ETA: 110s - loss: 4.9381 - acc: 0.012 - ETA: 107s - loss: 4.9382 - acc: 0.012 - ETA: 104s - loss: 4.9377 - acc: 0.012 - ETA: 102s - loss: 4.9367 - acc: 0.012 - ETA: 99s - loss: 4.9364 - acc: 0.012 - ETA: 96s - loss: 4.9358 - acc: 0.01 - ETA: 93s - loss: 4.9346 - acc: 0.01 - ETA: 90s - loss: 4.9340 - acc: 0.01 - ETA: 88s - loss: 4.9338 - acc: 0.01 - ETA: 85s - loss: 4.9329 - acc: 0.01 - ETA: 82s - loss: 4.9318 - acc: 0.01 - ETA: 79s - loss: 4.9316 - acc: 0.01 - ETA: 77s - loss: 4.9309 - acc: 0.01 - ETA: 74s - loss: 4.9310 - acc: 0.01 - ETA: 71s - loss: 4.9301 - acc: 0.01 - ETA: 69s - loss: 4.9290 - acc: 0.01 - ETA: 66s - loss: 4.9282 - acc: 0.01 - ETA: 63s - loss: 4.9278 - acc: 0.01 - ETA: 60s - loss: 4.9278 - acc: 0.01 - ETA: 58s - loss: 4.9268 - acc: 0.01 - ETA: 55s - loss: 4.9270 - acc: 0.01 - ETA: 52s - loss: 4.9263 - acc: 0.01 - ETA: 49s - loss: 4.9262 - acc: 0.01 - ETA: 47s - loss: 4.9255 - acc: 0.01 - ETA: 44s - loss: 4.9248 - acc: 0.01 - ETA: 41s - loss: 4.9242 - acc: 0.01 - ETA: 38s - loss: 4.9237 - acc: 0.01 - ETA: 35s - loss: 4.9232 - acc: 0.01 - ETA: 33s - loss: 4.9224 - acc: 0.01 - ETA: 30s - loss: 4.9211 - acc: 0.01 - ETA: 27s - loss: 4.9212 - acc: 0.01 - ETA: 24s - loss: 4.9202 - acc: 0.01 - ETA: 22s - loss: 4.9193 - acc: 0.01 - ETA: 19s - loss: 4.9182 - acc: 0.01 - ETA: 16s - loss: 4.9177 - acc: 0.01 - ETA: 13s - loss: 4.9174 - acc: 0.01 - ETA: 11s - loss: 4.9166 - acc: 0.01 - ETA: 8s - loss: 4.9156 - acc: 0.0128 - ETA: 5s - loss: 4.9147 - acc: 0.013 - ETA: 2s - loss: 4.9139 - acc: 0.0131Epoch 00000: val_loss improved from inf to 4.63898, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 971s - loss: 4.9132 - acc: 0.0132 - val_loss: 4.6390 - val_acc: 0.0455\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 761s - loss: 4.4741 - acc: 0.150 - ETA: 759s - loss: 4.6809 - acc: 0.075 - ETA: 800s - loss: 4.6147 - acc: 0.083 - ETA: 832s - loss: 4.6046 - acc: 0.075 - ETA: 822s - loss: 4.5905 - acc: 0.070 - ETA: 817s - loss: 4.6424 - acc: 0.058 - ETA: 813s - loss: 4.6432 - acc: 0.057 - ETA: 800s - loss: 4.6489 - acc: 0.050 - ETA: 804s - loss: 4.6351 - acc: 0.044 - ETA: 810s - loss: 4.6455 - acc: 0.045 - ETA: 800s - loss: 4.6288 - acc: 0.045 - ETA: 791s - loss: 4.6459 - acc: 0.041 - ETA: 785s - loss: 4.6306 - acc: 0.042 - ETA: 778s - loss: 4.6118 - acc: 0.050 - ETA: 773s - loss: 4.6197 - acc: 0.050 - ETA: 782s - loss: 4.6217 - acc: 0.046 - ETA: 775s - loss: 4.6053 - acc: 0.050 - ETA: 769s - loss: 4.6002 - acc: 0.047 - ETA: 764s - loss: 4.6103 - acc: 0.044 - ETA: 761s - loss: 4.6224 - acc: 0.045 - ETA: 758s - loss: 4.6303 - acc: 0.042 - ETA: 753s - loss: 4.6319 - acc: 0.040 - ETA: 757s - loss: 4.6283 - acc: 0.039 - ETA: 755s - loss: 4.6112 - acc: 0.041 - ETA: 752s - loss: 4.6158 - acc: 0.042 - ETA: 753s - loss: 4.6131 - acc: 0.042 - ETA: 751s - loss: 4.6132 - acc: 0.040 - ETA: 748s - loss: 4.6118 - acc: 0.039 - ETA: 753s - loss: 4.6036 - acc: 0.044 - ETA: 761s - loss: 4.6024 - acc: 0.046 - ETA: 764s - loss: 4.6061 - acc: 0.046 - ETA: 767s - loss: 4.6058 - acc: 0.045 - ETA: 767s - loss: 4.6092 - acc: 0.045 - ETA: 766s - loss: 4.6088 - acc: 0.044 - ETA: 768s - loss: 4.6086 - acc: 0.044 - ETA: 772s - loss: 4.6088 - acc: 0.044 - ETA: 774s - loss: 4.6009 - acc: 0.044 - ETA: 773s - loss: 4.6061 - acc: 0.043 - ETA: 781s - loss: 4.6043 - acc: 0.043 - ETA: 785s - loss: 4.6067 - acc: 0.043 - ETA: 786s - loss: 4.6004 - acc: 0.043 - ETA: 785s - loss: 4.6008 - acc: 0.045 - ETA: 782s - loss: 4.5953 - acc: 0.046 - ETA: 787s - loss: 4.5896 - acc: 0.045 - ETA: 792s - loss: 4.5917 - acc: 0.045 - ETA: 796s - loss: 4.5889 - acc: 0.045 - ETA: 799s - loss: 4.5869 - acc: 0.046 - ETA: 805s - loss: 4.5780 - acc: 0.049 - ETA: 807s - loss: 4.5677 - acc: 0.049 - ETA: 806s - loss: 4.5796 - acc: 0.050 - ETA: 807s - loss: 4.5847 - acc: 0.049 - ETA: 804s - loss: 4.5837 - acc: 0.048 - ETA: 803s - loss: 4.5815 - acc: 0.047 - ETA: 803s - loss: 4.5819 - acc: 0.047 - ETA: 804s - loss: 4.5804 - acc: 0.048 - ETA: 801s - loss: 4.5776 - acc: 0.048 - ETA: 799s - loss: 4.5774 - acc: 0.047 - ETA: 798s - loss: 4.5757 - acc: 0.048 - ETA: 801s - loss: 4.5791 - acc: 0.048 - ETA: 801s - loss: 4.5801 - acc: 0.049 - ETA: 799s - loss: 4.5711 - acc: 0.050 - ETA: 796s - loss: 4.5718 - acc: 0.050 - ETA: 794s - loss: 4.5714 - acc: 0.049 - ETA: 792s - loss: 4.5692 - acc: 0.050 - ETA: 791s - loss: 4.5673 - acc: 0.050 - ETA: 788s - loss: 4.5689 - acc: 0.050 - ETA: 785s - loss: 4.5699 - acc: 0.050 - ETA: 783s - loss: 4.5709 - acc: 0.051 - ETA: 782s - loss: 4.5704 - acc: 0.051 - ETA: 779s - loss: 4.5740 - acc: 0.050 - ETA: 776s - loss: 4.5741 - acc: 0.050 - ETA: 773s - loss: 4.5748 - acc: 0.050 - ETA: 771s - loss: 4.5738 - acc: 0.050 - ETA: 770s - loss: 4.5824 - acc: 0.050 - ETA: 768s - loss: 4.5809 - acc: 0.049 - ETA: 766s - loss: 4.5844 - acc: 0.048 - ETA: 764s - loss: 4.5810 - acc: 0.049 - ETA: 763s - loss: 4.5787 - acc: 0.049 - ETA: 762s - loss: 4.5774 - acc: 0.049 - ETA: 760s - loss: 4.5746 - acc: 0.050 - ETA: 757s - loss: 4.5777 - acc: 0.050 - ETA: 754s - loss: 4.5814 - acc: 0.050 - ETA: 751s - loss: 4.5788 - acc: 0.050 - ETA: 749s - loss: 4.5773 - acc: 0.051 - ETA: 745s - loss: 4.5791 - acc: 0.051 - ETA: 741s - loss: 4.5808 - acc: 0.051 - ETA: 737s - loss: 4.5783 - acc: 0.050 - ETA: 734s - loss: 4.5748 - acc: 0.050 - ETA: 732s - loss: 4.5780 - acc: 0.050 - ETA: 728s - loss: 4.5763 - acc: 0.050 - ETA: 725s - loss: 4.5756 - acc: 0.051 - ETA: 722s - loss: 4.5741 - acc: 0.051 - ETA: 720s - loss: 4.5711 - acc: 0.051 - ETA: 719s - loss: 4.5707 - acc: 0.051 - ETA: 715s - loss: 4.5701 - acc: 0.051 - ETA: 712s - loss: 4.5696 - acc: 0.051 - ETA: 710s - loss: 4.5712 - acc: 0.051 - ETA: 706s - loss: 4.5723 - acc: 0.051 - ETA: 703s - loss: 4.5697 - acc: 0.051 - ETA: 700s - loss: 4.5708 - acc: 0.052 - ETA: 697s - loss: 4.5696 - acc: 0.051 - ETA: 693s - loss: 4.5685 - acc: 0.051 - ETA: 689s - loss: 4.5693 - acc: 0.051 - ETA: 686s - loss: 4.5680 - acc: 0.051 - ETA: 683s - loss: 4.5662 - acc: 0.051 - ETA: 679s - loss: 4.5672 - acc: 0.050 - ETA: 676s - loss: 4.5620 - acc: 0.051 - ETA: 672s - loss: 4.5626 - acc: 0.051 - ETA: 669s - loss: 4.5592 - acc: 0.051 - ETA: 666s - loss: 4.5588 - acc: 0.051 - ETA: 663s - loss: 4.5600 - acc: 0.051 - ETA: 659s - loss: 4.5592 - acc: 0.051 - ETA: 656s - loss: 4.5601 - acc: 0.050 - ETA: 652s - loss: 4.5613 - acc: 0.050 - ETA: 649s - loss: 4.5599 - acc: 0.051 - ETA: 646s - loss: 4.5610 - acc: 0.050 - ETA: 643s - loss: 4.5614 - acc: 0.050 - ETA: 640s - loss: 4.5594 - acc: 0.051 - ETA: 636s - loss: 4.5577 - acc: 0.050 - ETA: 633s - loss: 4.5592 - acc: 0.050 - ETA: 629s - loss: 4.5586 - acc: 0.050 - ETA: 627s - loss: 4.5556 - acc: 0.050 - ETA: 624s - loss: 4.5578 - acc: 0.050 - ETA: 621s - loss: 4.5573 - acc: 0.051 - ETA: 617s - loss: 4.5581 - acc: 0.051 - ETA: 614s - loss: 4.5577 - acc: 0.051 - ETA: 611s - loss: 4.5565 - acc: 0.051 - ETA: 608s - loss: 4.5561 - acc: 0.051 - ETA: 605s - loss: 4.5536 - acc: 0.051 - ETA: 602s - loss: 4.5515 - acc: 0.051 - ETA: 598s - loss: 4.5505 - acc: 0.051 - ETA: 595s - loss: 4.5514 - acc: 0.051 - ETA: 592s - loss: 4.5489 - acc: 0.051 - ETA: 589s - loss: 4.5473 - acc: 0.051 - ETA: 586s - loss: 4.5461 - acc: 0.052 - ETA: 583s - loss: 4.5439 - acc: 0.052 - ETA: 580s - loss: 4.5480 - acc: 0.052 - ETA: 576s - loss: 4.5490 - acc: 0.051 - ETA: 574s - loss: 4.5474 - acc: 0.052 - ETA: 571s - loss: 4.5482 - acc: 0.052 - ETA: 567s - loss: 4.5478 - acc: 0.052 - ETA: 564s - loss: 4.5457 - acc: 0.053 - ETA: 561s - loss: 4.5446 - acc: 0.053 - ETA: 558s - loss: 4.5417 - acc: 0.053 - ETA: 555s - loss: 4.5401 - acc: 0.054 - ETA: 552s - loss: 4.5389 - acc: 0.054 - ETA: 548s - loss: 4.5365 - acc: 0.054 - ETA: 545s - loss: 4.5342 - acc: 0.055 - ETA: 542s - loss: 4.5354 - acc: 0.055 - ETA: 539s - loss: 4.5348 - acc: 0.054 - ETA: 536s - loss: 4.5353 - acc: 0.054 - ETA: 533s - loss: 4.5337 - acc: 0.054 - ETA: 529s - loss: 4.5326 - acc: 0.054 - ETA: 526s - loss: 4.5320 - acc: 0.054 - ETA: 523s - loss: 4.5305 - acc: 0.055 - ETA: 520s - loss: 4.5311 - acc: 0.055 - ETA: 517s - loss: 4.5299 - acc: 0.055 - ETA: 514s - loss: 4.5281 - acc: 0.055 - ETA: 511s - loss: 4.5260 - acc: 0.055 - ETA: 507s - loss: 4.5235 - acc: 0.055 - ETA: 504s - loss: 4.5227 - acc: 0.055 - ETA: 502s - loss: 4.5224 - acc: 0.054 - ETA: 499s - loss: 4.5209 - acc: 0.054 - ETA: 496s - loss: 4.5198 - acc: 0.054 - ETA: 492s - loss: 4.5214 - acc: 0.054 - ETA: 489s - loss: 4.5198 - acc: 0.053 - ETA: 486s - loss: 4.5208 - acc: 0.053 - ETA: 484s - loss: 4.5187 - acc: 0.053 - ETA: 481s - loss: 4.5193 - acc: 0.053 - ETA: 478s - loss: 4.5160 - acc: 0.053 - ETA: 474s - loss: 4.5150 - acc: 0.053 - ETA: 471s - loss: 4.5149 - acc: 0.053 - ETA: 469s - loss: 4.5143 - acc: 0.052 - ETA: 466s - loss: 4.5164 - acc: 0.053 - ETA: 463s - loss: 4.5162 - acc: 0.052 - ETA: 460s - loss: 4.5156 - acc: 0.052 - ETA: 457s - loss: 4.5159 - acc: 0.052 - ETA: 454s - loss: 4.5139 - acc: 0.052 - ETA: 451s - loss: 4.5115 - acc: 0.053 - ETA: 448s - loss: 4.5129 - acc: 0.053 - ETA: 445s - loss: 4.5123 - acc: 0.053 - ETA: 442s - loss: 4.5142 - acc: 0.053 - ETA: 439s - loss: 4.5150 - acc: 0.053 - ETA: 436s - loss: 4.5162 - acc: 0.053 - ETA: 433s - loss: 4.5143 - acc: 0.053 - ETA: 430s - loss: 4.5129 - acc: 0.053 - ETA: 427s - loss: 4.5116 - acc: 0.053 - ETA: 424s - loss: 4.5130 - acc: 0.054 - ETA: 421s - loss: 4.5143 - acc: 0.053 - ETA: 419s - loss: 4.5129 - acc: 0.053 - ETA: 416s - loss: 4.5120 - acc: 0.053 - ETA: 413s - loss: 4.5096 - acc: 0.053 - ETA: 410s - loss: 4.5105 - acc: 0.053 - ETA: 408s - loss: 4.5104 - acc: 0.053 - ETA: 405s - loss: 4.5103 - acc: 0.053 - ETA: 402s - loss: 4.5105 - acc: 0.053 - ETA: 399s - loss: 4.5112 - acc: 0.053 - ETA: 396s - loss: 4.5119 - acc: 0.053 - ETA: 393s - loss: 4.5094 - acc: 0.053 - ETA: 390s - loss: 4.5101 - acc: 0.052 - ETA: 387s - loss: 4.5084 - acc: 0.052 - ETA: 383s - loss: 4.5081 - acc: 0.052 - ETA: 380s - loss: 4.5084 - acc: 0.052 - ETA: 377s - loss: 4.5088 - acc: 0.0522"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 373s - loss: 4.5090 - acc: 0.052 - ETA: 370s - loss: 4.5087 - acc: 0.052 - ETA: 367s - loss: 4.5081 - acc: 0.052 - ETA: 364s - loss: 4.5081 - acc: 0.052 - ETA: 361s - loss: 4.5089 - acc: 0.052 - ETA: 357s - loss: 4.5106 - acc: 0.053 - ETA: 354s - loss: 4.5094 - acc: 0.052 - ETA: 351s - loss: 4.5094 - acc: 0.052 - ETA: 348s - loss: 4.5076 - acc: 0.052 - ETA: 345s - loss: 4.5050 - acc: 0.053 - ETA: 342s - loss: 4.5056 - acc: 0.053 - ETA: 338s - loss: 4.5050 - acc: 0.053 - ETA: 335s - loss: 4.5060 - acc: 0.053 - ETA: 332s - loss: 4.5035 - acc: 0.053 - ETA: 329s - loss: 4.5007 - acc: 0.053 - ETA: 326s - loss: 4.4983 - acc: 0.054 - ETA: 323s - loss: 4.4999 - acc: 0.054 - ETA: 320s - loss: 4.5004 - acc: 0.053 - ETA: 317s - loss: 4.5003 - acc: 0.053 - ETA: 314s - loss: 4.4989 - acc: 0.053 - ETA: 311s - loss: 4.5006 - acc: 0.053 - ETA: 308s - loss: 4.4997 - acc: 0.053 - ETA: 305s - loss: 4.4995 - acc: 0.053 - ETA: 302s - loss: 4.4994 - acc: 0.053 - ETA: 299s - loss: 4.4991 - acc: 0.053 - ETA: 296s - loss: 4.4985 - acc: 0.053 - ETA: 293s - loss: 4.4989 - acc: 0.053 - ETA: 290s - loss: 4.4976 - acc: 0.053 - ETA: 287s - loss: 4.4965 - acc: 0.053 - ETA: 284s - loss: 4.4969 - acc: 0.053 - ETA: 281s - loss: 4.5001 - acc: 0.052 - ETA: 278s - loss: 4.4996 - acc: 0.052 - ETA: 275s - loss: 4.4980 - acc: 0.052 - ETA: 272s - loss: 4.4967 - acc: 0.052 - ETA: 269s - loss: 4.4968 - acc: 0.052 - ETA: 266s - loss: 4.4957 - acc: 0.053 - ETA: 263s - loss: 4.4958 - acc: 0.053 - ETA: 260s - loss: 4.4959 - acc: 0.053 - ETA: 257s - loss: 4.4952 - acc: 0.053 - ETA: 254s - loss: 4.4948 - acc: 0.053 - ETA: 251s - loss: 4.4936 - acc: 0.053 - ETA: 249s - loss: 4.4941 - acc: 0.053 - ETA: 246s - loss: 4.4939 - acc: 0.053 - ETA: 243s - loss: 4.4934 - acc: 0.053 - ETA: 240s - loss: 4.4920 - acc: 0.054 - ETA: 237s - loss: 4.4923 - acc: 0.054 - ETA: 234s - loss: 4.4921 - acc: 0.054 - ETA: 231s - loss: 4.4934 - acc: 0.054 - ETA: 228s - loss: 4.4917 - acc: 0.054 - ETA: 225s - loss: 4.4915 - acc: 0.053 - ETA: 222s - loss: 4.4896 - acc: 0.054 - ETA: 219s - loss: 4.4894 - acc: 0.054 - ETA: 216s - loss: 4.4906 - acc: 0.054 - ETA: 213s - loss: 4.4902 - acc: 0.054 - ETA: 210s - loss: 4.4900 - acc: 0.054 - ETA: 207s - loss: 4.4894 - acc: 0.054 - ETA: 204s - loss: 4.4897 - acc: 0.054 - ETA: 202s - loss: 4.4890 - acc: 0.054 - ETA: 199s - loss: 4.4883 - acc: 0.054 - ETA: 196s - loss: 4.4881 - acc: 0.054 - ETA: 193s - loss: 4.4854 - acc: 0.054 - ETA: 190s - loss: 4.4827 - acc: 0.055 - ETA: 187s - loss: 4.4823 - acc: 0.055 - ETA: 184s - loss: 4.4816 - acc: 0.055 - ETA: 181s - loss: 4.4811 - acc: 0.055 - ETA: 178s - loss: 4.4816 - acc: 0.055 - ETA: 175s - loss: 4.4805 - acc: 0.055 - ETA: 173s - loss: 4.4797 - acc: 0.055 - ETA: 170s - loss: 4.4797 - acc: 0.055 - ETA: 167s - loss: 4.4792 - acc: 0.055 - ETA: 164s - loss: 4.4783 - acc: 0.055 - ETA: 161s - loss: 4.4792 - acc: 0.055 - ETA: 158s - loss: 4.4771 - acc: 0.054 - ETA: 155s - loss: 4.4776 - acc: 0.054 - ETA: 153s - loss: 4.4780 - acc: 0.054 - ETA: 150s - loss: 4.4759 - acc: 0.055 - ETA: 147s - loss: 4.4750 - acc: 0.054 - ETA: 144s - loss: 4.4745 - acc: 0.054 - ETA: 141s - loss: 4.4742 - acc: 0.054 - ETA: 138s - loss: 4.4742 - acc: 0.054 - ETA: 136s - loss: 4.4744 - acc: 0.054 - ETA: 133s - loss: 4.4752 - acc: 0.054 - ETA: 130s - loss: 4.4744 - acc: 0.054 - ETA: 127s - loss: 4.4749 - acc: 0.054 - ETA: 124s - loss: 4.4746 - acc: 0.054 - ETA: 121s - loss: 4.4757 - acc: 0.054 - ETA: 119s - loss: 4.4752 - acc: 0.054 - ETA: 116s - loss: 4.4739 - acc: 0.054 - ETA: 113s - loss: 4.4718 - acc: 0.054 - ETA: 110s - loss: 4.4725 - acc: 0.054 - ETA: 107s - loss: 4.4713 - acc: 0.054 - ETA: 105s - loss: 4.4727 - acc: 0.053 - ETA: 102s - loss: 4.4734 - acc: 0.053 - ETA: 99s - loss: 4.4724 - acc: 0.054 - ETA: 96s - loss: 4.4723 - acc: 0.05 - ETA: 93s - loss: 4.4721 - acc: 0.05 - ETA: 91s - loss: 4.4716 - acc: 0.05 - ETA: 88s - loss: 4.4712 - acc: 0.05 - ETA: 85s - loss: 4.4710 - acc: 0.05 - ETA: 82s - loss: 4.4710 - acc: 0.05 - ETA: 79s - loss: 4.4702 - acc: 0.05 - ETA: 77s - loss: 4.4699 - acc: 0.05 - ETA: 74s - loss: 4.4709 - acc: 0.05 - ETA: 71s - loss: 4.4701 - acc: 0.05 - ETA: 68s - loss: 4.4704 - acc: 0.05 - ETA: 65s - loss: 4.4687 - acc: 0.05 - ETA: 63s - loss: 4.4694 - acc: 0.05 - ETA: 60s - loss: 4.4696 - acc: 0.05 - ETA: 57s - loss: 4.4687 - acc: 0.05 - ETA: 54s - loss: 4.4710 - acc: 0.05 - ETA: 52s - loss: 4.4712 - acc: 0.05 - ETA: 49s - loss: 4.4708 - acc: 0.05 - ETA: 46s - loss: 4.4707 - acc: 0.05 - ETA: 43s - loss: 4.4699 - acc: 0.05 - ETA: 41s - loss: 4.4704 - acc: 0.05 - ETA: 38s - loss: 4.4694 - acc: 0.05 - ETA: 35s - loss: 4.4672 - acc: 0.05 - ETA: 32s - loss: 4.4659 - acc: 0.05 - ETA: 30s - loss: 4.4656 - acc: 0.05 - ETA: 27s - loss: 4.4641 - acc: 0.05 - ETA: 24s - loss: 4.4641 - acc: 0.05 - ETA: 21s - loss: 4.4632 - acc: 0.05 - ETA: 19s - loss: 4.4611 - acc: 0.05 - ETA: 16s - loss: 4.4614 - acc: 0.05 - ETA: 13s - loss: 4.4603 - acc: 0.05 - ETA: 10s - loss: 4.4604 - acc: 0.05 - ETA: 8s - loss: 4.4599 - acc: 0.0559 - ETA: 5s - loss: 4.4584 - acc: 0.055 - ETA: 2s - loss: 4.4569 - acc: 0.0563Epoch 00001: val_loss improved from 4.63898 to 4.39548, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 951s - loss: 4.4562 - acc: 0.0566 - val_loss: 4.3955 - val_acc: 0.0587\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 719s - loss: 3.6564 - acc: 0.200 - ETA: 730s - loss: 4.1216 - acc: 0.150 - ETA: 745s - loss: 3.9505 - acc: 0.133 - ETA: 750s - loss: 3.9057 - acc: 0.112 - ETA: 758s - loss: 4.1084 - acc: 0.100 - ETA: 766s - loss: 4.0817 - acc: 0.116 - ETA: 799s - loss: 4.0268 - acc: 0.114 - ETA: 795s - loss: 4.0307 - acc: 0.112 - ETA: 796s - loss: 3.9918 - acc: 0.122 - ETA: 796s - loss: 4.0408 - acc: 0.115 - ETA: 790s - loss: 4.0011 - acc: 0.113 - ETA: 788s - loss: 4.0092 - acc: 0.104 - ETA: 796s - loss: 4.0083 - acc: 0.100 - ETA: 793s - loss: 4.0068 - acc: 0.107 - ETA: 790s - loss: 3.9892 - acc: 0.110 - ETA: 785s - loss: 3.9935 - acc: 0.106 - ETA: 780s - loss: 3.9976 - acc: 0.105 - ETA: 780s - loss: 3.9777 - acc: 0.105 - ETA: 774s - loss: 3.9769 - acc: 0.113 - ETA: 782s - loss: 3.9602 - acc: 0.117 - ETA: 775s - loss: 3.9521 - acc: 0.119 - ETA: 769s - loss: 3.9587 - acc: 0.120 - ETA: 766s - loss: 3.9294 - acc: 0.126 - ETA: 763s - loss: 3.9238 - acc: 0.129 - ETA: 764s - loss: 3.9165 - acc: 0.132 - ETA: 761s - loss: 3.9269 - acc: 0.128 - ETA: 760s - loss: 3.9315 - acc: 0.125 - ETA: 755s - loss: 3.9234 - acc: 0.125 - ETA: 750s - loss: 3.9311 - acc: 0.124 - ETA: 746s - loss: 3.9233 - acc: 0.126 - ETA: 743s - loss: 3.9239 - acc: 0.127 - ETA: 740s - loss: 3.9174 - acc: 0.128 - ETA: 741s - loss: 3.9201 - acc: 0.128 - ETA: 738s - loss: 3.9036 - acc: 0.132 - ETA: 736s - loss: 3.8886 - acc: 0.135 - ETA: 733s - loss: 3.8856 - acc: 0.138 - ETA: 729s - loss: 3.8784 - acc: 0.141 - ETA: 726s - loss: 3.9482 - acc: 0.138 - ETA: 725s - loss: 3.9346 - acc: 0.142 - ETA: 723s - loss: 3.9200 - acc: 0.146 - ETA: 721s - loss: 3.9273 - acc: 0.146 - ETA: 717s - loss: 3.9171 - acc: 0.142 - ETA: 713s - loss: 3.9128 - acc: 0.140 - ETA: 710s - loss: 3.9185 - acc: 0.139 - ETA: 707s - loss: 3.9071 - acc: 0.142 - ETA: 707s - loss: 3.9105 - acc: 0.140 - ETA: 703s - loss: 3.9121 - acc: 0.141 - ETA: 700s - loss: 3.8982 - acc: 0.143 - ETA: 697s - loss: 3.8909 - acc: 0.142 - ETA: 694s - loss: 3.8856 - acc: 0.143 - ETA: 692s - loss: 3.8774 - acc: 0.145 - ETA: 689s - loss: 3.8711 - acc: 0.145 - ETA: 689s - loss: 3.8704 - acc: 0.146 - ETA: 687s - loss: 3.8685 - acc: 0.145 - ETA: 682s - loss: 3.8710 - acc: 0.146 - ETA: 679s - loss: 3.8637 - acc: 0.146 - ETA: 676s - loss: 3.8606 - acc: 0.146 - ETA: 673s - loss: 3.8531 - acc: 0.147 - ETA: 671s - loss: 3.8492 - acc: 0.147 - ETA: 671s - loss: 3.8473 - acc: 0.146 - ETA: 668s - loss: 3.8473 - acc: 0.145 - ETA: 665s - loss: 3.8377 - acc: 0.148 - ETA: 663s - loss: 3.8361 - acc: 0.149 - ETA: 660s - loss: 3.8366 - acc: 0.147 - ETA: 658s - loss: 3.8448 - acc: 0.147 - ETA: 656s - loss: 3.8468 - acc: 0.147 - ETA: 653s - loss: 3.8329 - acc: 0.150 - ETA: 650s - loss: 3.8310 - acc: 0.150 - ETA: 647s - loss: 3.8351 - acc: 0.148 - ETA: 644s - loss: 3.8326 - acc: 0.150 - ETA: 641s - loss: 3.8290 - acc: 0.150 - ETA: 640s - loss: 3.8259 - acc: 0.151 - ETA: 637s - loss: 3.8292 - acc: 0.152 - ETA: 634s - loss: 3.8391 - acc: 0.152 - ETA: 630s - loss: 3.8340 - acc: 0.152 - ETA: 627s - loss: 3.8258 - acc: 0.153 - ETA: 624s - loss: 3.8297 - acc: 0.151 - ETA: 621s - loss: 3.8363 - acc: 0.150 - ETA: 619s - loss: 3.8347 - acc: 0.151 - ETA: 616s - loss: 3.8308 - acc: 0.153 - ETA: 613s - loss: 3.8321 - acc: 0.151 - ETA: 610s - loss: 3.8326 - acc: 0.151 - ETA: 608s - loss: 3.8296 - acc: 0.152 - ETA: 605s - loss: 3.8213 - acc: 0.154 - ETA: 604s - loss: 3.8181 - acc: 0.154 - ETA: 602s - loss: 3.8214 - acc: 0.154 - ETA: 599s - loss: 3.8177 - acc: 0.154 - ETA: 596s - loss: 3.8125 - acc: 0.155 - ETA: 595s - loss: 3.8119 - acc: 0.153 - ETA: 592s - loss: 3.8109 - acc: 0.152 - ETA: 589s - loss: 3.8052 - acc: 0.154 - ETA: 586s - loss: 3.8069 - acc: 0.154 - ETA: 585s - loss: 3.8058 - acc: 0.153 - ETA: 583s - loss: 3.8025 - acc: 0.154 - ETA: 580s - loss: 3.8028 - acc: 0.153 - ETA: 578s - loss: 3.8035 - acc: 0.153 - ETA: 575s - loss: 3.8005 - acc: 0.155 - ETA: 572s - loss: 3.7940 - acc: 0.156 - ETA: 571s - loss: 3.7985 - acc: 0.155 - ETA: 569s - loss: 3.8033 - acc: 0.155 - ETA: 566s - loss: 3.8027 - acc: 0.156 - ETA: 563s - loss: 3.8065 - acc: 0.155 - ETA: 560s - loss: 3.8042 - acc: 0.156 - ETA: 558s - loss: 3.8073 - acc: 0.155 - ETA: 556s - loss: 3.8055 - acc: 0.155 - ETA: 554s - loss: 3.8025 - acc: 0.156 - ETA: 551s - loss: 3.8050 - acc: 0.157 - ETA: 548s - loss: 3.8047 - acc: 0.156 - ETA: 546s - loss: 3.8040 - acc: 0.157 - ETA: 543s - loss: 3.8021 - acc: 0.157 - ETA: 541s - loss: 3.8035 - acc: 0.157 - ETA: 540s - loss: 3.7962 - acc: 0.158 - ETA: 538s - loss: 3.7971 - acc: 0.157 - ETA: 536s - loss: 3.7894 - acc: 0.159 - ETA: 534s - loss: 3.7905 - acc: 0.159 - ETA: 532s - loss: 3.7933 - acc: 0.158 - ETA: 531s - loss: 3.8029 - acc: 0.157 - ETA: 531s - loss: 3.8049 - acc: 0.157 - ETA: 531s - loss: 3.8016 - acc: 0.156 - ETA: 530s - loss: 3.8026 - acc: 0.156 - ETA: 528s - loss: 3.7993 - acc: 0.156 - ETA: 526s - loss: 3.8029 - acc: 0.155 - ETA: 524s - loss: 3.8054 - acc: 0.156 - ETA: 522s - loss: 3.8049 - acc: 0.156 - ETA: 519s - loss: 3.8074 - acc: 0.156 - ETA: 517s - loss: 3.8074 - acc: 0.156 - ETA: 514s - loss: 3.8091 - acc: 0.156 - ETA: 511s - loss: 3.8096 - acc: 0.155 - ETA: 510s - loss: 3.8105 - acc: 0.155 - ETA: 507s - loss: 3.8088 - acc: 0.155 - ETA: 505s - loss: 3.8123 - acc: 0.155 - ETA: 502s - loss: 3.8114 - acc: 0.154 - ETA: 500s - loss: 3.8128 - acc: 0.154 - ETA: 497s - loss: 3.8143 - acc: 0.155 - ETA: 495s - loss: 3.8119 - acc: 0.155 - ETA: 493s - loss: 3.8086 - acc: 0.155 - ETA: 491s - loss: 3.8098 - acc: 0.155 - ETA: 488s - loss: 3.8109 - acc: 0.155 - ETA: 485s - loss: 3.8062 - acc: 0.155 - ETA: 483s - loss: 3.8080 - acc: 0.156 - ETA: 480s - loss: 3.8095 - acc: 0.156 - ETA: 478s - loss: 3.8110 - acc: 0.155 - ETA: 475s - loss: 3.8152 - acc: 0.155 - ETA: 472s - loss: 3.8114 - acc: 0.154 - ETA: 470s - loss: 3.8110 - acc: 0.154 - ETA: 467s - loss: 3.8123 - acc: 0.154 - ETA: 464s - loss: 3.8104 - acc: 0.154 - ETA: 462s - loss: 3.8066 - acc: 0.155 - ETA: 459s - loss: 3.8115 - acc: 0.155 - ETA: 457s - loss: 3.8104 - acc: 0.155 - ETA: 454s - loss: 3.8145 - acc: 0.155 - ETA: 451s - loss: 3.8165 - acc: 0.155 - ETA: 449s - loss: 3.8139 - acc: 0.156 - ETA: 446s - loss: 3.8125 - acc: 0.156 - ETA: 444s - loss: 3.8134 - acc: 0.156 - ETA: 442s - loss: 3.8161 - acc: 0.156 - ETA: 439s - loss: 3.8139 - acc: 0.156 - ETA: 436s - loss: 3.8138 - acc: 0.156 - ETA: 434s - loss: 3.8133 - acc: 0.156 - ETA: 432s - loss: 3.8125 - acc: 0.155 - ETA: 429s - loss: 3.8092 - acc: 0.156 - ETA: 427s - loss: 3.8054 - acc: 0.156 - ETA: 424s - loss: 3.8063 - acc: 0.156 - ETA: 422s - loss: 3.8043 - acc: 0.156 - ETA: 420s - loss: 3.8036 - acc: 0.156 - ETA: 417s - loss: 3.8083 - acc: 0.155 - ETA: 415s - loss: 3.8049 - acc: 0.156 - ETA: 413s - loss: 3.8050 - acc: 0.156 - ETA: 411s - loss: 3.8048 - acc: 0.155 - ETA: 408s - loss: 3.8055 - acc: 0.155 - ETA: 406s - loss: 3.8051 - acc: 0.155 - ETA: 403s - loss: 3.8038 - acc: 0.155 - ETA: 401s - loss: 3.8015 - acc: 0.155 - ETA: 399s - loss: 3.8022 - acc: 0.155 - ETA: 396s - loss: 3.8026 - acc: 0.155 - ETA: 394s - loss: 3.7996 - acc: 0.155 - ETA: 392s - loss: 3.7965 - acc: 0.156 - ETA: 389s - loss: 3.7960 - acc: 0.156 - ETA: 387s - loss: 3.7965 - acc: 0.155 - ETA: 385s - loss: 3.7986 - acc: 0.155 - ETA: 382s - loss: 3.8013 - acc: 0.155 - ETA: 379s - loss: 3.7996 - acc: 0.155 - ETA: 377s - loss: 3.7999 - acc: 0.156 - ETA: 374s - loss: 3.7979 - acc: 0.155 - ETA: 372s - loss: 3.7960 - acc: 0.156 - ETA: 370s - loss: 3.7980 - acc: 0.155 - ETA: 367s - loss: 3.7973 - acc: 0.156 - ETA: 364s - loss: 3.7974 - acc: 0.156 - ETA: 362s - loss: 3.7977 - acc: 0.156 - ETA: 359s - loss: 3.7970 - acc: 0.156 - ETA: 357s - loss: 3.7980 - acc: 0.156 - ETA: 354s - loss: 3.7988 - acc: 0.156 - ETA: 352s - loss: 3.7959 - acc: 0.156 - ETA: 350s - loss: 3.7949 - acc: 0.156 - ETA: 347s - loss: 3.7976 - acc: 0.156 - ETA: 345s - loss: 3.7980 - acc: 0.157 - ETA: 342s - loss: 3.7999 - acc: 0.157 - ETA: 340s - loss: 3.7999 - acc: 0.156 - ETA: 337s - loss: 3.8003 - acc: 0.156 - ETA: 335s - loss: 3.7991 - acc: 0.157 - ETA: 332s - loss: 3.7987 - acc: 0.157 - ETA: 329s - loss: 3.7995 - acc: 0.157 - ETA: 327s - loss: 3.7978 - acc: 0.157 - ETA: 324s - loss: 3.7990 - acc: 0.1581"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 322s - loss: 3.7998 - acc: 0.157 - ETA: 320s - loss: 3.7986 - acc: 0.158 - ETA: 317s - loss: 3.8006 - acc: 0.158 - ETA: 315s - loss: 3.7986 - acc: 0.158 - ETA: 312s - loss: 3.7990 - acc: 0.158 - ETA: 310s - loss: 3.7985 - acc: 0.159 - ETA: 308s - loss: 3.7949 - acc: 0.159 - ETA: 305s - loss: 3.7922 - acc: 0.159 - ETA: 303s - loss: 3.7916 - acc: 0.160 - ETA: 300s - loss: 3.7915 - acc: 0.160 - ETA: 297s - loss: 3.7924 - acc: 0.160 - ETA: 295s - loss: 3.7898 - acc: 0.160 - ETA: 293s - loss: 3.7886 - acc: 0.160 - ETA: 290s - loss: 3.7871 - acc: 0.160 - ETA: 288s - loss: 3.7860 - acc: 0.160 - ETA: 285s - loss: 3.7879 - acc: 0.160 - ETA: 282s - loss: 3.7884 - acc: 0.159 - ETA: 280s - loss: 3.7892 - acc: 0.159 - ETA: 277s - loss: 3.7893 - acc: 0.159 - ETA: 275s - loss: 3.7893 - acc: 0.160 - ETA: 272s - loss: 3.7897 - acc: 0.160 - ETA: 270s - loss: 3.7889 - acc: 0.160 - ETA: 267s - loss: 3.7883 - acc: 0.160 - ETA: 265s - loss: 3.7905 - acc: 0.160 - ETA: 262s - loss: 3.7913 - acc: 0.160 - ETA: 259s - loss: 3.7909 - acc: 0.159 - ETA: 257s - loss: 3.7962 - acc: 0.159 - ETA: 255s - loss: 3.7959 - acc: 0.159 - ETA: 252s - loss: 3.7989 - acc: 0.159 - ETA: 249s - loss: 3.8001 - acc: 0.158 - ETA: 247s - loss: 3.7992 - acc: 0.158 - ETA: 244s - loss: 3.7995 - acc: 0.158 - ETA: 242s - loss: 3.7986 - acc: 0.158 - ETA: 239s - loss: 3.7973 - acc: 0.158 - ETA: 237s - loss: 3.7985 - acc: 0.158 - ETA: 234s - loss: 3.7977 - acc: 0.159 - ETA: 232s - loss: 3.7983 - acc: 0.158 - ETA: 229s - loss: 3.7974 - acc: 0.159 - ETA: 227s - loss: 3.7972 - acc: 0.159 - ETA: 225s - loss: 3.7986 - acc: 0.159 - ETA: 222s - loss: 3.7979 - acc: 0.159 - ETA: 220s - loss: 3.7985 - acc: 0.159 - ETA: 218s - loss: 3.7970 - acc: 0.159 - ETA: 215s - loss: 3.7966 - acc: 0.159 - ETA: 213s - loss: 3.7957 - acc: 0.159 - ETA: 211s - loss: 3.7947 - acc: 0.160 - ETA: 208s - loss: 3.7931 - acc: 0.160 - ETA: 206s - loss: 3.7926 - acc: 0.160 - ETA: 203s - loss: 3.7941 - acc: 0.160 - ETA: 200s - loss: 3.7952 - acc: 0.160 - ETA: 198s - loss: 3.7956 - acc: 0.160 - ETA: 196s - loss: 3.7932 - acc: 0.159 - ETA: 193s - loss: 3.7917 - acc: 0.159 - ETA: 190s - loss: 3.7923 - acc: 0.159 - ETA: 188s - loss: 3.7951 - acc: 0.159 - ETA: 185s - loss: 3.7966 - acc: 0.159 - ETA: 183s - loss: 3.7959 - acc: 0.159 - ETA: 181s - loss: 3.7956 - acc: 0.159 - ETA: 178s - loss: 3.7967 - acc: 0.159 - ETA: 175s - loss: 3.8001 - acc: 0.159 - ETA: 173s - loss: 3.8000 - acc: 0.159 - ETA: 170s - loss: 3.8004 - acc: 0.159 - ETA: 168s - loss: 3.7978 - acc: 0.160 - ETA: 166s - loss: 3.7960 - acc: 0.160 - ETA: 163s - loss: 3.7970 - acc: 0.159 - ETA: 160s - loss: 3.7958 - acc: 0.159 - ETA: 158s - loss: 3.7950 - acc: 0.160 - ETA: 155s - loss: 3.7957 - acc: 0.159 - ETA: 153s - loss: 3.7949 - acc: 0.159 - ETA: 150s - loss: 3.7962 - acc: 0.159 - ETA: 148s - loss: 3.7953 - acc: 0.160 - ETA: 145s - loss: 3.7990 - acc: 0.160 - ETA: 143s - loss: 3.8009 - acc: 0.159 - ETA: 140s - loss: 3.8001 - acc: 0.159 - ETA: 138s - loss: 3.8017 - acc: 0.159 - ETA: 135s - loss: 3.8041 - acc: 0.159 - ETA: 133s - loss: 3.8047 - acc: 0.159 - ETA: 130s - loss: 3.8045 - acc: 0.158 - ETA: 128s - loss: 3.8044 - acc: 0.158 - ETA: 125s - loss: 3.8043 - acc: 0.159 - ETA: 123s - loss: 3.8038 - acc: 0.159 - ETA: 120s - loss: 3.8042 - acc: 0.158 - ETA: 118s - loss: 3.8051 - acc: 0.158 - ETA: 115s - loss: 3.8053 - acc: 0.157 - ETA: 113s - loss: 3.8054 - acc: 0.157 - ETA: 110s - loss: 3.8064 - acc: 0.157 - ETA: 108s - loss: 3.8093 - acc: 0.156 - ETA: 105s - loss: 3.8080 - acc: 0.156 - ETA: 103s - loss: 3.8084 - acc: 0.156 - ETA: 100s - loss: 3.8057 - acc: 0.157 - ETA: 98s - loss: 3.8092 - acc: 0.156 - ETA: 95s - loss: 3.8097 - acc: 0.15 - ETA: 93s - loss: 3.8103 - acc: 0.15 - ETA: 90s - loss: 3.8100 - acc: 0.15 - ETA: 88s - loss: 3.8091 - acc: 0.15 - ETA: 85s - loss: 3.8106 - acc: 0.15 - ETA: 83s - loss: 3.8132 - acc: 0.15 - ETA: 80s - loss: 3.8101 - acc: 0.15 - ETA: 78s - loss: 3.8104 - acc: 0.15 - ETA: 75s - loss: 3.8094 - acc: 0.15 - ETA: 73s - loss: 3.8134 - acc: 0.15 - ETA: 70s - loss: 3.8128 - acc: 0.15 - ETA: 68s - loss: 3.8168 - acc: 0.15 - ETA: 65s - loss: 3.8183 - acc: 0.15 - ETA: 62s - loss: 3.8195 - acc: 0.15 - ETA: 60s - loss: 3.8181 - acc: 0.15 - ETA: 57s - loss: 3.8208 - acc: 0.15 - ETA: 55s - loss: 3.8211 - acc: 0.15 - ETA: 52s - loss: 3.8217 - acc: 0.15 - ETA: 50s - loss: 3.8198 - acc: 0.15 - ETA: 47s - loss: 3.8191 - acc: 0.15 - ETA: 45s - loss: 3.8183 - acc: 0.15 - ETA: 42s - loss: 3.8188 - acc: 0.15 - ETA: 40s - loss: 3.8195 - acc: 0.15 - ETA: 37s - loss: 3.8176 - acc: 0.15 - ETA: 35s - loss: 3.8189 - acc: 0.15 - ETA: 32s - loss: 3.8190 - acc: 0.15 - ETA: 30s - loss: 3.8196 - acc: 0.15 - ETA: 27s - loss: 3.8188 - acc: 0.15 - ETA: 25s - loss: 3.8170 - acc: 0.15 - ETA: 22s - loss: 3.8161 - acc: 0.15 - ETA: 20s - loss: 3.8139 - acc: 0.15 - ETA: 17s - loss: 3.8149 - acc: 0.15 - ETA: 15s - loss: 3.8144 - acc: 0.15 - ETA: 12s - loss: 3.8129 - acc: 0.15 - ETA: 10s - loss: 3.8119 - acc: 0.15 - ETA: 7s - loss: 3.8129 - acc: 0.1560 - ETA: 5s - loss: 3.8148 - acc: 0.156 - ETA: 2s - loss: 3.8138 - acc: 0.1563Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 875s - loss: 3.8142 - acc: 0.1558 - val_loss: 4.4939 - val_acc: 0.0743\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 867s - loss: 3.1502 - acc: 0.400 - ETA: 936s - loss: 2.8170 - acc: 0.450 - ETA: 892s - loss: 3.3041 - acc: 0.400 - ETA: 861s - loss: 4.4099 - acc: 0.312 - ETA: 844s - loss: 4.1517 - acc: 0.310 - ETA: 836s - loss: 3.9964 - acc: 0.300 - ETA: 830s - loss: 3.8083 - acc: 0.342 - ETA: 841s - loss: 3.6512 - acc: 0.343 - ETA: 832s - loss: 3.5546 - acc: 0.327 - ETA: 833s - loss: 3.4219 - acc: 0.340 - ETA: 826s - loss: 3.4041 - acc: 0.327 - ETA: 831s - loss: 3.3101 - acc: 0.333 - ETA: 821s - loss: 3.2482 - acc: 0.334 - ETA: 828s - loss: 3.2171 - acc: 0.332 - ETA: 832s - loss: 3.2143 - acc: 0.323 - ETA: 827s - loss: 3.1676 - acc: 0.334 - ETA: 823s - loss: 3.1752 - acc: 0.332 - ETA: 819s - loss: 3.2482 - acc: 0.316 - ETA: 814s - loss: 3.2090 - acc: 0.321 - ETA: 814s - loss: 3.1654 - acc: 0.327 - ETA: 820s - loss: 3.1707 - acc: 0.326 - ETA: 815s - loss: 3.1298 - acc: 0.331 - ETA: 811s - loss: 3.1537 - acc: 0.330 - ETA: 806s - loss: 3.1277 - acc: 0.335 - ETA: 807s - loss: 3.1135 - acc: 0.332 - ETA: 809s - loss: 3.0992 - acc: 0.336 - ETA: 813s - loss: 3.0736 - acc: 0.338 - ETA: 810s - loss: 3.0652 - acc: 0.341 - ETA: 807s - loss: 3.0469 - acc: 0.339 - ETA: 806s - loss: 3.0307 - acc: 0.341 - ETA: 804s - loss: 3.0285 - acc: 0.341 - ETA: 811s - loss: 3.0142 - acc: 0.342 - ETA: 814s - loss: 3.0379 - acc: 0.337 - ETA: 818s - loss: 3.0331 - acc: 0.335 - ETA: 815s - loss: 3.0456 - acc: 0.331 - ETA: 813s - loss: 3.0498 - acc: 0.329 - ETA: 811s - loss: 3.0350 - acc: 0.328 - ETA: 809s - loss: 3.0257 - acc: 0.326 - ETA: 804s - loss: 3.0134 - acc: 0.324 - ETA: 799s - loss: 3.0055 - acc: 0.327 - ETA: 794s - loss: 3.0012 - acc: 0.325 - ETA: 790s - loss: 2.9934 - acc: 0.326 - ETA: 790s - loss: 2.9944 - acc: 0.323 - ETA: 788s - loss: 2.9911 - acc: 0.321 - ETA: 784s - loss: 2.9759 - acc: 0.323 - ETA: 780s - loss: 2.9761 - acc: 0.320 - ETA: 776s - loss: 2.9672 - acc: 0.322 - ETA: 772s - loss: 2.9584 - acc: 0.324 - ETA: 770s - loss: 2.9525 - acc: 0.325 - ETA: 769s - loss: 2.9520 - acc: 0.325 - ETA: 766s - loss: 2.9468 - acc: 0.325 - ETA: 762s - loss: 2.9452 - acc: 0.324 - ETA: 759s - loss: 2.9448 - acc: 0.324 - ETA: 755s - loss: 2.9255 - acc: 0.327 - ETA: 754s - loss: 2.9147 - acc: 0.330 - ETA: 752s - loss: 2.9215 - acc: 0.328 - ETA: 749s - loss: 2.9243 - acc: 0.328 - ETA: 745s - loss: 2.9337 - acc: 0.325 - ETA: 741s - loss: 2.9332 - acc: 0.326 - ETA: 738s - loss: 2.9348 - acc: 0.324 - ETA: 735s - loss: 2.9205 - acc: 0.328 - ETA: 735s - loss: 2.9025 - acc: 0.331 - ETA: 732s - loss: 2.9046 - acc: 0.330 - ETA: 728s - loss: 2.8981 - acc: 0.332 - ETA: 725s - loss: 2.9074 - acc: 0.330 - ETA: 722s - loss: 2.9044 - acc: 0.330 - ETA: 720s - loss: 2.9123 - acc: 0.327 - ETA: 719s - loss: 2.9143 - acc: 0.325 - ETA: 715s - loss: 2.9127 - acc: 0.326 - ETA: 712s - loss: 2.9066 - acc: 0.327 - ETA: 708s - loss: 2.8972 - acc: 0.328 - ETA: 705s - loss: 2.8924 - acc: 0.327 - ETA: 704s - loss: 2.8844 - acc: 0.328 - ETA: 701s - loss: 2.8921 - acc: 0.328 - ETA: 697s - loss: 2.8966 - acc: 0.327 - ETA: 694s - loss: 2.8949 - acc: 0.326 - ETA: 690s - loss: 2.9013 - acc: 0.325 - ETA: 687s - loss: 2.9025 - acc: 0.325 - ETA: 685s - loss: 2.8899 - acc: 0.329 - ETA: 683s - loss: 2.8939 - acc: 0.328 - ETA: 679s - loss: 2.8974 - acc: 0.328 - ETA: 676s - loss: 2.8894 - acc: 0.329 - ETA: 673s - loss: 2.8741 - acc: 0.334 - ETA: 670s - loss: 2.8825 - acc: 0.333 - ETA: 669s - loss: 2.8825 - acc: 0.333 - ETA: 666s - loss: 2.8794 - acc: 0.334 - ETA: 663s - loss: 2.8787 - acc: 0.333 - ETA: 661s - loss: 2.8784 - acc: 0.333 - ETA: 659s - loss: 2.8725 - acc: 0.333 - ETA: 656s - loss: 2.8710 - acc: 0.334 - ETA: 653s - loss: 2.8726 - acc: 0.335 - ETA: 652s - loss: 2.8764 - acc: 0.335 - ETA: 648s - loss: 2.8736 - acc: 0.336 - ETA: 644s - loss: 2.8781 - acc: 0.335 - ETA: 641s - loss: 2.8757 - acc: 0.336 - ETA: 637s - loss: 2.8729 - acc: 0.337 - ETA: 636s - loss: 2.8679 - acc: 0.338 - ETA: 633s - loss: 2.8721 - acc: 0.337 - ETA: 630s - loss: 2.8660 - acc: 0.337 - ETA: 627s - loss: 2.8663 - acc: 0.337 - ETA: 624s - loss: 2.8793 - acc: 0.335 - ETA: 620s - loss: 2.8757 - acc: 0.335 - ETA: 617s - loss: 2.8740 - acc: 0.335 - ETA: 615s - loss: 2.8731 - acc: 0.337 - ETA: 611s - loss: 2.8761 - acc: 0.335 - ETA: 608s - loss: 2.8773 - acc: 0.334 - ETA: 605s - loss: 2.8763 - acc: 0.335 - ETA: 601s - loss: 2.8813 - acc: 0.334 - ETA: 598s - loss: 2.8778 - acc: 0.334 - ETA: 596s - loss: 2.8764 - acc: 0.335 - ETA: 592s - loss: 2.8792 - acc: 0.333 - ETA: 589s - loss: 2.8809 - acc: 0.333 - ETA: 587s - loss: 2.8837 - acc: 0.332 - ETA: 583s - loss: 2.8833 - acc: 0.332 - ETA: 580s - loss: 2.8939 - acc: 0.332 - ETA: 576s - loss: 2.8919 - acc: 0.332 - ETA: 574s - loss: 2.8907 - acc: 0.332 - ETA: 571s - loss: 2.8873 - acc: 0.333 - ETA: 568s - loss: 2.8812 - acc: 0.334 - ETA: 564s - loss: 2.8785 - acc: 0.335 - ETA: 561s - loss: 2.8877 - acc: 0.334 - ETA: 558s - loss: 2.8957 - acc: 0.334 - ETA: 555s - loss: 2.8913 - acc: 0.335 - ETA: 553s - loss: 2.8839 - acc: 0.338 - ETA: 550s - loss: 2.8793 - acc: 0.338 - ETA: 546s - loss: 2.8778 - acc: 0.338 - ETA: 543s - loss: 2.8725 - acc: 0.339 - ETA: 540s - loss: 2.8811 - acc: 0.338 - ETA: 537s - loss: 2.8844 - acc: 0.337 - ETA: 535s - loss: 2.8783 - acc: 0.338 - ETA: 531s - loss: 2.8749 - acc: 0.339 - ETA: 528s - loss: 2.8708 - acc: 0.340 - ETA: 525s - loss: 2.8727 - acc: 0.339 - ETA: 522s - loss: 2.8734 - acc: 0.338 - ETA: 519s - loss: 2.8772 - acc: 0.338 - ETA: 516s - loss: 2.8748 - acc: 0.338 - ETA: 514s - loss: 2.8720 - acc: 0.338 - ETA: 511s - loss: 2.8730 - acc: 0.337 - ETA: 508s - loss: 2.8789 - acc: 0.336 - ETA: 505s - loss: 2.8805 - acc: 0.335 - ETA: 502s - loss: 2.8829 - acc: 0.335 - ETA: 499s - loss: 2.8807 - acc: 0.336 - ETA: 497s - loss: 2.8842 - acc: 0.335 - ETA: 495s - loss: 2.8852 - acc: 0.335 - ETA: 491s - loss: 2.8816 - acc: 0.336 - ETA: 489s - loss: 2.8788 - acc: 0.336 - ETA: 486s - loss: 2.8720 - acc: 0.338 - ETA: 483s - loss: 2.8650 - acc: 0.339 - ETA: 481s - loss: 2.8621 - acc: 0.339 - ETA: 479s - loss: 2.8582 - acc: 0.341 - ETA: 476s - loss: 2.8575 - acc: 0.340 - ETA: 473s - loss: 2.8624 - acc: 0.340 - ETA: 470s - loss: 2.8662 - acc: 0.340 - ETA: 467s - loss: 2.8659 - acc: 0.340 - ETA: 465s - loss: 2.8633 - acc: 0.341 - ETA: 462s - loss: 2.8606 - acc: 0.341 - ETA: 460s - loss: 2.8592 - acc: 0.342 - ETA: 457s - loss: 2.8557 - acc: 0.343 - ETA: 455s - loss: 2.8520 - acc: 0.343 - ETA: 452s - loss: 2.8510 - acc: 0.342 - ETA: 450s - loss: 2.8471 - acc: 0.343 - ETA: 449s - loss: 2.8493 - acc: 0.343 - ETA: 447s - loss: 2.8507 - acc: 0.342 - ETA: 445s - loss: 2.8491 - acc: 0.343 - ETA: 442s - loss: 2.8499 - acc: 0.342 - ETA: 440s - loss: 2.8463 - acc: 0.342 - ETA: 437s - loss: 2.8422 - acc: 0.343 - ETA: 434s - loss: 2.8345 - acc: 0.344 - ETA: 431s - loss: 2.8311 - acc: 0.344 - ETA: 428s - loss: 2.8291 - acc: 0.345 - ETA: 425s - loss: 2.8270 - acc: 0.345 - ETA: 422s - loss: 2.8256 - acc: 0.346 - ETA: 420s - loss: 2.8198 - acc: 0.347 - ETA: 417s - loss: 2.8267 - acc: 0.347 - ETA: 415s - loss: 2.8283 - acc: 0.346 - ETA: 412s - loss: 2.8284 - acc: 0.346 - ETA: 409s - loss: 2.8275 - acc: 0.346 - ETA: 406s - loss: 2.8260 - acc: 0.346 - ETA: 403s - loss: 2.8306 - acc: 0.345 - ETA: 401s - loss: 2.8339 - acc: 0.344 - ETA: 399s - loss: 2.8417 - acc: 0.343 - ETA: 396s - loss: 2.8414 - acc: 0.343 - ETA: 393s - loss: 2.8419 - acc: 0.343 - ETA: 390s - loss: 2.8454 - acc: 0.343 - ETA: 387s - loss: 2.8463 - acc: 0.343 - ETA: 385s - loss: 2.8446 - acc: 0.343 - ETA: 383s - loss: 2.8424 - acc: 0.344 - ETA: 380s - loss: 2.8432 - acc: 0.344 - ETA: 377s - loss: 2.8417 - acc: 0.345 - ETA: 374s - loss: 2.8434 - acc: 0.345 - ETA: 372s - loss: 2.8421 - acc: 0.345 - ETA: 369s - loss: 2.8444 - acc: 0.345 - ETA: 367s - loss: 2.8409 - acc: 0.345 - ETA: 364s - loss: 2.8403 - acc: 0.345 - ETA: 362s - loss: 2.8425 - acc: 0.345 - ETA: 359s - loss: 2.8450 - acc: 0.344 - ETA: 356s - loss: 2.8482 - acc: 0.343 - ETA: 353s - loss: 2.8505 - acc: 0.343 - ETA: 351s - loss: 2.8476 - acc: 0.343 - ETA: 348s - loss: 2.8499 - acc: 0.343 - ETA: 346s - loss: 2.8476 - acc: 0.343 - ETA: 343s - loss: 2.8486 - acc: 0.343 - ETA: 340s - loss: 2.8481 - acc: 0.343 - ETA: 338s - loss: 2.8520 - acc: 0.3422"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 335s - loss: 2.8517 - acc: 0.342 - ETA: 333s - loss: 2.8527 - acc: 0.342 - ETA: 330s - loss: 2.8542 - acc: 0.341 - ETA: 327s - loss: 2.8559 - acc: 0.340 - ETA: 325s - loss: 2.8580 - acc: 0.340 - ETA: 322s - loss: 2.8555 - acc: 0.341 - ETA: 320s - loss: 2.8580 - acc: 0.341 - ETA: 317s - loss: 2.8635 - acc: 0.341 - ETA: 314s - loss: 2.8702 - acc: 0.340 - ETA: 312s - loss: 2.8718 - acc: 0.340 - ETA: 309s - loss: 2.8718 - acc: 0.340 - ETA: 306s - loss: 2.8730 - acc: 0.339 - ETA: 304s - loss: 2.8730 - acc: 0.338 - ETA: 301s - loss: 2.8730 - acc: 0.339 - ETA: 299s - loss: 2.8693 - acc: 0.340 - ETA: 296s - loss: 2.8694 - acc: 0.340 - ETA: 293s - loss: 2.8696 - acc: 0.339 - ETA: 291s - loss: 2.8699 - acc: 0.340 - ETA: 288s - loss: 2.8680 - acc: 0.340 - ETA: 285s - loss: 2.8694 - acc: 0.339 - ETA: 283s - loss: 2.8711 - acc: 0.339 - ETA: 280s - loss: 2.8694 - acc: 0.339 - ETA: 277s - loss: 2.8701 - acc: 0.339 - ETA: 275s - loss: 2.8720 - acc: 0.339 - ETA: 272s - loss: 2.8750 - acc: 0.338 - ETA: 270s - loss: 2.8745 - acc: 0.338 - ETA: 267s - loss: 2.8729 - acc: 0.338 - ETA: 264s - loss: 2.8762 - acc: 0.338 - ETA: 262s - loss: 2.8744 - acc: 0.338 - ETA: 259s - loss: 2.8736 - acc: 0.338 - ETA: 257s - loss: 2.8743 - acc: 0.337 - ETA: 254s - loss: 2.8755 - acc: 0.337 - ETA: 252s - loss: 2.8780 - acc: 0.337 - ETA: 249s - loss: 2.8781 - acc: 0.336 - ETA: 246s - loss: 2.8765 - acc: 0.336 - ETA: 244s - loss: 2.8764 - acc: 0.336 - ETA: 241s - loss: 2.8753 - acc: 0.336 - ETA: 239s - loss: 2.8749 - acc: 0.336 - ETA: 236s - loss: 2.8765 - acc: 0.335 - ETA: 233s - loss: 2.8734 - acc: 0.336 - ETA: 231s - loss: 2.8741 - acc: 0.336 - ETA: 228s - loss: 2.8743 - acc: 0.336 - ETA: 225s - loss: 2.8745 - acc: 0.336 - ETA: 223s - loss: 2.8742 - acc: 0.336 - ETA: 220s - loss: 2.8770 - acc: 0.336 - ETA: 218s - loss: 2.8800 - acc: 0.336 - ETA: 215s - loss: 2.8794 - acc: 0.336 - ETA: 212s - loss: 2.8759 - acc: 0.336 - ETA: 210s - loss: 2.8751 - acc: 0.337 - ETA: 207s - loss: 2.8768 - acc: 0.336 - ETA: 205s - loss: 2.8750 - acc: 0.336 - ETA: 202s - loss: 2.8758 - acc: 0.336 - ETA: 199s - loss: 2.8788 - acc: 0.335 - ETA: 197s - loss: 2.8777 - acc: 0.336 - ETA: 194s - loss: 2.8763 - acc: 0.336 - ETA: 191s - loss: 2.8818 - acc: 0.335 - ETA: 189s - loss: 2.8824 - acc: 0.335 - ETA: 186s - loss: 2.8958 - acc: 0.333 - ETA: 184s - loss: 2.9022 - acc: 0.332 - ETA: 181s - loss: 2.9052 - acc: 0.332 - ETA: 178s - loss: 2.9051 - acc: 0.332 - ETA: 176s - loss: 2.9048 - acc: 0.332 - ETA: 173s - loss: 2.9015 - acc: 0.333 - ETA: 171s - loss: 2.9035 - acc: 0.333 - ETA: 168s - loss: 2.9019 - acc: 0.333 - ETA: 165s - loss: 2.8991 - acc: 0.333 - ETA: 163s - loss: 2.8990 - acc: 0.333 - ETA: 160s - loss: 2.8992 - acc: 0.334 - ETA: 157s - loss: 2.8990 - acc: 0.333 - ETA: 155s - loss: 2.8999 - acc: 0.333 - ETA: 152s - loss: 2.9008 - acc: 0.333 - ETA: 150s - loss: 2.8996 - acc: 0.333 - ETA: 147s - loss: 2.9033 - acc: 0.332 - ETA: 144s - loss: 2.9036 - acc: 0.332 - ETA: 142s - loss: 2.9037 - acc: 0.332 - ETA: 139s - loss: 2.9046 - acc: 0.331 - ETA: 137s - loss: 2.9061 - acc: 0.331 - ETA: 134s - loss: 2.9054 - acc: 0.331 - ETA: 132s - loss: 2.9070 - acc: 0.330 - ETA: 129s - loss: 2.9077 - acc: 0.330 - ETA: 126s - loss: 2.9095 - acc: 0.330 - ETA: 124s - loss: 2.9090 - acc: 0.330 - ETA: 121s - loss: 2.9120 - acc: 0.329 - ETA: 119s - loss: 2.9140 - acc: 0.329 - ETA: 116s - loss: 2.9166 - acc: 0.327 - ETA: 114s - loss: 2.9152 - acc: 0.327 - ETA: 111s - loss: 2.9177 - acc: 0.327 - ETA: 109s - loss: 2.9226 - acc: 0.327 - ETA: 106s - loss: 2.9231 - acc: 0.327 - ETA: 104s - loss: 2.9217 - acc: 0.326 - ETA: 101s - loss: 2.9222 - acc: 0.326 - ETA: 98s - loss: 2.9200 - acc: 0.327 - ETA: 96s - loss: 2.9186 - acc: 0.32 - ETA: 93s - loss: 2.9174 - acc: 0.32 - ETA: 90s - loss: 2.9173 - acc: 0.32 - ETA: 88s - loss: 2.9179 - acc: 0.32 - ETA: 85s - loss: 2.9215 - acc: 0.32 - ETA: 83s - loss: 2.9219 - acc: 0.32 - ETA: 80s - loss: 2.9257 - acc: 0.32 - ETA: 77s - loss: 2.9254 - acc: 0.32 - ETA: 75s - loss: 2.9263 - acc: 0.32 - ETA: 72s - loss: 2.9249 - acc: 0.32 - ETA: 70s - loss: 2.9263 - acc: 0.32 - ETA: 67s - loss: 2.9289 - acc: 0.32 - ETA: 64s - loss: 2.9293 - acc: 0.32 - ETA: 62s - loss: 2.9287 - acc: 0.32 - ETA: 59s - loss: 2.9289 - acc: 0.32 - ETA: 56s - loss: 2.9273 - acc: 0.32 - ETA: 54s - loss: 2.9263 - acc: 0.32 - ETA: 51s - loss: 2.9266 - acc: 0.32 - ETA: 49s - loss: 2.9255 - acc: 0.32 - ETA: 46s - loss: 2.9242 - acc: 0.32 - ETA: 44s - loss: 2.9246 - acc: 0.32 - ETA: 41s - loss: 2.9227 - acc: 0.32 - ETA: 38s - loss: 2.9197 - acc: 0.32 - ETA: 36s - loss: 2.9190 - acc: 0.32 - ETA: 33s - loss: 2.9176 - acc: 0.32 - ETA: 31s - loss: 2.9183 - acc: 0.32 - ETA: 28s - loss: 2.9160 - acc: 0.32 - ETA: 25s - loss: 2.9191 - acc: 0.32 - ETA: 23s - loss: 2.9205 - acc: 0.32 - ETA: 20s - loss: 2.9195 - acc: 0.32 - ETA: 18s - loss: 2.9211 - acc: 0.32 - ETA: 15s - loss: 2.9209 - acc: 0.32 - ETA: 12s - loss: 2.9215 - acc: 0.32 - ETA: 10s - loss: 2.9242 - acc: 0.32 - ETA: 7s - loss: 2.9260 - acc: 0.3246 - ETA: 5s - loss: 2.9269 - acc: 0.324 - ETA: 2s - loss: 2.9274 - acc: 0.3239Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 892s - loss: 2.9318 - acc: 0.3235 - val_loss: 4.8924 - val_acc: 0.0671\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 783s - loss: 1.7360 - acc: 0.750 - ETA: 801s - loss: 2.0261 - acc: 0.650 - ETA: 839s - loss: 2.8930 - acc: 0.533 - ETA: 854s - loss: 3.1163 - acc: 0.437 - ETA: 838s - loss: 2.8436 - acc: 0.440 - ETA: 830s - loss: 2.6852 - acc: 0.441 - ETA: 816s - loss: 2.5898 - acc: 0.450 - ETA: 809s - loss: 2.4494 - acc: 0.475 - ETA: 805s - loss: 2.3592 - acc: 0.488 - ETA: 814s - loss: 2.2207 - acc: 0.505 - ETA: 812s - loss: 2.1193 - acc: 0.522 - ETA: 803s - loss: 2.0426 - acc: 0.537 - ETA: 797s - loss: 1.9786 - acc: 0.553 - ETA: 792s - loss: 1.9447 - acc: 0.557 - ETA: 792s - loss: 1.9217 - acc: 0.563 - ETA: 801s - loss: 1.8574 - acc: 0.575 - ETA: 795s - loss: 1.9327 - acc: 0.570 - ETA: 790s - loss: 2.0766 - acc: 0.558 - ETA: 785s - loss: 2.0499 - acc: 0.557 - ETA: 780s - loss: 2.0580 - acc: 0.555 - ETA: 777s - loss: 2.0220 - acc: 0.559 - ETA: 778s - loss: 1.9973 - acc: 0.568 - ETA: 777s - loss: 1.9749 - acc: 0.576 - ETA: 772s - loss: 1.9823 - acc: 0.570 - ETA: 767s - loss: 1.9837 - acc: 0.572 - ETA: 763s - loss: 1.9787 - acc: 0.569 - ETA: 759s - loss: 1.9610 - acc: 0.572 - ETA: 759s - loss: 1.9397 - acc: 0.571 - ETA: 760s - loss: 1.9275 - acc: 0.570 - ETA: 758s - loss: 1.9132 - acc: 0.571 - ETA: 753s - loss: 1.8935 - acc: 0.575 - ETA: 750s - loss: 1.8690 - acc: 0.578 - ETA: 747s - loss: 1.8527 - acc: 0.574 - ETA: 743s - loss: 1.8409 - acc: 0.573 - ETA: 740s - loss: 1.8260 - acc: 0.574 - ETA: 741s - loss: 1.8268 - acc: 0.573 - ETA: 739s - loss: 1.8702 - acc: 0.567 - ETA: 736s - loss: 1.9341 - acc: 0.559 - ETA: 733s - loss: 1.9313 - acc: 0.557 - ETA: 731s - loss: 1.9343 - acc: 0.557 - ETA: 734s - loss: 1.9371 - acc: 0.557 - ETA: 736s - loss: 1.9290 - acc: 0.557 - ETA: 735s - loss: 1.9326 - acc: 0.553 - ETA: 733s - loss: 1.9182 - acc: 0.558 - ETA: 731s - loss: 1.9204 - acc: 0.556 - ETA: 728s - loss: 1.9159 - acc: 0.557 - ETA: 729s - loss: 1.8922 - acc: 0.560 - ETA: 727s - loss: 1.8856 - acc: 0.555 - ETA: 724s - loss: 1.8894 - acc: 0.555 - ETA: 724s - loss: 1.8742 - acc: 0.558 - ETA: 722s - loss: 1.8711 - acc: 0.558 - ETA: 718s - loss: 1.8667 - acc: 0.559 - ETA: 715s - loss: 1.8796 - acc: 0.557 - ETA: 714s - loss: 1.8810 - acc: 0.557 - ETA: 709s - loss: 1.8781 - acc: 0.557 - ETA: 705s - loss: 1.8775 - acc: 0.558 - ETA: 701s - loss: 1.8775 - acc: 0.558 - ETA: 698s - loss: 1.8773 - acc: 0.558 - ETA: 695s - loss: 1.8566 - acc: 0.563 - ETA: 692s - loss: 1.8529 - acc: 0.565 - ETA: 689s - loss: 1.8483 - acc: 0.566 - ETA: 686s - loss: 1.8495 - acc: 0.564 - ETA: 683s - loss: 1.8554 - acc: 0.563 - ETA: 679s - loss: 1.8613 - acc: 0.563 - ETA: 678s - loss: 1.8577 - acc: 0.563 - ETA: 682s - loss: 1.8439 - acc: 0.564 - ETA: 682s - loss: 1.8360 - acc: 0.564 - ETA: 683s - loss: 1.8420 - acc: 0.563 - ETA: 679s - loss: 1.8552 - acc: 0.560 - ETA: 678s - loss: 1.8465 - acc: 0.561 - ETA: 675s - loss: 1.8479 - acc: 0.562 - ETA: 677s - loss: 1.8503 - acc: 0.559 - ETA: 674s - loss: 1.8424 - acc: 0.561 - ETA: 672s - loss: 1.8398 - acc: 0.562 - ETA: 670s - loss: 1.8428 - acc: 0.562 - ETA: 668s - loss: 1.8357 - acc: 0.565 - ETA: 667s - loss: 1.8322 - acc: 0.566 - ETA: 666s - loss: 1.8256 - acc: 0.569 - ETA: 667s - loss: 1.8315 - acc: 0.567 - ETA: 665s - loss: 1.8199 - acc: 0.570 - ETA: 663s - loss: 1.8194 - acc: 0.569 - ETA: 664s - loss: 1.8385 - acc: 0.567 - ETA: 661s - loss: 1.8525 - acc: 0.565 - ETA: 658s - loss: 1.8666 - acc: 0.562 - ETA: 656s - loss: 1.8642 - acc: 0.561 - ETA: 655s - loss: 1.8617 - acc: 0.562 - ETA: 653s - loss: 1.8578 - acc: 0.562 - ETA: 651s - loss: 1.8535 - acc: 0.563 - ETA: 647s - loss: 1.8523 - acc: 0.562 - ETA: 647s - loss: 1.8436 - acc: 0.565 - ETA: 648s - loss: 1.8394 - acc: 0.565 - ETA: 648s - loss: 1.8440 - acc: 0.563 - ETA: 646s - loss: 1.8455 - acc: 0.561 - ETA: 644s - loss: 1.8435 - acc: 0.562 - ETA: 641s - loss: 1.8426 - acc: 0.563 - ETA: 638s - loss: 1.8398 - acc: 0.564 - ETA: 634s - loss: 1.8399 - acc: 0.564 - ETA: 633s - loss: 1.8377 - acc: 0.563 - ETA: 631s - loss: 1.8305 - acc: 0.564 - ETA: 628s - loss: 1.8278 - acc: 0.566 - ETA: 624s - loss: 1.8348 - acc: 0.565 - ETA: 621s - loss: 1.8287 - acc: 0.567 - ETA: 617s - loss: 1.8424 - acc: 0.564 - ETA: 614s - loss: 1.8450 - acc: 0.563 - ETA: 612s - loss: 1.8478 - acc: 0.563 - ETA: 610s - loss: 1.8602 - acc: 0.562 - ETA: 607s - loss: 1.8563 - acc: 0.563 - ETA: 604s - loss: 1.8575 - acc: 0.561 - ETA: 601s - loss: 1.8581 - acc: 0.561 - ETA: 598s - loss: 1.8589 - acc: 0.561 - ETA: 597s - loss: 1.8676 - acc: 0.559 - ETA: 594s - loss: 1.8669 - acc: 0.559 - ETA: 590s - loss: 1.8693 - acc: 0.558 - ETA: 587s - loss: 1.8682 - acc: 0.557 - ETA: 584s - loss: 1.8804 - acc: 0.555 - ETA: 581s - loss: 1.8850 - acc: 0.556 - ETA: 579s - loss: 1.8836 - acc: 0.555 - ETA: 577s - loss: 1.8839 - acc: 0.555 - ETA: 574s - loss: 1.8775 - acc: 0.556 - ETA: 570s - loss: 1.8772 - acc: 0.556 - ETA: 567s - loss: 1.8834 - acc: 0.554 - ETA: 564s - loss: 1.8851 - acc: 0.552 - ETA: 562s - loss: 1.8873 - acc: 0.552 - ETA: 560s - loss: 1.8883 - acc: 0.552 - ETA: 557s - loss: 1.8880 - acc: 0.551 - ETA: 554s - loss: 1.8950 - acc: 0.550 - ETA: 551s - loss: 1.8951 - acc: 0.549 - ETA: 547s - loss: 1.8905 - acc: 0.549 - ETA: 545s - loss: 1.8912 - acc: 0.550 - ETA: 543s - loss: 1.8931 - acc: 0.549 - ETA: 540s - loss: 1.8990 - acc: 0.547 - ETA: 537s - loss: 1.9008 - acc: 0.547 - ETA: 534s - loss: 1.9046 - acc: 0.546 - ETA: 531s - loss: 1.9037 - acc: 0.546 - ETA: 528s - loss: 1.9009 - acc: 0.546 - ETA: 526s - loss: 1.9060 - acc: 0.546 - ETA: 523s - loss: 1.9044 - acc: 0.546 - ETA: 520s - loss: 1.8998 - acc: 0.547 - ETA: 517s - loss: 1.8993 - acc: 0.548 - ETA: 514s - loss: 1.8991 - acc: 0.548 - ETA: 512s - loss: 1.9153 - acc: 0.546 - ETA: 510s - loss: 1.9224 - acc: 0.545 - ETA: 508s - loss: 1.9297 - acc: 0.544 - ETA: 505s - loss: 1.9339 - acc: 0.543 - ETA: 502s - loss: 1.9340 - acc: 0.542 - ETA: 499s - loss: 1.9303 - acc: 0.542 - ETA: 497s - loss: 1.9312 - acc: 0.542 - ETA: 495s - loss: 1.9293 - acc: 0.542 - ETA: 492s - loss: 1.9294 - acc: 0.541 - ETA: 489s - loss: 1.9367 - acc: 0.540 - ETA: 486s - loss: 1.9360 - acc: 0.539 - ETA: 484s - loss: 1.9297 - acc: 0.540 - ETA: 481s - loss: 1.9286 - acc: 0.540 - ETA: 478s - loss: 1.9282 - acc: 0.540 - ETA: 475s - loss: 1.9246 - acc: 0.541 - ETA: 473s - loss: 1.9228 - acc: 0.541 - ETA: 470s - loss: 1.9248 - acc: 0.540 - ETA: 467s - loss: 1.9261 - acc: 0.539 - ETA: 464s - loss: 1.9311 - acc: 0.539 - ETA: 461s - loss: 1.9315 - acc: 0.539 - ETA: 459s - loss: 1.9351 - acc: 0.538 - ETA: 456s - loss: 1.9320 - acc: 0.539 - ETA: 453s - loss: 1.9280 - acc: 0.540 - ETA: 450s - loss: 1.9308 - acc: 0.538 - ETA: 447s - loss: 1.9316 - acc: 0.538 - ETA: 445s - loss: 1.9323 - acc: 0.538 - ETA: 442s - loss: 1.9341 - acc: 0.537 - ETA: 439s - loss: 1.9347 - acc: 0.537 - ETA: 436s - loss: 1.9293 - acc: 0.539 - ETA: 433s - loss: 1.9315 - acc: 0.539 - ETA: 430s - loss: 1.9336 - acc: 0.539 - ETA: 427s - loss: 1.9387 - acc: 0.538 - ETA: 425s - loss: 1.9394 - acc: 0.538 - ETA: 422s - loss: 1.9366 - acc: 0.539 - ETA: 419s - loss: 1.9362 - acc: 0.538 - ETA: 416s - loss: 1.9386 - acc: 0.537 - ETA: 413s - loss: 1.9376 - acc: 0.537 - ETA: 410s - loss: 1.9374 - acc: 0.537 - ETA: 407s - loss: 1.9358 - acc: 0.538 - ETA: 404s - loss: 1.9390 - acc: 0.537 - ETA: 401s - loss: 1.9381 - acc: 0.536 - ETA: 398s - loss: 1.9357 - acc: 0.536 - ETA: 396s - loss: 1.9316 - acc: 0.538 - ETA: 393s - loss: 1.9299 - acc: 0.539 - ETA: 390s - loss: 1.9286 - acc: 0.539 - ETA: 388s - loss: 1.9336 - acc: 0.538 - ETA: 385s - loss: 1.9303 - acc: 0.539 - ETA: 382s - loss: 1.9314 - acc: 0.539 - ETA: 380s - loss: 1.9331 - acc: 0.539 - ETA: 377s - loss: 1.9382 - acc: 0.539 - ETA: 374s - loss: 1.9369 - acc: 0.539 - ETA: 371s - loss: 1.9364 - acc: 0.539 - ETA: 369s - loss: 1.9337 - acc: 0.539 - ETA: 366s - loss: 1.9341 - acc: 0.539 - ETA: 363s - loss: 1.9493 - acc: 0.537 - ETA: 361s - loss: 1.9572 - acc: 0.536 - ETA: 358s - loss: 1.9567 - acc: 0.536 - ETA: 355s - loss: 1.9655 - acc: 0.535 - ETA: 353s - loss: 1.9689 - acc: 0.534 - ETA: 350s - loss: 1.9719 - acc: 0.533 - ETA: 348s - loss: 1.9737 - acc: 0.533 - ETA: 345s - loss: 1.9754 - acc: 0.533 - ETA: 343s - loss: 1.9740 - acc: 0.533 - ETA: 340s - loss: 1.9761 - acc: 0.5328"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 338s - loss: 1.9775 - acc: 0.532 - ETA: 336s - loss: 1.9759 - acc: 0.532 - ETA: 334s - loss: 1.9712 - acc: 0.533 - ETA: 331s - loss: 1.9727 - acc: 0.533 - ETA: 329s - loss: 1.9727 - acc: 0.533 - ETA: 326s - loss: 1.9744 - acc: 0.533 - ETA: 323s - loss: 1.9761 - acc: 0.533 - ETA: 321s - loss: 1.9765 - acc: 0.533 - ETA: 318s - loss: 1.9767 - acc: 0.533 - ETA: 315s - loss: 1.9754 - acc: 0.533 - ETA: 312s - loss: 1.9743 - acc: 0.533 - ETA: 310s - loss: 1.9724 - acc: 0.533 - ETA: 307s - loss: 1.9734 - acc: 0.532 - ETA: 304s - loss: 1.9744 - acc: 0.532 - ETA: 302s - loss: 1.9757 - acc: 0.532 - ETA: 299s - loss: 1.9761 - acc: 0.532 - ETA: 296s - loss: 1.9765 - acc: 0.532 - ETA: 294s - loss: 1.9747 - acc: 0.532 - ETA: 291s - loss: 1.9733 - acc: 0.533 - ETA: 288s - loss: 1.9751 - acc: 0.533 - ETA: 286s - loss: 1.9770 - acc: 0.533 - ETA: 283s - loss: 1.9766 - acc: 0.533 - ETA: 280s - loss: 1.9752 - acc: 0.533 - ETA: 277s - loss: 1.9746 - acc: 0.533 - ETA: 275s - loss: 1.9771 - acc: 0.533 - ETA: 272s - loss: 1.9758 - acc: 0.533 - ETA: 269s - loss: 1.9774 - acc: 0.533 - ETA: 267s - loss: 1.9759 - acc: 0.533 - ETA: 264s - loss: 1.9845 - acc: 0.532 - ETA: 261s - loss: 1.9846 - acc: 0.532 - ETA: 258s - loss: 1.9848 - acc: 0.531 - ETA: 256s - loss: 1.9852 - acc: 0.531 - ETA: 253s - loss: 1.9832 - acc: 0.532 - ETA: 251s - loss: 1.9835 - acc: 0.531 - ETA: 248s - loss: 1.9836 - acc: 0.531 - ETA: 245s - loss: 1.9866 - acc: 0.530 - ETA: 242s - loss: 1.9902 - acc: 0.529 - ETA: 240s - loss: 1.9887 - acc: 0.530 - ETA: 237s - loss: 1.9875 - acc: 0.530 - ETA: 235s - loss: 1.9942 - acc: 0.529 - ETA: 232s - loss: 1.9983 - acc: 0.528 - ETA: 229s - loss: 1.9975 - acc: 0.528 - ETA: 226s - loss: 1.9997 - acc: 0.527 - ETA: 224s - loss: 1.9980 - acc: 0.528 - ETA: 221s - loss: 1.9982 - acc: 0.528 - ETA: 219s - loss: 2.0098 - acc: 0.526 - ETA: 216s - loss: 2.0087 - acc: 0.526 - ETA: 214s - loss: 2.0115 - acc: 0.526 - ETA: 211s - loss: 2.0170 - acc: 0.525 - ETA: 208s - loss: 2.0165 - acc: 0.525 - ETA: 206s - loss: 2.0180 - acc: 0.524 - ETA: 203s - loss: 2.0168 - acc: 0.525 - ETA: 200s - loss: 2.0191 - acc: 0.524 - ETA: 198s - loss: 2.0199 - acc: 0.524 - ETA: 195s - loss: 2.0206 - acc: 0.524 - ETA: 193s - loss: 2.0181 - acc: 0.525 - ETA: 190s - loss: 2.0177 - acc: 0.525 - ETA: 187s - loss: 2.0183 - acc: 0.525 - ETA: 185s - loss: 2.0174 - acc: 0.525 - ETA: 182s - loss: 2.0169 - acc: 0.525 - ETA: 179s - loss: 2.0152 - acc: 0.525 - ETA: 177s - loss: 2.0147 - acc: 0.525 - ETA: 174s - loss: 2.0180 - acc: 0.525 - ETA: 172s - loss: 2.0214 - acc: 0.524 - ETA: 169s - loss: 2.0267 - acc: 0.523 - ETA: 166s - loss: 2.0257 - acc: 0.524 - ETA: 164s - loss: 2.0250 - acc: 0.524 - ETA: 161s - loss: 2.0224 - acc: 0.524 - ETA: 158s - loss: 2.0234 - acc: 0.524 - ETA: 156s - loss: 2.0231 - acc: 0.524 - ETA: 153s - loss: 2.0241 - acc: 0.524 - ETA: 150s - loss: 2.0228 - acc: 0.524 - ETA: 148s - loss: 2.0204 - acc: 0.525 - ETA: 145s - loss: 2.0210 - acc: 0.525 - ETA: 143s - loss: 2.0219 - acc: 0.525 - ETA: 140s - loss: 2.0237 - acc: 0.524 - ETA: 137s - loss: 2.0232 - acc: 0.524 - ETA: 135s - loss: 2.0243 - acc: 0.524 - ETA: 132s - loss: 2.0210 - acc: 0.525 - ETA: 129s - loss: 2.0214 - acc: 0.525 - ETA: 127s - loss: 2.0206 - acc: 0.525 - ETA: 124s - loss: 2.0200 - acc: 0.525 - ETA: 122s - loss: 2.0196 - acc: 0.525 - ETA: 119s - loss: 2.0201 - acc: 0.525 - ETA: 116s - loss: 2.0176 - acc: 0.525 - ETA: 114s - loss: 2.0150 - acc: 0.525 - ETA: 111s - loss: 2.0132 - acc: 0.525 - ETA: 109s - loss: 2.0143 - acc: 0.525 - ETA: 106s - loss: 2.0149 - acc: 0.525 - ETA: 103s - loss: 2.0148 - acc: 0.525 - ETA: 101s - loss: 2.0180 - acc: 0.524 - ETA: 98s - loss: 2.0177 - acc: 0.524 - ETA: 96s - loss: 2.0187 - acc: 0.52 - ETA: 93s - loss: 2.0220 - acc: 0.52 - ETA: 90s - loss: 2.0241 - acc: 0.52 - ETA: 88s - loss: 2.0242 - acc: 0.52 - ETA: 85s - loss: 2.0223 - acc: 0.52 - ETA: 83s - loss: 2.0209 - acc: 0.52 - ETA: 80s - loss: 2.0194 - acc: 0.52 - ETA: 77s - loss: 2.0199 - acc: 0.52 - ETA: 75s - loss: 2.0191 - acc: 0.52 - ETA: 72s - loss: 2.0196 - acc: 0.52 - ETA: 70s - loss: 2.0242 - acc: 0.52 - ETA: 67s - loss: 2.0282 - acc: 0.52 - ETA: 64s - loss: 2.0275 - acc: 0.52 - ETA: 62s - loss: 2.0262 - acc: 0.52 - ETA: 59s - loss: 2.0259 - acc: 0.52 - ETA: 57s - loss: 2.0280 - acc: 0.52 - ETA: 54s - loss: 2.0592 - acc: 0.52 - ETA: 51s - loss: 2.0689 - acc: 0.52 - ETA: 49s - loss: 2.0696 - acc: 0.52 - ETA: 46s - loss: 2.0749 - acc: 0.51 - ETA: 44s - loss: 2.0765 - acc: 0.51 - ETA: 41s - loss: 2.0764 - acc: 0.51 - ETA: 38s - loss: 2.0797 - acc: 0.51 - ETA: 36s - loss: 2.0823 - acc: 0.51 - ETA: 33s - loss: 2.0826 - acc: 0.51 - ETA: 31s - loss: 2.0815 - acc: 0.51 - ETA: 28s - loss: 2.0815 - acc: 0.51 - ETA: 25s - loss: 2.0827 - acc: 0.51 - ETA: 23s - loss: 2.0837 - acc: 0.51 - ETA: 20s - loss: 2.0845 - acc: 0.51 - ETA: 18s - loss: 2.0833 - acc: 0.51 - ETA: 15s - loss: 2.0812 - acc: 0.51 - ETA: 12s - loss: 2.0792 - acc: 0.51 - ETA: 10s - loss: 2.0809 - acc: 0.51 - ETA: 7s - loss: 2.0813 - acc: 0.5180 - ETA: 5s - loss: 2.0821 - acc: 0.517 - ETA: 2s - loss: 2.0804 - acc: 0.5180Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 896s - loss: 2.0824 - acc: 0.5181 - val_loss: 6.0047 - val_acc: 0.0599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e623f6a400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 4.9043%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 133s - loss: 15.7034 - acc: 0.0000e+ - ETA: 18s - loss: 14.7770 - acc: 0.0063     - ETA: 11s - loss: 14.7371 - acc: 0.010 - ETA: 8s - loss: 14.6839 - acc: 0.014 - ETA: 7s - loss: 14.5121 - acc: 0.01 - ETA: 6s - loss: 14.3485 - acc: 0.01 - ETA: 5s - loss: 14.3249 - acc: 0.02 - ETA: 4s - loss: 14.2925 - acc: 0.02 - ETA: 4s - loss: 14.1629 - acc: 0.02 - ETA: 4s - loss: 14.1550 - acc: 0.03 - ETA: 4s - loss: 14.1056 - acc: 0.03 - ETA: 3s - loss: 14.0170 - acc: 0.03 - ETA: 3s - loss: 13.9069 - acc: 0.03 - ETA: 3s - loss: 13.8101 - acc: 0.03 - ETA: 3s - loss: 13.7624 - acc: 0.03 - ETA: 3s - loss: 13.7221 - acc: 0.04 - ETA: 2s - loss: 13.6041 - acc: 0.04 - ETA: 2s - loss: 13.5462 - acc: 0.05 - ETA: 2s - loss: 13.4458 - acc: 0.05 - ETA: 2s - loss: 13.3527 - acc: 0.05 - ETA: 2s - loss: 13.2854 - acc: 0.06 - ETA: 2s - loss: 13.1983 - acc: 0.06 - ETA: 2s - loss: 13.1263 - acc: 0.06 - ETA: 2s - loss: 13.0922 - acc: 0.07 - ETA: 2s - loss: 13.0281 - acc: 0.07 - ETA: 1s - loss: 13.0026 - acc: 0.07 - ETA: 1s - loss: 12.9616 - acc: 0.07 - ETA: 1s - loss: 12.9102 - acc: 0.07 - ETA: 1s - loss: 12.8362 - acc: 0.08 - ETA: 1s - loss: 12.7746 - acc: 0.08 - ETA: 1s - loss: 12.7259 - acc: 0.08 - ETA: 1s - loss: 12.7000 - acc: 0.08 - ETA: 1s - loss: 12.6901 - acc: 0.09 - ETA: 1s - loss: 12.6916 - acc: 0.09 - ETA: 1s - loss: 12.6085 - acc: 0.09 - ETA: 1s - loss: 12.5660 - acc: 0.09 - ETA: 1s - loss: 12.5436 - acc: 0.09 - ETA: 1s - loss: 12.5153 - acc: 0.10 - ETA: 1s - loss: 12.4764 - acc: 0.10 - ETA: 0s - loss: 12.4316 - acc: 0.10 - ETA: 0s - loss: 12.4058 - acc: 0.10 - ETA: 0s - loss: 12.3462 - acc: 0.10 - ETA: 0s - loss: 12.3238 - acc: 0.11 - ETA: 0s - loss: 12.3015 - acc: 0.11 - ETA: 0s - loss: 12.2766 - acc: 0.11 - ETA: 0s - loss: 12.2508 - acc: 0.11 - ETA: 0s - loss: 12.2289 - acc: 0.11 - ETA: 0s - loss: 12.2119 - acc: 0.11 - ETA: 0s - loss: 12.1659 - acc: 0.12 - ETA: 0s - loss: 12.1365 - acc: 0.12 - ETA: 0s - loss: 12.1260 - acc: 0.12 - ETA: 0s - loss: 12.1132 - acc: 0.12 - ETA: 0s - loss: 12.0992 - acc: 0.12 - ETA: 0s - loss: 12.0899 - acc: 0.12 - ETA: 0s - loss: 12.0636 - acc: 0.1288Epoch 00000: val_loss improved from inf to 10.76894, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 12.0537 - acc: 0.1292 - val_loss: 10.7689 - val_acc: 0.2096\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 6.9812 - acc: 0.350 - ETA: 2s - loss: 9.6462 - acc: 0.281 - ETA: 2s - loss: 9.7636 - acc: 0.282 - ETA: 2s - loss: 9.7705 - acc: 0.275 - ETA: 2s - loss: 10.0726 - acc: 0.26 - ETA: 2s - loss: 9.9924 - acc: 0.2641 - ETA: 2s - loss: 10.0198 - acc: 0.26 - ETA: 2s - loss: 9.8723 - acc: 0.2761 - ETA: 2s - loss: 9.7760 - acc: 0.274 - ETA: 2s - loss: 9.8817 - acc: 0.272 - ETA: 2s - loss: 9.8755 - acc: 0.274 - ETA: 2s - loss: 9.9411 - acc: 0.267 - ETA: 2s - loss: 9.9351 - acc: 0.270 - ETA: 2s - loss: 9.9333 - acc: 0.267 - ETA: 2s - loss: 9.9474 - acc: 0.267 - ETA: 2s - loss: 9.8823 - acc: 0.273 - ETA: 2s - loss: 9.8525 - acc: 0.276 - ETA: 2s - loss: 9.9031 - acc: 0.274 - ETA: 2s - loss: 9.9180 - acc: 0.274 - ETA: 1s - loss: 9.9377 - acc: 0.274 - ETA: 1s - loss: 9.9360 - acc: 0.276 - ETA: 1s - loss: 9.8880 - acc: 0.281 - ETA: 1s - loss: 9.8634 - acc: 0.282 - ETA: 1s - loss: 9.8682 - acc: 0.282 - ETA: 1s - loss: 9.8510 - acc: 0.283 - ETA: 1s - loss: 9.8665 - acc: 0.282 - ETA: 1s - loss: 9.8368 - acc: 0.283 - ETA: 1s - loss: 9.8839 - acc: 0.281 - ETA: 1s - loss: 9.8648 - acc: 0.281 - ETA: 1s - loss: 9.8459 - acc: 0.283 - ETA: 1s - loss: 9.8629 - acc: 0.282 - ETA: 1s - loss: 9.8867 - acc: 0.281 - ETA: 1s - loss: 9.8706 - acc: 0.282 - ETA: 1s - loss: 9.8884 - acc: 0.281 - ETA: 1s - loss: 9.8912 - acc: 0.282 - ETA: 1s - loss: 9.8588 - acc: 0.283 - ETA: 1s - loss: 9.8719 - acc: 0.282 - ETA: 0s - loss: 9.8845 - acc: 0.283 - ETA: 0s - loss: 9.8921 - acc: 0.282 - ETA: 0s - loss: 9.8637 - acc: 0.283 - ETA: 0s - loss: 9.8386 - acc: 0.286 - ETA: 0s - loss: 9.8339 - acc: 0.286 - ETA: 0s - loss: 9.8207 - acc: 0.288 - ETA: 0s - loss: 9.8362 - acc: 0.286 - ETA: 0s - loss: 9.8204 - acc: 0.287 - ETA: 0s - loss: 9.8203 - acc: 0.286 - ETA: 0s - loss: 9.8473 - acc: 0.285 - ETA: 0s - loss: 9.8421 - acc: 0.285 - ETA: 0s - loss: 9.8356 - acc: 0.285 - ETA: 0s - loss: 9.8458 - acc: 0.285 - ETA: 0s - loss: 9.8531 - acc: 0.284 - ETA: 0s - loss: 9.8484 - acc: 0.285 - ETA: 0s - loss: 9.8554 - acc: 0.284 - ETA: 0s - loss: 9.8513 - acc: 0.284 - ETA: 0s - loss: 9.8468 - acc: 0.285 - ETA: 0s - loss: 9.8474 - acc: 0.2851Epoch 00001: val_loss improved from 10.76894 to 9.56559, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.8434 - acc: 0.2852 - val_loss: 9.5656 - val_acc: 0.2874\n",
      "Epoch 3/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 8.2052 - acc: 0.450 - ETA: 2s - loss: 8.5645 - acc: 0.381 - ETA: 2s - loss: 8.8901 - acc: 0.375 - ETA: 2s - loss: 8.9079 - acc: 0.372 - ETA: 2s - loss: 8.8002 - acc: 0.372 - ETA: 2s - loss: 9.1603 - acc: 0.356 - ETA: 2s - loss: 9.2039 - acc: 0.354 - ETA: 2s - loss: 9.1232 - acc: 0.353 - ETA: 2s - loss: 9.1865 - acc: 0.348 - ETA: 2s - loss: 9.2205 - acc: 0.346 - ETA: 2s - loss: 9.1087 - acc: 0.350 - ETA: 2s - loss: 9.1316 - acc: 0.351 - ETA: 2s - loss: 9.1563 - acc: 0.348 - ETA: 2s - loss: 9.1907 - acc: 0.347 - ETA: 2s - loss: 9.1912 - acc: 0.349 - ETA: 2s - loss: 9.1898 - acc: 0.353 - ETA: 2s - loss: 9.1071 - acc: 0.356 - ETA: 2s - loss: 9.1047 - acc: 0.356 - ETA: 2s - loss: 9.0373 - acc: 0.362 - ETA: 1s - loss: 9.0595 - acc: 0.362 - ETA: 1s - loss: 9.0300 - acc: 0.364 - ETA: 1s - loss: 9.0791 - acc: 0.361 - ETA: 1s - loss: 9.0820 - acc: 0.361 - ETA: 1s - loss: 9.1309 - acc: 0.359 - ETA: 1s - loss: 9.0936 - acc: 0.362 - ETA: 1s - loss: 9.0875 - acc: 0.362 - ETA: 1s - loss: 9.0961 - acc: 0.361 - ETA: 1s - loss: 9.0870 - acc: 0.362 - ETA: 1s - loss: 9.0839 - acc: 0.362 - ETA: 1s - loss: 9.0769 - acc: 0.362 - ETA: 1s - loss: 9.0572 - acc: 0.363 - ETA: 1s - loss: 9.0651 - acc: 0.362 - ETA: 1s - loss: 9.0895 - acc: 0.360 - ETA: 1s - loss: 9.0859 - acc: 0.361 - ETA: 1s - loss: 9.0808 - acc: 0.362 - ETA: 1s - loss: 9.0830 - acc: 0.360 - ETA: 1s - loss: 9.0845 - acc: 0.361 - ETA: 1s - loss: 9.0935 - acc: 0.360 - ETA: 0s - loss: 9.0536 - acc: 0.363 - ETA: 0s - loss: 9.0615 - acc: 0.362 - ETA: 0s - loss: 9.0598 - acc: 0.362 - ETA: 0s - loss: 9.0711 - acc: 0.362 - ETA: 0s - loss: 9.0876 - acc: 0.361 - ETA: 0s - loss: 9.0627 - acc: 0.362 - ETA: 0s - loss: 9.0691 - acc: 0.361 - ETA: 0s - loss: 9.0822 - acc: 0.360 - ETA: 0s - loss: 9.0629 - acc: 0.361 - ETA: 0s - loss: 9.0626 - acc: 0.361 - ETA: 0s - loss: 9.0447 - acc: 0.362 - ETA: 0s - loss: 9.0275 - acc: 0.364 - ETA: 0s - loss: 9.0155 - acc: 0.364 - ETA: 0s - loss: 9.0216 - acc: 0.364 - ETA: 0s - loss: 9.0286 - acc: 0.364 - ETA: 0s - loss: 9.0236 - acc: 0.365 - ETA: 0s - loss: 9.0162 - acc: 0.365 - ETA: 0s - loss: 9.0218 - acc: 0.365 - ETA: 0s - loss: 9.0171 - acc: 0.364 - ETA: 0s - loss: 9.0129 - acc: 0.365 - ETA: 0s - loss: 9.0345 - acc: 0.364 - ETA: 0s - loss: 9.0383 - acc: 0.364 - ETA: 0s - loss: 9.0439 - acc: 0.364 - ETA: 0s - loss: 9.0442 - acc: 0.364 - ETA: 0s - loss: 9.0526 - acc: 0.364 - ETA: 0s - loss: 9.0614 - acc: 0.3634Epoch 00002: val_loss improved from 9.56559 to 9.26612, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.0658 - acc: 0.3633 - val_loss: 9.2661 - val_acc: 0.3353\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.9794 - acc: 0.400 - ETA: 2s - loss: 8.4679 - acc: 0.412 - ETA: 2s - loss: 8.8932 - acc: 0.385 - ETA: 2s - loss: 8.8357 - acc: 0.389 - ETA: 3s - loss: 8.8266 - acc: 0.383 - ETA: 3s - loss: 9.0094 - acc: 0.377 - ETA: 2s - loss: 9.0092 - acc: 0.378 - ETA: 2s - loss: 8.8964 - acc: 0.386 - ETA: 2s - loss: 8.9507 - acc: 0.381 - ETA: 2s - loss: 8.9373 - acc: 0.385 - ETA: 2s - loss: 8.9097 - acc: 0.390 - ETA: 2s - loss: 8.9495 - acc: 0.389 - ETA: 2s - loss: 8.8544 - acc: 0.392 - ETA: 2s - loss: 8.8253 - acc: 0.395 - ETA: 2s - loss: 8.8264 - acc: 0.393 - ETA: 2s - loss: 8.8275 - acc: 0.391 - ETA: 2s - loss: 8.7796 - acc: 0.392 - ETA: 2s - loss: 8.7208 - acc: 0.395 - ETA: 2s - loss: 8.7322 - acc: 0.395 - ETA: 2s - loss: 8.7919 - acc: 0.391 - ETA: 2s - loss: 8.7541 - acc: 0.396 - ETA: 2s - loss: 8.7266 - acc: 0.397 - ETA: 2s - loss: 8.7500 - acc: 0.396 - ETA: 2s - loss: 8.8045 - acc: 0.393 - ETA: 2s - loss: 8.8165 - acc: 0.393 - ETA: 2s - loss: 8.9158 - acc: 0.386 - ETA: 1s - loss: 8.8935 - acc: 0.386 - ETA: 1s - loss: 8.8527 - acc: 0.387 - ETA: 1s - loss: 8.8559 - acc: 0.388 - ETA: 1s - loss: 8.8524 - acc: 0.389 - ETA: 1s - loss: 8.8232 - acc: 0.392 - ETA: 1s - loss: 8.8382 - acc: 0.390 - ETA: 1s - loss: 8.8020 - acc: 0.392 - ETA: 1s - loss: 8.8407 - acc: 0.390 - ETA: 1s - loss: 8.8171 - acc: 0.392 - ETA: 1s - loss: 8.8453 - acc: 0.391 - ETA: 1s - loss: 8.8086 - acc: 0.394 - ETA: 1s - loss: 8.8243 - acc: 0.392 - ETA: 1s - loss: 8.8252 - acc: 0.391 - ETA: 1s - loss: 8.8403 - acc: 0.392 - ETA: 1s - loss: 8.8776 - acc: 0.389 - ETA: 0s - loss: 8.8861 - acc: 0.389 - ETA: 0s - loss: 8.8542 - acc: 0.391 - ETA: 0s - loss: 8.8137 - acc: 0.394 - ETA: 0s - loss: 8.8095 - acc: 0.394 - ETA: 0s - loss: 8.7821 - acc: 0.395 - ETA: 0s - loss: 8.8001 - acc: 0.395 - ETA: 0s - loss: 8.7862 - acc: 0.395 - ETA: 0s - loss: 8.7755 - acc: 0.396 - ETA: 0s - loss: 8.7635 - acc: 0.397 - ETA: 0s - loss: 8.7298 - acc: 0.399 - ETA: 0s - loss: 8.7401 - acc: 0.398 - ETA: 0s - loss: 8.7515 - acc: 0.398 - ETA: 0s - loss: 8.7154 - acc: 0.400 - ETA: 0s - loss: 8.7054 - acc: 0.400 - ETA: 0s - loss: 8.6934 - acc: 0.402 - ETA: 0s - loss: 8.7035 - acc: 0.4017Epoch 00003: val_loss improved from 9.26612 to 9.08477, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.6980 - acc: 0.4016 - val_loss: 9.0848 - val_acc: 0.3485\n",
      "Epoch 5/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 8.0972 - acc: 0.500 - ETA: 2s - loss: 8.3031 - acc: 0.466 - ETA: 2s - loss: 8.3628 - acc: 0.453 - ETA: 2s - loss: 7.9424 - acc: 0.485 - ETA: 2s - loss: 8.1705 - acc: 0.468 - ETA: 2s - loss: 8.3618 - acc: 0.456 - ETA: 2s - loss: 8.5034 - acc: 0.447 - ETA: 2s - loss: 8.6147 - acc: 0.441 - ETA: 2s - loss: 8.6114 - acc: 0.439 - ETA: 2s - loss: 8.5786 - acc: 0.437 - ETA: 2s - loss: 8.3418 - acc: 0.448 - ETA: 2s - loss: 8.5052 - acc: 0.438 - ETA: 2s - loss: 8.5168 - acc: 0.437 - ETA: 2s - loss: 8.4873 - acc: 0.441 - ETA: 2s - loss: 8.5360 - acc: 0.437 - ETA: 1s - loss: 8.4726 - acc: 0.439 - ETA: 1s - loss: 8.5078 - acc: 0.438 - ETA: 1s - loss: 8.5468 - acc: 0.435 - ETA: 1s - loss: 8.5323 - acc: 0.434 - ETA: 1s - loss: 8.5257 - acc: 0.435 - ETA: 1s - loss: 8.5211 - acc: 0.435 - ETA: 1s - loss: 8.5439 - acc: 0.433 - ETA: 1s - loss: 8.5961 - acc: 0.429 - ETA: 1s - loss: 8.6048 - acc: 0.428 - ETA: 1s - loss: 8.6132 - acc: 0.428 - ETA: 1s - loss: 8.6264 - acc: 0.427 - ETA: 1s - loss: 8.6062 - acc: 0.428 - ETA: 1s - loss: 8.5717 - acc: 0.430 - ETA: 1s - loss: 8.6076 - acc: 0.428 - ETA: 1s - loss: 8.6042 - acc: 0.429 - ETA: 1s - loss: 8.5876 - acc: 0.429 - ETA: 1s - loss: 8.5998 - acc: 0.428 - ETA: 1s - loss: 8.6087 - acc: 0.428 - ETA: 0s - loss: 8.5958 - acc: 0.429 - ETA: 0s - loss: 8.5862 - acc: 0.429 - ETA: 0s - loss: 8.5602 - acc: 0.430 - ETA: 0s - loss: 8.5552 - acc: 0.430 - ETA: 0s - loss: 8.5455 - acc: 0.431 - ETA: 0s - loss: 8.5392 - acc: 0.431 - ETA: 0s - loss: 8.5428 - acc: 0.431 - ETA: 0s - loss: 8.5321 - acc: 0.431 - ETA: 0s - loss: 8.5292 - acc: 0.431 - ETA: 0s - loss: 8.5316 - acc: 0.431 - ETA: 0s - loss: 8.5345 - acc: 0.431 - ETA: 0s - loss: 8.5166 - acc: 0.432 - ETA: 0s - loss: 8.5138 - acc: 0.432 - ETA: 0s - loss: 8.5347 - acc: 0.431 - ETA: 0s - loss: 8.5358 - acc: 0.431 - ETA: 0s - loss: 8.5407 - acc: 0.431 - ETA: 0s - loss: 8.5461 - acc: 0.431 - ETA: 0s - loss: 8.5416 - acc: 0.432 - ETA: 0s - loss: 8.5317 - acc: 0.4328Epoch 00004: val_loss improved from 9.08477 to 8.97910, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 8.5435 - acc: 0.4323 - val_loss: 8.9791 - val_acc: 0.3653\n",
      "Epoch 6/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 9.6826 - acc: 0.400 - ETA: 2s - loss: 8.5476 - acc: 0.437 - ETA: 2s - loss: 7.8174 - acc: 0.493 - ETA: 2s - loss: 7.7896 - acc: 0.488 - ETA: 2s - loss: 7.9938 - acc: 0.475 - ETA: 2s - loss: 8.0300 - acc: 0.475 - ETA: 2s - loss: 8.1045 - acc: 0.467 - ETA: 2s - loss: 8.0792 - acc: 0.466 - ETA: 2s - loss: 8.1740 - acc: 0.462 - ETA: 2s - loss: 8.1856 - acc: 0.460 - ETA: 2s - loss: 8.0830 - acc: 0.466 - ETA: 2s - loss: 8.1616 - acc: 0.459 - ETA: 2s - loss: 8.1940 - acc: 0.458 - ETA: 2s - loss: 8.1666 - acc: 0.460 - ETA: 1s - loss: 8.2495 - acc: 0.455 - ETA: 1s - loss: 8.3360 - acc: 0.450 - ETA: 1s - loss: 8.3988 - acc: 0.446 - ETA: 1s - loss: 8.4456 - acc: 0.443 - ETA: 1s - loss: 8.4755 - acc: 0.442 - ETA: 1s - loss: 8.4657 - acc: 0.443 - ETA: 1s - loss: 8.4269 - acc: 0.445 - ETA: 1s - loss: 8.4514 - acc: 0.444 - ETA: 1s - loss: 8.4192 - acc: 0.445 - ETA: 1s - loss: 8.4121 - acc: 0.444 - ETA: 1s - loss: 8.4227 - acc: 0.444 - ETA: 1s - loss: 8.4862 - acc: 0.440 - ETA: 1s - loss: 8.4908 - acc: 0.439 - ETA: 1s - loss: 8.5257 - acc: 0.437 - ETA: 1s - loss: 8.5097 - acc: 0.438 - ETA: 1s - loss: 8.5040 - acc: 0.439 - ETA: 1s - loss: 8.5086 - acc: 0.439 - ETA: 1s - loss: 8.5420 - acc: 0.437 - ETA: 1s - loss: 8.5102 - acc: 0.438 - ETA: 1s - loss: 8.5168 - acc: 0.438 - ETA: 1s - loss: 8.5032 - acc: 0.438 - ETA: 0s - loss: 8.5019 - acc: 0.438 - ETA: 0s - loss: 8.4697 - acc: 0.440 - ETA: 0s - loss: 8.4828 - acc: 0.439 - ETA: 0s - loss: 8.4818 - acc: 0.439 - ETA: 0s - loss: 8.4807 - acc: 0.439 - ETA: 0s - loss: 8.4710 - acc: 0.440 - ETA: 0s - loss: 8.4638 - acc: 0.441 - ETA: 0s - loss: 8.4658 - acc: 0.440 - ETA: 0s - loss: 8.4448 - acc: 0.441 - ETA: 0s - loss: 8.4387 - acc: 0.441 - ETA: 0s - loss: 8.4594 - acc: 0.439 - ETA: 0s - loss: 8.4514 - acc: 0.439 - ETA: 0s - loss: 8.4623 - acc: 0.439 - ETA: 0s - loss: 8.4528 - acc: 0.439 - ETA: 0s - loss: 8.4567 - acc: 0.439 - ETA: 0s - loss: 8.4708 - acc: 0.438 - ETA: 0s - loss: 8.4545 - acc: 0.439 - ETA: 0s - loss: 8.4456 - acc: 0.4404Epoch 00005: val_loss improved from 8.97910 to 8.86312, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.4415 - acc: 0.4413 - val_loss: 8.8631 - val_acc: 0.3653\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 2s - loss: 5.8925 - acc: 0.600 - ETA: 2s - loss: 7.7709 - acc: 0.481 - ETA: 2s - loss: 8.2354 - acc: 0.460 - ETA: 2s - loss: 7.8487 - acc: 0.480 - ETA: 2s - loss: 7.8435 - acc: 0.480 - ETA: 2s - loss: 7.9737 - acc: 0.470 - ETA: 2s - loss: 7.8617 - acc: 0.469 - ETA: 2s - loss: 8.0037 - acc: 0.461 - ETA: 2s - loss: 7.9440 - acc: 0.464 - ETA: 2s - loss: 7.9274 - acc: 0.465 - ETA: 2s - loss: 7.8252 - acc: 0.474 - ETA: 2s - loss: 7.8610 - acc: 0.473 - ETA: 2s - loss: 7.8706 - acc: 0.474 - ETA: 2s - loss: 7.9555 - acc: 0.470 - ETA: 2s - loss: 7.9444 - acc: 0.471 - ETA: 1s - loss: 7.9369 - acc: 0.471 - ETA: 1s - loss: 7.9458 - acc: 0.469 - ETA: 1s - loss: 7.9785 - acc: 0.467 - ETA: 1s - loss: 7.9477 - acc: 0.469 - ETA: 1s - loss: 7.9666 - acc: 0.468 - ETA: 1s - loss: 8.0197 - acc: 0.465 - ETA: 1s - loss: 8.0520 - acc: 0.464 - ETA: 1s - loss: 8.0553 - acc: 0.463 - ETA: 1s - loss: 8.0669 - acc: 0.462 - ETA: 1s - loss: 8.0624 - acc: 0.461 - ETA: 1s - loss: 8.0922 - acc: 0.460 - ETA: 1s - loss: 8.1298 - acc: 0.458 - ETA: 1s - loss: 8.1199 - acc: 0.457 - ETA: 1s - loss: 8.1520 - acc: 0.455 - ETA: 1s - loss: 8.1671 - acc: 0.454 - ETA: 1s - loss: 8.1869 - acc: 0.453 - ETA: 1s - loss: 8.2089 - acc: 0.452 - ETA: 1s - loss: 8.2118 - acc: 0.452 - ETA: 1s - loss: 8.2079 - acc: 0.452 - ETA: 1s - loss: 8.2188 - acc: 0.452 - ETA: 1s - loss: 8.2100 - acc: 0.452 - ETA: 0s - loss: 8.1973 - acc: 0.453 - ETA: 0s - loss: 8.1917 - acc: 0.453 - ETA: 0s - loss: 8.1495 - acc: 0.456 - ETA: 0s - loss: 8.1441 - acc: 0.456 - ETA: 0s - loss: 8.1685 - acc: 0.455 - ETA: 0s - loss: 8.1844 - acc: 0.454 - ETA: 0s - loss: 8.1715 - acc: 0.454 - ETA: 0s - loss: 8.1838 - acc: 0.453 - ETA: 0s - loss: 8.1828 - acc: 0.452 - ETA: 0s - loss: 8.1940 - acc: 0.452 - ETA: 0s - loss: 8.1809 - acc: 0.453 - ETA: 0s - loss: 8.1445 - acc: 0.454 - ETA: 0s - loss: 8.1400 - acc: 0.455 - ETA: 0s - loss: 8.1246 - acc: 0.455 - ETA: 0s - loss: 8.1378 - acc: 0.454 - ETA: 0s - loss: 8.1283 - acc: 0.453 - ETA: 0s - loss: 8.1355 - acc: 0.453 - ETA: 0s - loss: 8.1363 - acc: 0.4541Epoch 00006: val_loss improved from 8.86312 to 8.55766, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1262 - acc: 0.4548 - val_loss: 8.5577 - val_acc: 0.3796\n",
      "Epoch 8/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.0624 - acc: 0.500 - ETA: 2s - loss: 8.9425 - acc: 0.425 - ETA: 2s - loss: 8.3807 - acc: 0.466 - ETA: 2s - loss: 7.8419 - acc: 0.490 - ETA: 2s - loss: 8.0659 - acc: 0.470 - ETA: 2s - loss: 8.0478 - acc: 0.471 - ETA: 2s - loss: 8.0580 - acc: 0.472 - ETA: 2s - loss: 8.1465 - acc: 0.467 - ETA: 2s - loss: 8.1761 - acc: 0.467 - ETA: 2s - loss: 8.1745 - acc: 0.466 - ETA: 2s - loss: 8.2338 - acc: 0.462 - ETA: 2s - loss: 8.3088 - acc: 0.460 - ETA: 2s - loss: 8.3437 - acc: 0.458 - ETA: 2s - loss: 8.4220 - acc: 0.453 - ETA: 1s - loss: 8.4799 - acc: 0.448 - ETA: 1s - loss: 8.4099 - acc: 0.452 - ETA: 1s - loss: 8.3243 - acc: 0.457 - ETA: 1s - loss: 8.2377 - acc: 0.463 - ETA: 1s - loss: 8.2159 - acc: 0.464 - ETA: 1s - loss: 8.1176 - acc: 0.470 - ETA: 1s - loss: 8.1201 - acc: 0.469 - ETA: 1s - loss: 8.0704 - acc: 0.471 - ETA: 1s - loss: 8.0257 - acc: 0.474 - ETA: 1s - loss: 7.9963 - acc: 0.476 - ETA: 1s - loss: 7.9742 - acc: 0.478 - ETA: 1s - loss: 7.9223 - acc: 0.480 - ETA: 1s - loss: 7.8463 - acc: 0.484 - ETA: 1s - loss: 7.8534 - acc: 0.484 - ETA: 1s - loss: 7.8455 - acc: 0.484 - ETA: 1s - loss: 7.8355 - acc: 0.485 - ETA: 1s - loss: 7.8610 - acc: 0.484 - ETA: 1s - loss: 7.8668 - acc: 0.483 - ETA: 1s - loss: 7.9114 - acc: 0.481 - ETA: 1s - loss: 7.9145 - acc: 0.481 - ETA: 0s - loss: 7.9135 - acc: 0.481 - ETA: 0s - loss: 7.9089 - acc: 0.481 - ETA: 0s - loss: 7.9284 - acc: 0.480 - ETA: 0s - loss: 7.9163 - acc: 0.481 - ETA: 0s - loss: 7.8982 - acc: 0.483 - ETA: 0s - loss: 7.9013 - acc: 0.483 - ETA: 0s - loss: 7.9085 - acc: 0.482 - ETA: 0s - loss: 7.9223 - acc: 0.481 - ETA: 0s - loss: 7.9442 - acc: 0.480 - ETA: 0s - loss: 7.9490 - acc: 0.480 - ETA: 0s - loss: 7.9510 - acc: 0.480 - ETA: 0s - loss: 7.9525 - acc: 0.480 - ETA: 0s - loss: 7.9451 - acc: 0.480 - ETA: 0s - loss: 7.9321 - acc: 0.481 - ETA: 0s - loss: 7.9309 - acc: 0.481 - ETA: 0s - loss: 7.9299 - acc: 0.481 - ETA: 0s - loss: 7.9238 - acc: 0.482 - ETA: 0s - loss: 7.9403 - acc: 0.480 - ETA: 0s - loss: 7.9492 - acc: 0.480 - ETA: 0s - loss: 7.9342 - acc: 0.481 - ETA: 0s - loss: 7.9187 - acc: 0.482 - ETA: 0s - loss: 7.9286 - acc: 0.481 - ETA: 0s - loss: 7.9321 - acc: 0.4821Epoch 00007: val_loss improved from 8.55766 to 8.48869, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9353 - acc: 0.4820 - val_loss: 8.4887 - val_acc: 0.3964\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 8.9276 - acc: 0.450 - ETA: 2s - loss: 8.0834 - acc: 0.485 - ETA: 3s - loss: 7.8735 - acc: 0.504 - ETA: 3s - loss: 7.9678 - acc: 0.491 - ETA: 3s - loss: 7.8575 - acc: 0.497 - ETA: 3s - loss: 7.8642 - acc: 0.496 - ETA: 3s - loss: 7.8740 - acc: 0.494 - ETA: 3s - loss: 7.7511 - acc: 0.501 - ETA: 3s - loss: 7.5891 - acc: 0.512 - ETA: 3s - loss: 7.6388 - acc: 0.510 - ETA: 3s - loss: 7.6650 - acc: 0.507 - ETA: 3s - loss: 7.6518 - acc: 0.508 - ETA: 3s - loss: 7.5854 - acc: 0.514 - ETA: 2s - loss: 7.5646 - acc: 0.514 - ETA: 2s - loss: 7.5228 - acc: 0.517 - ETA: 2s - loss: 7.6034 - acc: 0.512 - ETA: 2s - loss: 7.6484 - acc: 0.509 - ETA: 2s - loss: 7.7570 - acc: 0.500 - ETA: 2s - loss: 7.7883 - acc: 0.498 - ETA: 2s - loss: 7.8564 - acc: 0.494 - ETA: 2s - loss: 7.8314 - acc: 0.496 - ETA: 2s - loss: 7.7959 - acc: 0.498 - ETA: 2s - loss: 7.8334 - acc: 0.496 - ETA: 2s - loss: 7.8242 - acc: 0.496 - ETA: 2s - loss: 7.7615 - acc: 0.499 - ETA: 2s - loss: 7.7529 - acc: 0.500 - ETA: 2s - loss: 7.8011 - acc: 0.496 - ETA: 2s - loss: 7.7979 - acc: 0.496 - ETA: 2s - loss: 7.8479 - acc: 0.493 - ETA: 1s - loss: 7.8949 - acc: 0.490 - ETA: 1s - loss: 7.8972 - acc: 0.490 - ETA: 1s - loss: 7.8739 - acc: 0.492 - ETA: 1s - loss: 7.8993 - acc: 0.490 - ETA: 1s - loss: 7.8634 - acc: 0.491 - ETA: 1s - loss: 7.8771 - acc: 0.491 - ETA: 1s - loss: 7.8672 - acc: 0.491 - ETA: 1s - loss: 7.8783 - acc: 0.491 - ETA: 1s - loss: 7.8493 - acc: 0.494 - ETA: 1s - loss: 7.8648 - acc: 0.493 - ETA: 1s - loss: 7.8555 - acc: 0.493 - ETA: 1s - loss: 7.8455 - acc: 0.493 - ETA: 1s - loss: 7.8613 - acc: 0.492 - ETA: 0s - loss: 7.8530 - acc: 0.492 - ETA: 0s - loss: 7.8773 - acc: 0.490 - ETA: 0s - loss: 7.8611 - acc: 0.491 - ETA: 0s - loss: 7.8744 - acc: 0.490 - ETA: 0s - loss: 7.8902 - acc: 0.489 - ETA: 0s - loss: 7.8774 - acc: 0.489 - ETA: 0s - loss: 7.8927 - acc: 0.489 - ETA: 0s - loss: 7.9126 - acc: 0.488 - ETA: 0s - loss: 7.8945 - acc: 0.488 - ETA: 0s - loss: 7.8804 - acc: 0.489 - ETA: 0s - loss: 7.8938 - acc: 0.489 - ETA: 0s - loss: 7.8926 - acc: 0.489 - ETA: 0s - loss: 7.8874 - acc: 0.489 - ETA: 0s - loss: 7.8886 - acc: 0.490 - ETA: 0s - loss: 7.8917 - acc: 0.489 - ETA: 0s - loss: 7.8806 - acc: 0.4908Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.8854 - acc: 0.4906 - val_loss: 8.4947 - val_acc: 0.3964\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 2s - loss: 9.7540 - acc: 0.350 - ETA: 2s - loss: 8.1099 - acc: 0.481 - ETA: 2s - loss: 8.0790 - acc: 0.473 - ETA: 2s - loss: 8.0957 - acc: 0.475 - ETA: 2s - loss: 7.7993 - acc: 0.496 - ETA: 2s - loss: 7.8343 - acc: 0.495 - ETA: 2s - loss: 7.8246 - acc: 0.497 - ETA: 2s - loss: 7.7275 - acc: 0.504 - ETA: 2s - loss: 7.7849 - acc: 0.500 - ETA: 2s - loss: 7.8149 - acc: 0.500 - ETA: 2s - loss: 7.8491 - acc: 0.495 - ETA: 2s - loss: 7.8003 - acc: 0.497 - ETA: 2s - loss: 7.7821 - acc: 0.498 - ETA: 2s - loss: 7.7909 - acc: 0.497 - ETA: 1s - loss: 7.8058 - acc: 0.496 - ETA: 1s - loss: 7.8095 - acc: 0.496 - ETA: 1s - loss: 7.7528 - acc: 0.500 - ETA: 1s - loss: 7.7963 - acc: 0.498 - ETA: 1s - loss: 7.8190 - acc: 0.496 - ETA: 1s - loss: 7.7939 - acc: 0.496 - ETA: 1s - loss: 7.7926 - acc: 0.496 - ETA: 1s - loss: 7.7577 - acc: 0.498 - ETA: 1s - loss: 7.7547 - acc: 0.499 - ETA: 1s - loss: 7.7510 - acc: 0.500 - ETA: 1s - loss: 7.7656 - acc: 0.499 - ETA: 1s - loss: 7.7591 - acc: 0.499 - ETA: 1s - loss: 7.7257 - acc: 0.501 - ETA: 1s - loss: 7.7307 - acc: 0.501 - ETA: 1s - loss: 7.7531 - acc: 0.500 - ETA: 1s - loss: 7.7252 - acc: 0.501 - ETA: 1s - loss: 7.7584 - acc: 0.500 - ETA: 1s - loss: 7.7429 - acc: 0.500 - ETA: 1s - loss: 7.7587 - acc: 0.499 - ETA: 0s - loss: 7.7834 - acc: 0.497 - ETA: 0s - loss: 7.7818 - acc: 0.497 - ETA: 0s - loss: 7.7614 - acc: 0.498 - ETA: 0s - loss: 7.7711 - acc: 0.497 - ETA: 0s - loss: 7.7614 - acc: 0.497 - ETA: 0s - loss: 7.7573 - acc: 0.498 - ETA: 0s - loss: 7.7665 - acc: 0.497 - ETA: 0s - loss: 7.7882 - acc: 0.496 - ETA: 0s - loss: 7.7748 - acc: 0.497 - ETA: 0s - loss: 7.7757 - acc: 0.496 - ETA: 0s - loss: 7.7712 - acc: 0.496 - ETA: 0s - loss: 7.7703 - acc: 0.496 - ETA: 0s - loss: 7.7620 - acc: 0.497 - ETA: 0s - loss: 7.7753 - acc: 0.496 - ETA: 0s - loss: 7.7649 - acc: 0.496 - ETA: 0s - loss: 7.7907 - acc: 0.495 - ETA: 0s - loss: 7.7918 - acc: 0.495 - ETA: 0s - loss: 7.7978 - acc: 0.495 - ETA: 0s - loss: 7.7896 - acc: 0.4961Epoch 00009: val_loss improved from 8.48869 to 8.44738, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 7.7904 - acc: 0.4961 - val_loss: 8.4474 - val_acc: 0.3856\n",
      "Epoch 11/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 8.0629 - acc: 0.500 - ETA: 2s - loss: 8.3710 - acc: 0.481 - ETA: 2s - loss: 8.0214 - acc: 0.496 - ETA: 2s - loss: 7.8366 - acc: 0.504 - ETA: 2s - loss: 7.6698 - acc: 0.508 - ETA: 2s - loss: 7.6873 - acc: 0.505 - ETA: 2s - loss: 7.6311 - acc: 0.509 - ETA: 2s - loss: 7.7846 - acc: 0.502 - ETA: 2s - loss: 7.8589 - acc: 0.499 - ETA: 2s - loss: 7.9465 - acc: 0.493 - ETA: 2s - loss: 7.8848 - acc: 0.498 - ETA: 2s - loss: 7.8827 - acc: 0.498 - ETA: 2s - loss: 7.8013 - acc: 0.502 - ETA: 2s - loss: 7.7554 - acc: 0.505 - ETA: 1s - loss: 7.8178 - acc: 0.501 - ETA: 1s - loss: 7.7623 - acc: 0.503 - ETA: 1s - loss: 7.8717 - acc: 0.497 - ETA: 1s - loss: 7.9064 - acc: 0.493 - ETA: 1s - loss: 7.8756 - acc: 0.496 - ETA: 1s - loss: 7.8229 - acc: 0.499 - ETA: 1s - loss: 7.8233 - acc: 0.499 - ETA: 1s - loss: 7.8227 - acc: 0.500 - ETA: 1s - loss: 7.8629 - acc: 0.497 - ETA: 1s - loss: 7.8458 - acc: 0.499 - ETA: 1s - loss: 7.8136 - acc: 0.501 - ETA: 1s - loss: 7.8232 - acc: 0.500 - ETA: 1s - loss: 7.8286 - acc: 0.500 - ETA: 1s - loss: 7.7990 - acc: 0.502 - ETA: 1s - loss: 7.7970 - acc: 0.502 - ETA: 1s - loss: 7.8000 - acc: 0.502 - ETA: 1s - loss: 7.8342 - acc: 0.500 - ETA: 1s - loss: 7.8312 - acc: 0.500 - ETA: 1s - loss: 7.8272 - acc: 0.501 - ETA: 0s - loss: 7.7792 - acc: 0.504 - ETA: 0s - loss: 7.7755 - acc: 0.504 - ETA: 0s - loss: 7.7971 - acc: 0.503 - ETA: 0s - loss: 7.7790 - acc: 0.504 - ETA: 0s - loss: 7.7918 - acc: 0.504 - ETA: 0s - loss: 7.7969 - acc: 0.503 - ETA: 0s - loss: 7.8256 - acc: 0.502 - ETA: 0s - loss: 7.7941 - acc: 0.504 - ETA: 0s - loss: 7.7722 - acc: 0.505 - ETA: 0s - loss: 7.7704 - acc: 0.505 - ETA: 0s - loss: 7.7823 - acc: 0.503 - ETA: 0s - loss: 7.7847 - acc: 0.503 - ETA: 0s - loss: 7.7639 - acc: 0.504 - ETA: 0s - loss: 7.7559 - acc: 0.504 - ETA: 0s - loss: 7.7262 - acc: 0.505 - ETA: 0s - loss: 7.7408 - acc: 0.504 - ETA: 0s - loss: 7.7055 - acc: 0.506 - ETA: 0s - loss: 7.7174 - acc: 0.505 - ETA: 0s - loss: 7.7274 - acc: 0.5050Epoch 00010: val_loss improved from 8.44738 to 8.36977, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 7.7235 - acc: 0.5049 - val_loss: 8.3698 - val_acc: 0.3940\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 7.2718 - acc: 0.550 - ETA: 2s - loss: 8.3997 - acc: 0.462 - ETA: 2s - loss: 7.7640 - acc: 0.496 - ETA: 2s - loss: 7.9265 - acc: 0.492 - ETA: 2s - loss: 7.8496 - acc: 0.498 - ETA: 2s - loss: 7.7757 - acc: 0.504 - ETA: 2s - loss: 7.6088 - acc: 0.515 - ETA: 2s - loss: 7.5748 - acc: 0.518 - ETA: 2s - loss: 7.5428 - acc: 0.520 - ETA: 2s - loss: 7.4925 - acc: 0.525 - ETA: 2s - loss: 7.6592 - acc: 0.514 - ETA: 2s - loss: 7.5824 - acc: 0.520 - ETA: 2s - loss: 7.6610 - acc: 0.515 - ETA: 2s - loss: 7.6544 - acc: 0.515 - ETA: 1s - loss: 7.6572 - acc: 0.515 - ETA: 1s - loss: 7.7288 - acc: 0.510 - ETA: 1s - loss: 7.7127 - acc: 0.511 - ETA: 1s - loss: 7.7393 - acc: 0.510 - ETA: 1s - loss: 7.6705 - acc: 0.514 - ETA: 1s - loss: 7.6607 - acc: 0.515 - ETA: 1s - loss: 7.6410 - acc: 0.515 - ETA: 1s - loss: 7.6760 - acc: 0.513 - ETA: 1s - loss: 7.6618 - acc: 0.514 - ETA: 1s - loss: 7.6751 - acc: 0.513 - ETA: 1s - loss: 7.7248 - acc: 0.510 - ETA: 1s - loss: 7.7019 - acc: 0.512 - ETA: 1s - loss: 7.6803 - acc: 0.513 - ETA: 1s - loss: 7.6879 - acc: 0.512 - ETA: 1s - loss: 7.6756 - acc: 0.513 - ETA: 1s - loss: 7.6720 - acc: 0.513 - ETA: 1s - loss: 7.6219 - acc: 0.515 - ETA: 1s - loss: 7.6197 - acc: 0.515 - ETA: 1s - loss: 7.6394 - acc: 0.514 - ETA: 0s - loss: 7.6279 - acc: 0.514 - ETA: 0s - loss: 7.6035 - acc: 0.516 - ETA: 0s - loss: 7.5911 - acc: 0.516 - ETA: 0s - loss: 7.5953 - acc: 0.516 - ETA: 0s - loss: 7.5941 - acc: 0.517 - ETA: 0s - loss: 7.5837 - acc: 0.517 - ETA: 0s - loss: 7.5887 - acc: 0.517 - ETA: 0s - loss: 7.5705 - acc: 0.518 - ETA: 0s - loss: 7.5592 - acc: 0.519 - ETA: 0s - loss: 7.5495 - acc: 0.520 - ETA: 0s - loss: 7.5513 - acc: 0.519 - ETA: 0s - loss: 7.5461 - acc: 0.520 - ETA: 0s - loss: 7.5538 - acc: 0.519 - ETA: 0s - loss: 7.5512 - acc: 0.519 - ETA: 0s - loss: 7.5661 - acc: 0.519 - ETA: 0s - loss: 7.5610 - acc: 0.519 - ETA: 0s - loss: 7.5724 - acc: 0.518 - ETA: 0s - loss: 7.5756 - acc: 0.518 - ETA: 0s - loss: 7.5763 - acc: 0.5186Epoch 00011: val_loss improved from 8.36977 to 8.29199, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 7.5652 - acc: 0.5195 - val_loss: 8.2920 - val_acc: 0.4132\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 5s - loss: 6.4486 - acc: 0.600 - ETA: 2s - loss: 7.4983 - acc: 0.531 - ETA: 2s - loss: 7.4208 - acc: 0.528 - ETA: 2s - loss: 7.2505 - acc: 0.542 - ETA: 2s - loss: 7.2519 - acc: 0.544 - ETA: 2s - loss: 7.0686 - acc: 0.554 - ETA: 2s - loss: 7.2626 - acc: 0.542 - ETA: 2s - loss: 7.2989 - acc: 0.540 - ETA: 2s - loss: 7.2797 - acc: 0.542 - ETA: 2s - loss: 7.2502 - acc: 0.545 - ETA: 2s - loss: 7.3512 - acc: 0.538 - ETA: 2s - loss: 7.3920 - acc: 0.534 - ETA: 2s - loss: 7.4139 - acc: 0.533 - ETA: 2s - loss: 7.4384 - acc: 0.531 - ETA: 2s - loss: 7.5078 - acc: 0.527 - ETA: 1s - loss: 7.5278 - acc: 0.526 - ETA: 1s - loss: 7.5519 - acc: 0.523 - ETA: 1s - loss: 7.5824 - acc: 0.521 - ETA: 1s - loss: 7.6474 - acc: 0.517 - ETA: 1s - loss: 7.6223 - acc: 0.518 - ETA: 1s - loss: 7.6088 - acc: 0.519 - ETA: 1s - loss: 7.5985 - acc: 0.520 - ETA: 1s - loss: 7.5673 - acc: 0.522 - ETA: 1s - loss: 7.5602 - acc: 0.523 - ETA: 1s - loss: 7.5480 - acc: 0.523 - ETA: 1s - loss: 7.5275 - acc: 0.525 - ETA: 1s - loss: 7.4966 - acc: 0.526 - ETA: 1s - loss: 7.4872 - acc: 0.527 - ETA: 1s - loss: 7.5168 - acc: 0.525 - ETA: 1s - loss: 7.5248 - acc: 0.525 - ETA: 1s - loss: 7.5374 - acc: 0.524 - ETA: 1s - loss: 7.5572 - acc: 0.523 - ETA: 1s - loss: 7.5459 - acc: 0.523 - ETA: 1s - loss: 7.5315 - acc: 0.524 - ETA: 0s - loss: 7.5397 - acc: 0.523 - ETA: 0s - loss: 7.5381 - acc: 0.522 - ETA: 0s - loss: 7.5571 - acc: 0.521 - ETA: 0s - loss: 7.5408 - acc: 0.522 - ETA: 0s - loss: 7.5199 - acc: 0.523 - ETA: 0s - loss: 7.5249 - acc: 0.523 - ETA: 0s - loss: 7.5152 - acc: 0.523 - ETA: 0s - loss: 7.5246 - acc: 0.523 - ETA: 0s - loss: 7.5347 - acc: 0.522 - ETA: 0s - loss: 7.5108 - acc: 0.524 - ETA: 0s - loss: 7.5087 - acc: 0.524 - ETA: 0s - loss: 7.5001 - acc: 0.524 - ETA: 0s - loss: 7.5122 - acc: 0.523 - ETA: 0s - loss: 7.5069 - acc: 0.523 - ETA: 0s - loss: 7.5062 - acc: 0.523 - ETA: 0s - loss: 7.5077 - acc: 0.523 - ETA: 0s - loss: 7.5077 - acc: 0.523 - ETA: 0s - loss: 7.5079 - acc: 0.523 - ETA: 0s - loss: 7.5034 - acc: 0.523 - ETA: 0s - loss: 7.5044 - acc: 0.523 - ETA: 0s - loss: 7.5159 - acc: 0.5228Epoch 00012: val_loss improved from 8.29199 to 8.28814, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.5192 - acc: 0.5225 - val_loss: 8.2881 - val_acc: 0.4096\n",
      "Epoch 14/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 6.5561 - acc: 0.500 - ETA: 4s - loss: 6.9538 - acc: 0.533 - ETA: 4s - loss: 6.9758 - acc: 0.533 - ETA: 4s - loss: 7.5532 - acc: 0.507 - ETA: 4s - loss: 7.7107 - acc: 0.502 - ETA: 3s - loss: 7.6121 - acc: 0.513 - ETA: 3s - loss: 7.6062 - acc: 0.513 - ETA: 3s - loss: 7.7168 - acc: 0.508 - ETA: 3s - loss: 7.7489 - acc: 0.506 - ETA: 3s - loss: 7.7231 - acc: 0.508 - ETA: 3s - loss: 7.6116 - acc: 0.512 - ETA: 3s - loss: 7.6512 - acc: 0.510 - ETA: 3s - loss: 7.6939 - acc: 0.508 - ETA: 3s - loss: 7.6882 - acc: 0.507 - ETA: 3s - loss: 7.7430 - acc: 0.503 - ETA: 3s - loss: 7.6807 - acc: 0.507 - ETA: 2s - loss: 7.6997 - acc: 0.507 - ETA: 2s - loss: 7.6565 - acc: 0.510 - ETA: 2s - loss: 7.6392 - acc: 0.512 - ETA: 2s - loss: 7.6706 - acc: 0.511 - ETA: 2s - loss: 7.6499 - acc: 0.513 - ETA: 2s - loss: 7.6091 - acc: 0.515 - ETA: 2s - loss: 7.5792 - acc: 0.517 - ETA: 2s - loss: 7.5739 - acc: 0.517 - ETA: 2s - loss: 7.5288 - acc: 0.520 - ETA: 2s - loss: 7.5076 - acc: 0.521 - ETA: 2s - loss: 7.5306 - acc: 0.519 - ETA: 2s - loss: 7.5618 - acc: 0.516 - ETA: 2s - loss: 7.5755 - acc: 0.516 - ETA: 2s - loss: 7.5229 - acc: 0.518 - ETA: 1s - loss: 7.5270 - acc: 0.518 - ETA: 1s - loss: 7.5725 - acc: 0.515 - ETA: 1s - loss: 7.5440 - acc: 0.517 - ETA: 1s - loss: 7.5360 - acc: 0.518 - ETA: 1s - loss: 7.5192 - acc: 0.519 - ETA: 1s - loss: 7.5373 - acc: 0.518 - ETA: 1s - loss: 7.5261 - acc: 0.519 - ETA: 1s - loss: 7.5232 - acc: 0.519 - ETA: 1s - loss: 7.5071 - acc: 0.521 - ETA: 1s - loss: 7.4802 - acc: 0.522 - ETA: 1s - loss: 7.4717 - acc: 0.522 - ETA: 1s - loss: 7.4617 - acc: 0.523 - ETA: 1s - loss: 7.5127 - acc: 0.520 - ETA: 1s - loss: 7.5250 - acc: 0.519 - ETA: 1s - loss: 7.5080 - acc: 0.520 - ETA: 1s - loss: 7.5027 - acc: 0.520 - ETA: 0s - loss: 7.4740 - acc: 0.522 - ETA: 0s - loss: 7.4884 - acc: 0.521 - ETA: 0s - loss: 7.5337 - acc: 0.518 - ETA: 0s - loss: 7.5336 - acc: 0.519 - ETA: 0s - loss: 7.5334 - acc: 0.519 - ETA: 0s - loss: 7.5136 - acc: 0.520 - ETA: 0s - loss: 7.5072 - acc: 0.521 - ETA: 0s - loss: 7.5049 - acc: 0.521 - ETA: 0s - loss: 7.5049 - acc: 0.521 - ETA: 0s - loss: 7.4888 - acc: 0.522 - ETA: 0s - loss: 7.4718 - acc: 0.523 - ETA: 0s - loss: 7.4734 - acc: 0.523 - ETA: 0s - loss: 7.4684 - acc: 0.523 - ETA: 0s - loss: 7.4429 - acc: 0.524 - ETA: 0s - loss: 7.4530 - acc: 0.5241Epoch 00013: val_loss improved from 8.28814 to 8.14509, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.4409 - acc: 0.5249 - val_loss: 8.1451 - val_acc: 0.4204\n",
      "Epoch 15/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 7.1983 - acc: 0.5312 - ETA: 2s - loss: 6.9466 - acc: 0.546 - ETA: 2s - loss: 7.1593 - acc: 0.538 - ETA: 2s - loss: 7.0046 - acc: 0.550 - ETA: 2s - loss: 6.9919 - acc: 0.551 - ETA: 2s - loss: 7.1653 - acc: 0.541 - ETA: 2s - loss: 7.1520 - acc: 0.543 - ETA: 2s - loss: 7.1858 - acc: 0.542 - ETA: 2s - loss: 7.1553 - acc: 0.545 - ETA: 2s - loss: 7.1206 - acc: 0.547 - ETA: 2s - loss: 7.2014 - acc: 0.542 - ETA: 2s - loss: 7.3418 - acc: 0.532 - ETA: 2s - loss: 7.3304 - acc: 0.533 - ETA: 1s - loss: 7.2888 - acc: 0.535 - ETA: 1s - loss: 7.2685 - acc: 0.535 - ETA: 1s - loss: 7.3072 - acc: 0.533 - ETA: 1s - loss: 7.3073 - acc: 0.533 - ETA: 1s - loss: 7.3333 - acc: 0.532 - ETA: 1s - loss: 7.3155 - acc: 0.533 - ETA: 1s - loss: 7.2689 - acc: 0.537 - ETA: 1s - loss: 7.3063 - acc: 0.534 - ETA: 1s - loss: 7.3286 - acc: 0.533 - ETA: 1s - loss: 7.3109 - acc: 0.534 - ETA: 1s - loss: 7.3510 - acc: 0.532 - ETA: 1s - loss: 7.3050 - acc: 0.535 - ETA: 1s - loss: 7.3016 - acc: 0.535 - ETA: 1s - loss: 7.3295 - acc: 0.533 - ETA: 1s - loss: 7.3073 - acc: 0.534 - ETA: 1s - loss: 7.2817 - acc: 0.536 - ETA: 1s - loss: 7.3023 - acc: 0.535 - ETA: 1s - loss: 7.3130 - acc: 0.535 - ETA: 1s - loss: 7.2919 - acc: 0.536 - ETA: 1s - loss: 7.2685 - acc: 0.538 - ETA: 0s - loss: 7.2692 - acc: 0.537 - ETA: 0s - loss: 7.2693 - acc: 0.538 - ETA: 0s - loss: 7.2742 - acc: 0.537 - ETA: 0s - loss: 7.2742 - acc: 0.537 - ETA: 0s - loss: 7.2957 - acc: 0.536 - ETA: 0s - loss: 7.2755 - acc: 0.537 - ETA: 0s - loss: 7.3125 - acc: 0.535 - ETA: 0s - loss: 7.2903 - acc: 0.537 - ETA: 0s - loss: 7.2809 - acc: 0.537 - ETA: 0s - loss: 7.2804 - acc: 0.537 - ETA: 0s - loss: 7.2727 - acc: 0.538 - ETA: 0s - loss: 7.2585 - acc: 0.539 - ETA: 0s - loss: 7.2734 - acc: 0.538 - ETA: 0s - loss: 7.2760 - acc: 0.538 - ETA: 0s - loss: 7.3034 - acc: 0.536 - ETA: 0s - loss: 7.3114 - acc: 0.536 - ETA: 0s - loss: 7.3217 - acc: 0.535 - ETA: 0s - loss: 7.3388 - acc: 0.534 - ETA: 0s - loss: 7.3166 - acc: 0.536 - ETA: 0s - loss: 7.3211 - acc: 0.536 - ETA: 0s - loss: 7.3456 - acc: 0.534 - ETA: 0s - loss: 7.3450 - acc: 0.534 - ETA: 0s - loss: 7.3771 - acc: 0.5324Epoch 00014: val_loss improved from 8.14509 to 8.11506, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.3815 - acc: 0.5322 - val_loss: 8.1151 - val_acc: 0.4192\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 5.6414 - acc: 0.650 - ETA: 2s - loss: 7.0437 - acc: 0.557 - ETA: 2s - loss: 7.2632 - acc: 0.539 - ETA: 2s - loss: 7.4202 - acc: 0.528 - ETA: 2s - loss: 7.2657 - acc: 0.541 - ETA: 2s - loss: 7.5016 - acc: 0.527 - ETA: 2s - loss: 7.4458 - acc: 0.531 - ETA: 2s - loss: 7.2670 - acc: 0.543 - ETA: 2s - loss: 7.3417 - acc: 0.539 - ETA: 2s - loss: 7.3395 - acc: 0.538 - ETA: 2s - loss: 7.4697 - acc: 0.530 - ETA: 2s - loss: 7.5526 - acc: 0.524 - ETA: 2s - loss: 7.5668 - acc: 0.522 - ETA: 2s - loss: 7.6086 - acc: 0.519 - ETA: 2s - loss: 7.6345 - acc: 0.518 - ETA: 2s - loss: 7.6963 - acc: 0.513 - ETA: 1s - loss: 7.6787 - acc: 0.515 - ETA: 1s - loss: 7.6550 - acc: 0.517 - ETA: 1s - loss: 7.6225 - acc: 0.518 - ETA: 1s - loss: 7.5954 - acc: 0.520 - ETA: 1s - loss: 7.5958 - acc: 0.519 - ETA: 1s - loss: 7.5576 - acc: 0.520 - ETA: 1s - loss: 7.5107 - acc: 0.523 - ETA: 1s - loss: 7.5667 - acc: 0.520 - ETA: 1s - loss: 7.5194 - acc: 0.522 - ETA: 1s - loss: 7.4901 - acc: 0.523 - ETA: 1s - loss: 7.4880 - acc: 0.523 - ETA: 1s - loss: 7.4660 - acc: 0.525 - ETA: 1s - loss: 7.4616 - acc: 0.525 - ETA: 1s - loss: 7.4919 - acc: 0.524 - ETA: 1s - loss: 7.5163 - acc: 0.522 - ETA: 1s - loss: 7.5277 - acc: 0.521 - ETA: 1s - loss: 7.4934 - acc: 0.523 - ETA: 1s - loss: 7.5047 - acc: 0.522 - ETA: 0s - loss: 7.5191 - acc: 0.522 - ETA: 0s - loss: 7.5120 - acc: 0.522 - ETA: 0s - loss: 7.5266 - acc: 0.522 - ETA: 0s - loss: 7.5025 - acc: 0.523 - ETA: 0s - loss: 7.4831 - acc: 0.525 - ETA: 0s - loss: 7.4718 - acc: 0.525 - ETA: 0s - loss: 7.4644 - acc: 0.526 - ETA: 0s - loss: 7.4533 - acc: 0.527 - ETA: 0s - loss: 7.4184 - acc: 0.529 - ETA: 0s - loss: 7.4049 - acc: 0.530 - ETA: 0s - loss: 7.3951 - acc: 0.531 - ETA: 0s - loss: 7.3839 - acc: 0.531 - ETA: 0s - loss: 7.3709 - acc: 0.532 - ETA: 0s - loss: 7.3612 - acc: 0.533 - ETA: 0s - loss: 7.3561 - acc: 0.533 - ETA: 0s - loss: 7.3569 - acc: 0.533 - ETA: 0s - loss: 7.3706 - acc: 0.532 - ETA: 0s - loss: 7.3612 - acc: 0.533 - ETA: 0s - loss: 7.3426 - acc: 0.534 - ETA: 0s - loss: 7.3247 - acc: 0.5352Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.3380 - acc: 0.5343 - val_loss: 8.1758 - val_acc: 0.4192\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 9.3191 - acc: 0.400 - ETA: 2s - loss: 8.0240 - acc: 0.493 - ETA: 2s - loss: 7.7074 - acc: 0.514 - ETA: 2s - loss: 7.7039 - acc: 0.514 - ETA: 2s - loss: 7.4298 - acc: 0.529 - ETA: 2s - loss: 7.3229 - acc: 0.538 - ETA: 2s - loss: 7.4449 - acc: 0.531 - ETA: 2s - loss: 7.5125 - acc: 0.525 - ETA: 2s - loss: 7.6126 - acc: 0.519 - ETA: 2s - loss: 7.5602 - acc: 0.521 - ETA: 2s - loss: 7.4480 - acc: 0.528 - ETA: 2s - loss: 7.3722 - acc: 0.534 - ETA: 2s - loss: 7.4707 - acc: 0.527 - ETA: 2s - loss: 7.4686 - acc: 0.528 - ETA: 2s - loss: 7.4395 - acc: 0.530 - ETA: 2s - loss: 7.4449 - acc: 0.530 - ETA: 2s - loss: 7.4860 - acc: 0.528 - ETA: 1s - loss: 7.4166 - acc: 0.531 - ETA: 1s - loss: 7.4238 - acc: 0.530 - ETA: 1s - loss: 7.4017 - acc: 0.532 - ETA: 1s - loss: 7.4590 - acc: 0.528 - ETA: 1s - loss: 7.4395 - acc: 0.530 - ETA: 1s - loss: 7.4507 - acc: 0.529 - ETA: 1s - loss: 7.4316 - acc: 0.530 - ETA: 1s - loss: 7.4082 - acc: 0.532 - ETA: 1s - loss: 7.3794 - acc: 0.534 - ETA: 1s - loss: 7.3800 - acc: 0.534 - ETA: 1s - loss: 7.3466 - acc: 0.536 - ETA: 1s - loss: 7.3213 - acc: 0.538 - ETA: 1s - loss: 7.2783 - acc: 0.541 - ETA: 1s - loss: 7.2437 - acc: 0.543 - ETA: 1s - loss: 7.2693 - acc: 0.541 - ETA: 1s - loss: 7.2626 - acc: 0.542 - ETA: 1s - loss: 7.2509 - acc: 0.543 - ETA: 1s - loss: 7.2562 - acc: 0.542 - ETA: 0s - loss: 7.2893 - acc: 0.541 - ETA: 0s - loss: 7.2504 - acc: 0.543 - ETA: 0s - loss: 7.2732 - acc: 0.541 - ETA: 0s - loss: 7.2640 - acc: 0.542 - ETA: 0s - loss: 7.2473 - acc: 0.543 - ETA: 0s - loss: 7.2282 - acc: 0.544 - ETA: 0s - loss: 7.2196 - acc: 0.545 - ETA: 0s - loss: 7.2238 - acc: 0.545 - ETA: 0s - loss: 7.2104 - acc: 0.545 - ETA: 0s - loss: 7.2419 - acc: 0.543 - ETA: 0s - loss: 7.2309 - acc: 0.544 - ETA: 0s - loss: 7.2259 - acc: 0.544 - ETA: 0s - loss: 7.2244 - acc: 0.544 - ETA: 0s - loss: 7.2169 - acc: 0.545 - ETA: 0s - loss: 7.2153 - acc: 0.545 - ETA: 0s - loss: 7.2176 - acc: 0.545 - ETA: 0s - loss: 7.2443 - acc: 0.543 - ETA: 0s - loss: 7.2602 - acc: 0.542 - ETA: 0s - loss: 7.2503 - acc: 0.5432Epoch 00016: val_loss improved from 8.11506 to 8.04576, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.2553 - acc: 0.5428 - val_loss: 8.0458 - val_acc: 0.4228\n",
      "Epoch 18/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 9.6709 - acc: 0.400 - ETA: 2s - loss: 6.9062 - acc: 0.557 - ETA: 2s - loss: 7.2528 - acc: 0.542 - ETA: 2s - loss: 7.4459 - acc: 0.531 - ETA: 2s - loss: 7.2970 - acc: 0.536 - ETA: 2s - loss: 6.9309 - acc: 0.558 - ETA: 2s - loss: 6.8718 - acc: 0.562 - ETA: 2s - loss: 6.9135 - acc: 0.560 - ETA: 2s - loss: 6.9145 - acc: 0.562 - ETA: 2s - loss: 6.9676 - acc: 0.558 - ETA: 2s - loss: 7.0660 - acc: 0.552 - ETA: 2s - loss: 6.9670 - acc: 0.558 - ETA: 2s - loss: 7.0094 - acc: 0.556 - ETA: 2s - loss: 7.0452 - acc: 0.553 - ETA: 2s - loss: 7.1322 - acc: 0.547 - ETA: 2s - loss: 7.1648 - acc: 0.546 - ETA: 2s - loss: 7.1757 - acc: 0.545 - ETA: 2s - loss: 7.1609 - acc: 0.546 - ETA: 1s - loss: 7.1981 - acc: 0.543 - ETA: 1s - loss: 7.1316 - acc: 0.547 - ETA: 1s - loss: 7.1835 - acc: 0.543 - ETA: 1s - loss: 7.1849 - acc: 0.541 - ETA: 1s - loss: 7.2025 - acc: 0.540 - ETA: 1s - loss: 7.2255 - acc: 0.539 - ETA: 1s - loss: 7.2272 - acc: 0.539 - ETA: 1s - loss: 7.2262 - acc: 0.540 - ETA: 1s - loss: 7.2055 - acc: 0.541 - ETA: 1s - loss: 7.1749 - acc: 0.543 - ETA: 1s - loss: 7.1311 - acc: 0.545 - ETA: 1s - loss: 7.1396 - acc: 0.545 - ETA: 1s - loss: 7.1795 - acc: 0.543 - ETA: 1s - loss: 7.1843 - acc: 0.542 - ETA: 1s - loss: 7.1796 - acc: 0.542 - ETA: 1s - loss: 7.1804 - acc: 0.542 - ETA: 1s - loss: 7.1883 - acc: 0.541 - ETA: 1s - loss: 7.2108 - acc: 0.540 - ETA: 0s - loss: 7.1993 - acc: 0.540 - ETA: 0s - loss: 7.2233 - acc: 0.539 - ETA: 0s - loss: 7.2262 - acc: 0.539 - ETA: 0s - loss: 7.2093 - acc: 0.540 - ETA: 0s - loss: 7.2013 - acc: 0.541 - ETA: 0s - loss: 7.1855 - acc: 0.542 - ETA: 0s - loss: 7.1831 - acc: 0.542 - ETA: 0s - loss: 7.1781 - acc: 0.542 - ETA: 0s - loss: 7.1907 - acc: 0.541 - ETA: 0s - loss: 7.1715 - acc: 0.542 - ETA: 0s - loss: 7.1586 - acc: 0.543 - ETA: 0s - loss: 7.1793 - acc: 0.541 - ETA: 0s - loss: 7.2117 - acc: 0.539 - ETA: 0s - loss: 7.1975 - acc: 0.540 - ETA: 0s - loss: 7.1889 - acc: 0.540 - ETA: 0s - loss: 7.1815 - acc: 0.540 - ETA: 0s - loss: 7.1739 - acc: 0.541 - ETA: 0s - loss: 7.1874 - acc: 0.540 - ETA: 0s - loss: 7.1753 - acc: 0.541 - ETA: 0s - loss: 7.1812 - acc: 0.541 - ETA: 0s - loss: 7.1730 - acc: 0.541 - ETA: 0s - loss: 7.1894 - acc: 0.541 - ETA: 0s - loss: 7.1787 - acc: 0.541 - ETA: 0s - loss: 7.1637 - acc: 0.542 - ETA: 0s - loss: 7.1839 - acc: 0.5415Epoch 00017: val_loss improved from 8.04576 to 7.94792, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.1949 - acc: 0.5409 - val_loss: 7.9479 - val_acc: 0.4287\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 2s - loss: 7.2534 - acc: 0.550 - ETA: 2s - loss: 7.4371 - acc: 0.535 - ETA: 2s - loss: 7.7583 - acc: 0.511 - ETA: 3s - loss: 7.5687 - acc: 0.522 - ETA: 3s - loss: 7.2366 - acc: 0.543 - ETA: 3s - loss: 7.1759 - acc: 0.543 - ETA: 2s - loss: 7.1296 - acc: 0.545 - ETA: 2s - loss: 7.1095 - acc: 0.547 - ETA: 2s - loss: 7.0507 - acc: 0.552 - ETA: 2s - loss: 6.9926 - acc: 0.556 - ETA: 2s - loss: 7.2221 - acc: 0.542 - ETA: 2s - loss: 7.1839 - acc: 0.542 - ETA: 2s - loss: 7.1296 - acc: 0.546 - ETA: 2s - loss: 7.2228 - acc: 0.539 - ETA: 2s - loss: 7.2793 - acc: 0.535 - ETA: 2s - loss: 7.2204 - acc: 0.539 - ETA: 2s - loss: 7.1192 - acc: 0.545 - ETA: 2s - loss: 7.0459 - acc: 0.550 - ETA: 2s - loss: 7.0578 - acc: 0.549 - ETA: 2s - loss: 7.0926 - acc: 0.547 - ETA: 2s - loss: 7.1475 - acc: 0.543 - ETA: 2s - loss: 7.1325 - acc: 0.544 - ETA: 2s - loss: 7.0795 - acc: 0.547 - ETA: 2s - loss: 7.1060 - acc: 0.545 - ETA: 2s - loss: 7.1056 - acc: 0.546 - ETA: 2s - loss: 7.1364 - acc: 0.544 - ETA: 2s - loss: 7.1187 - acc: 0.545 - ETA: 1s - loss: 7.1294 - acc: 0.545 - ETA: 1s - loss: 7.0934 - acc: 0.547 - ETA: 1s - loss: 7.0758 - acc: 0.548 - ETA: 1s - loss: 7.0810 - acc: 0.548 - ETA: 1s - loss: 7.0773 - acc: 0.548 - ETA: 1s - loss: 7.0974 - acc: 0.546 - ETA: 1s - loss: 7.0926 - acc: 0.546 - ETA: 1s - loss: 7.0968 - acc: 0.546 - ETA: 1s - loss: 7.1106 - acc: 0.544 - ETA: 1s - loss: 7.1004 - acc: 0.545 - ETA: 1s - loss: 7.0981 - acc: 0.545 - ETA: 1s - loss: 7.1119 - acc: 0.544 - ETA: 1s - loss: 7.0941 - acc: 0.546 - ETA: 1s - loss: 7.1153 - acc: 0.544 - ETA: 1s - loss: 7.1322 - acc: 0.544 - ETA: 0s - loss: 7.1122 - acc: 0.544 - ETA: 0s - loss: 7.1164 - acc: 0.545 - ETA: 0s - loss: 7.1268 - acc: 0.544 - ETA: 0s - loss: 7.1347 - acc: 0.544 - ETA: 0s - loss: 7.1488 - acc: 0.543 - ETA: 0s - loss: 7.1145 - acc: 0.545 - ETA: 0s - loss: 7.1031 - acc: 0.546 - ETA: 0s - loss: 7.0800 - acc: 0.547 - ETA: 0s - loss: 7.0801 - acc: 0.547 - ETA: 0s - loss: 7.0766 - acc: 0.548 - ETA: 0s - loss: 7.0566 - acc: 0.549 - ETA: 0s - loss: 7.0765 - acc: 0.548 - ETA: 0s - loss: 7.0890 - acc: 0.547 - ETA: 0s - loss: 7.0741 - acc: 0.547 - ETA: 0s - loss: 7.0830 - acc: 0.547 - ETA: 0s - loss: 7.0781 - acc: 0.5477Epoch 00018: val_loss improved from 7.94792 to 7.82328, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.0818 - acc: 0.5472 - val_loss: 7.8233 - val_acc: 0.4371\n",
      "Epoch 20/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 10.4778 - acc: 0.35 - ETA: 2s - loss: 8.1606 - acc: 0.4938 - ETA: 2s - loss: 7.9146 - acc: 0.507 - ETA: 2s - loss: 7.4870 - acc: 0.530 - ETA: 2s - loss: 7.2208 - acc: 0.546 - ETA: 2s - loss: 7.2275 - acc: 0.546 - ETA: 2s - loss: 7.1493 - acc: 0.550 - ETA: 2s - loss: 7.2036 - acc: 0.546 - ETA: 2s - loss: 7.2281 - acc: 0.544 - ETA: 2s - loss: 7.2024 - acc: 0.547 - ETA: 2s - loss: 7.1530 - acc: 0.550 - ETA: 2s - loss: 7.0873 - acc: 0.553 - ETA: 2s - loss: 7.0051 - acc: 0.558 - ETA: 2s - loss: 6.9611 - acc: 0.561 - ETA: 2s - loss: 6.9445 - acc: 0.562 - ETA: 2s - loss: 6.8829 - acc: 0.566 - ETA: 2s - loss: 6.8843 - acc: 0.566 - ETA: 1s - loss: 6.8590 - acc: 0.567 - ETA: 1s - loss: 6.8967 - acc: 0.564 - ETA: 1s - loss: 6.9111 - acc: 0.564 - ETA: 1s - loss: 6.8579 - acc: 0.567 - ETA: 1s - loss: 6.8686 - acc: 0.566 - ETA: 1s - loss: 6.8461 - acc: 0.568 - ETA: 1s - loss: 6.8649 - acc: 0.567 - ETA: 1s - loss: 6.9122 - acc: 0.564 - ETA: 1s - loss: 6.9100 - acc: 0.564 - ETA: 1s - loss: 6.9018 - acc: 0.564 - ETA: 1s - loss: 6.8933 - acc: 0.564 - ETA: 1s - loss: 6.8672 - acc: 0.566 - ETA: 1s - loss: 6.8754 - acc: 0.565 - ETA: 1s - loss: 6.8933 - acc: 0.564 - ETA: 1s - loss: 6.8767 - acc: 0.565 - ETA: 1s - loss: 6.8797 - acc: 0.564 - ETA: 0s - loss: 6.9152 - acc: 0.561 - ETA: 0s - loss: 6.9187 - acc: 0.561 - ETA: 0s - loss: 6.9480 - acc: 0.560 - ETA: 0s - loss: 6.9704 - acc: 0.559 - ETA: 0s - loss: 6.9626 - acc: 0.559 - ETA: 0s - loss: 6.9478 - acc: 0.560 - ETA: 0s - loss: 6.9753 - acc: 0.557 - ETA: 0s - loss: 6.9849 - acc: 0.557 - ETA: 0s - loss: 6.9781 - acc: 0.557 - ETA: 0s - loss: 6.9963 - acc: 0.555 - ETA: 0s - loss: 6.9933 - acc: 0.555 - ETA: 0s - loss: 6.9878 - acc: 0.555 - ETA: 0s - loss: 6.9665 - acc: 0.557 - ETA: 0s - loss: 6.9940 - acc: 0.555 - ETA: 0s - loss: 7.0030 - acc: 0.554 - ETA: 0s - loss: 6.9823 - acc: 0.556 - ETA: 0s - loss: 6.9743 - acc: 0.5564Epoch 00019: val_loss improved from 7.82328 to 7.79131, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 6.9667 - acc: 0.5564 - val_loss: 7.7913 - val_acc: 0.4228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e624438c88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 44.0191%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('C:\\\\Users\\\\MLUSER\\\\Anaconda3\\\\Lib\\\\site-packages\\\\opencv\\\\build\\\\etc\\\\DogResnet50Data.npz')\n",
    "train_DogResnet50 = bottleneck_features['train']\n",
    "valid_DogResnet50 = bottleneck_features['valid']\n",
    "test_DogResnet50 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_DogResnet50.shape[1:]))\n",
    "Resnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 164s - loss: 5.1886 - acc: 0.0000e+0 - ETA: 61s - loss: 6.2334 - acc: 0.0000e+0 - ETA: 50s - loss: 6.0875 - acc: 0.0000e+ - ETA: 46s - loss: 5.9001 - acc: 0.0100   - ETA: 42s - loss: 5.8620 - acc: 0.00 - ETA: 35s - loss: 5.8017 - acc: 0.01 - ETA: 33s - loss: 5.7410 - acc: 0.01 - ETA: 33s - loss: 5.5971 - acc: 0.02 - ETA: 29s - loss: 5.4126 - acc: 0.05 - ETA: 24s - loss: 5.2028 - acc: 0.07 - ETA: 19s - loss: 5.1336 - acc: 0.07 - ETA: 17s - loss: 5.0034 - acc: 0.09 - ETA: 15s - loss: 4.8780 - acc: 0.11 - ETA: 14s - loss: 4.7257 - acc: 0.12 - ETA: 13s - loss: 4.5810 - acc: 0.14 - ETA: 11s - loss: 4.4077 - acc: 0.16 - ETA: 10s - loss: 4.2849 - acc: 0.17 - ETA: 10s - loss: 4.1822 - acc: 0.19 - ETA: 9s - loss: 4.0487 - acc: 0.2143 - ETA: 9s - loss: 3.9811 - acc: 0.219 - ETA: 8s - loss: 3.8520 - acc: 0.233 - ETA: 8s - loss: 3.7760 - acc: 0.243 - ETA: 7s - loss: 3.6800 - acc: 0.256 - ETA: 8s - loss: 3.6302 - acc: 0.259 - ETA: 8s - loss: 3.6059 - acc: 0.262 - ETA: 8s - loss: 3.5848 - acc: 0.266 - ETA: 8s - loss: 3.5143 - acc: 0.276 - ETA: 8s - loss: 3.4559 - acc: 0.287 - ETA: 8s - loss: 3.4017 - acc: 0.296 - ETA: 7s - loss: 3.3433 - acc: 0.304 - ETA: 7s - loss: 3.2691 - acc: 0.316 - ETA: 7s - loss: 3.1897 - acc: 0.327 - ETA: 6s - loss: 3.1237 - acc: 0.337 - ETA: 6s - loss: 3.0883 - acc: 0.339 - ETA: 6s - loss: 3.0432 - acc: 0.347 - ETA: 6s - loss: 2.9822 - acc: 0.355 - ETA: 5s - loss: 2.9180 - acc: 0.366 - ETA: 5s - loss: 2.8652 - acc: 0.376 - ETA: 5s - loss: 2.8225 - acc: 0.385 - ETA: 5s - loss: 2.7675 - acc: 0.397 - ETA: 5s - loss: 2.7183 - acc: 0.404 - ETA: 5s - loss: 2.7059 - acc: 0.408 - ETA: 5s - loss: 2.6638 - acc: 0.416 - ETA: 5s - loss: 2.6284 - acc: 0.420 - ETA: 4s - loss: 2.5987 - acc: 0.424 - ETA: 4s - loss: 2.5632 - acc: 0.428 - ETA: 4s - loss: 2.5220 - acc: 0.436 - ETA: 4s - loss: 2.4792 - acc: 0.443 - ETA: 4s - loss: 2.4428 - acc: 0.449 - ETA: 4s - loss: 2.4200 - acc: 0.451 - ETA: 4s - loss: 2.3918 - acc: 0.456 - ETA: 3s - loss: 2.3620 - acc: 0.462 - ETA: 3s - loss: 2.3277 - acc: 0.468 - ETA: 3s - loss: 2.3151 - acc: 0.469 - ETA: 3s - loss: 2.2987 - acc: 0.473 - ETA: 3s - loss: 2.2680 - acc: 0.480 - ETA: 3s - loss: 2.2379 - acc: 0.486 - ETA: 3s - loss: 2.2074 - acc: 0.492 - ETA: 3s - loss: 2.1844 - acc: 0.496 - ETA: 3s - loss: 2.1607 - acc: 0.500 - ETA: 2s - loss: 2.1362 - acc: 0.504 - ETA: 2s - loss: 2.1244 - acc: 0.507 - ETA: 2s - loss: 2.1179 - acc: 0.508 - ETA: 2s - loss: 2.1048 - acc: 0.510 - ETA: 2s - loss: 2.0964 - acc: 0.511 - ETA: 2s - loss: 2.0858 - acc: 0.514 - ETA: 2s - loss: 2.0645 - acc: 0.518 - ETA: 2s - loss: 2.0506 - acc: 0.520 - ETA: 2s - loss: 2.0432 - acc: 0.521 - ETA: 2s - loss: 2.0328 - acc: 0.523 - ETA: 2s - loss: 2.0221 - acc: 0.526 - ETA: 2s - loss: 2.0165 - acc: 0.527 - ETA: 2s - loss: 2.0114 - acc: 0.528 - ETA: 2s - loss: 2.0042 - acc: 0.529 - ETA: 2s - loss: 1.9868 - acc: 0.532 - ETA: 2s - loss: 1.9621 - acc: 0.537 - ETA: 2s - loss: 1.9450 - acc: 0.540 - ETA: 1s - loss: 1.9322 - acc: 0.542 - ETA: 1s - loss: 1.9144 - acc: 0.546 - ETA: 1s - loss: 1.9006 - acc: 0.548 - ETA: 1s - loss: 1.8964 - acc: 0.549 - ETA: 1s - loss: 1.8942 - acc: 0.549 - ETA: 1s - loss: 1.8821 - acc: 0.551 - ETA: 1s - loss: 1.8654 - acc: 0.553 - ETA: 1s - loss: 1.8494 - acc: 0.556 - ETA: 1s - loss: 1.8356 - acc: 0.558 - ETA: 1s - loss: 1.8175 - acc: 0.563 - ETA: 1s - loss: 1.8093 - acc: 0.564 - ETA: 1s - loss: 1.7947 - acc: 0.567 - ETA: 1s - loss: 1.7877 - acc: 0.568 - ETA: 1s - loss: 1.7745 - acc: 0.571 - ETA: 0s - loss: 1.7638 - acc: 0.573 - ETA: 0s - loss: 1.7622 - acc: 0.573 - ETA: 0s - loss: 1.7583 - acc: 0.574 - ETA: 0s - loss: 1.7525 - acc: 0.575 - ETA: 0s - loss: 1.7489 - acc: 0.576 - ETA: 0s - loss: 1.7412 - acc: 0.577 - ETA: 0s - loss: 1.7331 - acc: 0.578 - ETA: 0s - loss: 1.7265 - acc: 0.580 - ETA: 0s - loss: 1.7221 - acc: 0.581 - ETA: 0s - loss: 1.7174 - acc: 0.582 - ETA: 0s - loss: 1.7116 - acc: 0.583 - ETA: 0s - loss: 1.7008 - acc: 0.585 - ETA: 0s - loss: 1.6903 - acc: 0.587 - ETA: 0s - loss: 1.6849 - acc: 0.588 - ETA: 0s - loss: 1.6799 - acc: 0.589 - ETA: 0s - loss: 1.6766 - acc: 0.590 - ETA: 0s - loss: 1.6719 - acc: 0.590 - ETA: 0s - loss: 1.6628 - acc: 0.592 - ETA: 0s - loss: 1.6522 - acc: 0.595 - ETA: 0s - loss: 1.6430 - acc: 0.597 - ETA: 0s - loss: 1.6379 - acc: 0.5974Epoch 00000: val_loss improved from inf to 0.82778, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.6311 - acc: 0.5990 - val_loss: 0.8278 - val_acc: 0.7521\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.3034 - acc: 0.900 - ETA: 5s - loss: 0.5228 - acc: 0.810 - ETA: 8s - loss: 0.5018 - acc: 0.825 - ETA: 8s - loss: 0.4215 - acc: 0.862 - ETA: 7s - loss: 0.4225 - acc: 0.866 - ETA: 10s - loss: 0.4271 - acc: 0.86 - ETA: 10s - loss: 0.4565 - acc: 0.84 - ETA: 10s - loss: 0.4537 - acc: 0.84 - ETA: 10s - loss: 0.4528 - acc: 0.84 - ETA: 9s - loss: 0.4452 - acc: 0.8545 - ETA: 8s - loss: 0.4180 - acc: 0.864 - ETA: 8s - loss: 0.4074 - acc: 0.867 - ETA: 8s - loss: 0.4042 - acc: 0.868 - ETA: 7s - loss: 0.4158 - acc: 0.866 - ETA: 7s - loss: 0.4315 - acc: 0.864 - ETA: 6s - loss: 0.4190 - acc: 0.870 - ETA: 6s - loss: 0.4111 - acc: 0.872 - ETA: 5s - loss: 0.4053 - acc: 0.876 - ETA: 6s - loss: 0.4055 - acc: 0.876 - ETA: 6s - loss: 0.4061 - acc: 0.875 - ETA: 6s - loss: 0.4063 - acc: 0.874 - ETA: 6s - loss: 0.4033 - acc: 0.875 - ETA: 6s - loss: 0.4062 - acc: 0.874 - ETA: 5s - loss: 0.4108 - acc: 0.873 - ETA: 5s - loss: 0.4127 - acc: 0.873 - ETA: 5s - loss: 0.4160 - acc: 0.873 - ETA: 5s - loss: 0.4122 - acc: 0.873 - ETA: 5s - loss: 0.4144 - acc: 0.873 - ETA: 5s - loss: 0.4068 - acc: 0.876 - ETA: 5s - loss: 0.4068 - acc: 0.876 - ETA: 5s - loss: 0.4084 - acc: 0.876 - ETA: 5s - loss: 0.4061 - acc: 0.877 - ETA: 5s - loss: 0.4052 - acc: 0.877 - ETA: 5s - loss: 0.4040 - acc: 0.879 - ETA: 5s - loss: 0.4064 - acc: 0.877 - ETA: 5s - loss: 0.4047 - acc: 0.879 - ETA: 5s - loss: 0.4028 - acc: 0.878 - ETA: 5s - loss: 0.4153 - acc: 0.876 - ETA: 5s - loss: 0.4257 - acc: 0.871 - ETA: 4s - loss: 0.4209 - acc: 0.872 - ETA: 4s - loss: 0.4198 - acc: 0.873 - ETA: 4s - loss: 0.4179 - acc: 0.874 - ETA: 4s - loss: 0.4125 - acc: 0.876 - ETA: 4s - loss: 0.4143 - acc: 0.874 - ETA: 4s - loss: 0.4140 - acc: 0.874 - ETA: 4s - loss: 0.4141 - acc: 0.874 - ETA: 4s - loss: 0.4127 - acc: 0.874 - ETA: 4s - loss: 0.4130 - acc: 0.875 - ETA: 3s - loss: 0.4088 - acc: 0.876 - ETA: 3s - loss: 0.4083 - acc: 0.877 - ETA: 3s - loss: 0.4108 - acc: 0.876 - ETA: 3s - loss: 0.4150 - acc: 0.876 - ETA: 3s - loss: 0.4148 - acc: 0.875 - ETA: 3s - loss: 0.4140 - acc: 0.875 - ETA: 3s - loss: 0.4183 - acc: 0.872 - ETA: 3s - loss: 0.4186 - acc: 0.872 - ETA: 3s - loss: 0.4246 - acc: 0.871 - ETA: 3s - loss: 0.4294 - acc: 0.869 - ETA: 3s - loss: 0.4362 - acc: 0.867 - ETA: 3s - loss: 0.4381 - acc: 0.866 - ETA: 2s - loss: 0.4354 - acc: 0.867 - ETA: 2s - loss: 0.4333 - acc: 0.868 - ETA: 2s - loss: 0.4328 - acc: 0.868 - ETA: 2s - loss: 0.4381 - acc: 0.867 - ETA: 2s - loss: 0.4350 - acc: 0.868 - ETA: 2s - loss: 0.4370 - acc: 0.868 - ETA: 2s - loss: 0.4369 - acc: 0.867 - ETA: 2s - loss: 0.4357 - acc: 0.868 - ETA: 2s - loss: 0.4347 - acc: 0.868 - ETA: 2s - loss: 0.4342 - acc: 0.868 - ETA: 1s - loss: 0.4367 - acc: 0.867 - ETA: 1s - loss: 0.4365 - acc: 0.866 - ETA: 1s - loss: 0.4360 - acc: 0.866 - ETA: 1s - loss: 0.4355 - acc: 0.866 - ETA: 1s - loss: 0.4369 - acc: 0.865 - ETA: 1s - loss: 0.4356 - acc: 0.865 - ETA: 1s - loss: 0.4375 - acc: 0.864 - ETA: 1s - loss: 0.4379 - acc: 0.864 - ETA: 1s - loss: 0.4379 - acc: 0.864 - ETA: 1s - loss: 0.4365 - acc: 0.864 - ETA: 1s - loss: 0.4353 - acc: 0.864 - ETA: 1s - loss: 0.4352 - acc: 0.864 - ETA: 1s - loss: 0.4353 - acc: 0.865 - ETA: 1s - loss: 0.4365 - acc: 0.865 - ETA: 1s - loss: 0.4355 - acc: 0.864 - ETA: 1s - loss: 0.4351 - acc: 0.864 - ETA: 1s - loss: 0.4339 - acc: 0.865 - ETA: 1s - loss: 0.4350 - acc: 0.864 - ETA: 1s - loss: 0.4344 - acc: 0.864 - ETA: 0s - loss: 0.4321 - acc: 0.865 - ETA: 0s - loss: 0.4333 - acc: 0.865 - ETA: 0s - loss: 0.4323 - acc: 0.865 - ETA: 0s - loss: 0.4334 - acc: 0.864 - ETA: 0s - loss: 0.4339 - acc: 0.864 - ETA: 0s - loss: 0.4336 - acc: 0.864 - ETA: 0s - loss: 0.4377 - acc: 0.864 - ETA: 0s - loss: 0.4372 - acc: 0.864 - ETA: 0s - loss: 0.4378 - acc: 0.863 - ETA: 0s - loss: 0.4357 - acc: 0.863 - ETA: 0s - loss: 0.4378 - acc: 0.863 - ETA: 0s - loss: 0.4373 - acc: 0.863 - ETA: 0s - loss: 0.4374 - acc: 0.863 - ETA: 0s - loss: 0.4374 - acc: 0.863 - ETA: 0s - loss: 0.4383 - acc: 0.863 - ETA: 0s - loss: 0.4377 - acc: 0.863 - ETA: 0s - loss: 0.4369 - acc: 0.863 - ETA: 0s - loss: 0.4374 - acc: 0.8631Epoch 00001: val_loss improved from 0.82778 to 0.70650, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.4377 - acc: 0.8629 - val_loss: 0.7065 - val_acc: 0.7749\n",
      "Epoch 3/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.1200 - acc: 0.950 - ETA: 8s - loss: 0.2253 - acc: 0.916 - ETA: 8s - loss: 0.2685 - acc: 0.910 - ETA: 7s - loss: 0.2855 - acc: 0.912 - ETA: 7s - loss: 0.3106 - acc: 0.909 - ETA: 6s - loss: 0.2964 - acc: 0.913 - ETA: 5s - loss: 0.2948 - acc: 0.910 - ETA: 5s - loss: 0.2652 - acc: 0.921 - ETA: 5s - loss: 0.2510 - acc: 0.926 - ETA: 5s - loss: 0.2607 - acc: 0.928 - ETA: 5s - loss: 0.2610 - acc: 0.926 - ETA: 4s - loss: 0.2469 - acc: 0.930 - ETA: 4s - loss: 0.2378 - acc: 0.932 - ETA: 4s - loss: 0.2379 - acc: 0.931 - ETA: 4s - loss: 0.2333 - acc: 0.933 - ETA: 4s - loss: 0.2388 - acc: 0.931 - ETA: 4s - loss: 0.2386 - acc: 0.928 - ETA: 4s - loss: 0.2333 - acc: 0.930 - ETA: 4s - loss: 0.2298 - acc: 0.932 - ETA: 3s - loss: 0.2352 - acc: 0.929 - ETA: 3s - loss: 0.2284 - acc: 0.932 - ETA: 3s - loss: 0.2260 - acc: 0.932 - ETA: 3s - loss: 0.2292 - acc: 0.931 - ETA: 3s - loss: 0.2355 - acc: 0.930 - ETA: 3s - loss: 0.2355 - acc: 0.929 - ETA: 3s - loss: 0.2370 - acc: 0.927 - ETA: 3s - loss: 0.2383 - acc: 0.927 - ETA: 3s - loss: 0.2416 - acc: 0.926 - ETA: 3s - loss: 0.2378 - acc: 0.928 - ETA: 3s - loss: 0.2409 - acc: 0.927 - ETA: 3s - loss: 0.2388 - acc: 0.928 - ETA: 3s - loss: 0.2373 - acc: 0.929 - ETA: 3s - loss: 0.2348 - acc: 0.929 - ETA: 2s - loss: 0.2336 - acc: 0.928 - ETA: 2s - loss: 0.2351 - acc: 0.928 - ETA: 2s - loss: 0.2391 - acc: 0.927 - ETA: 2s - loss: 0.2407 - acc: 0.926 - ETA: 2s - loss: 0.2436 - acc: 0.925 - ETA: 2s - loss: 0.2473 - acc: 0.923 - ETA: 2s - loss: 0.2500 - acc: 0.922 - ETA: 2s - loss: 0.2517 - acc: 0.922 - ETA: 2s - loss: 0.2515 - acc: 0.921 - ETA: 2s - loss: 0.2521 - acc: 0.921 - ETA: 2s - loss: 0.2511 - acc: 0.922 - ETA: 2s - loss: 0.2512 - acc: 0.922 - ETA: 2s - loss: 0.2531 - acc: 0.922 - ETA: 2s - loss: 0.2513 - acc: 0.923 - ETA: 2s - loss: 0.2533 - acc: 0.922 - ETA: 2s - loss: 0.2534 - acc: 0.922 - ETA: 2s - loss: 0.2542 - acc: 0.922 - ETA: 2s - loss: 0.2563 - acc: 0.920 - ETA: 2s - loss: 0.2553 - acc: 0.921 - ETA: 2s - loss: 0.2545 - acc: 0.921 - ETA: 2s - loss: 0.2540 - acc: 0.921 - ETA: 2s - loss: 0.2543 - acc: 0.921 - ETA: 2s - loss: 0.2558 - acc: 0.920 - ETA: 2s - loss: 0.2548 - acc: 0.921 - ETA: 2s - loss: 0.2543 - acc: 0.920 - ETA: 2s - loss: 0.2538 - acc: 0.921 - ETA: 1s - loss: 0.2533 - acc: 0.921 - ETA: 1s - loss: 0.2544 - acc: 0.920 - ETA: 1s - loss: 0.2546 - acc: 0.920 - ETA: 1s - loss: 0.2544 - acc: 0.919 - ETA: 1s - loss: 0.2547 - acc: 0.919 - ETA: 1s - loss: 0.2537 - acc: 0.920 - ETA: 1s - loss: 0.2530 - acc: 0.920 - ETA: 1s - loss: 0.2535 - acc: 0.920 - ETA: 1s - loss: 0.2538 - acc: 0.920 - ETA: 1s - loss: 0.2538 - acc: 0.920 - ETA: 1s - loss: 0.2516 - acc: 0.920 - ETA: 1s - loss: 0.2515 - acc: 0.920 - ETA: 1s - loss: 0.2526 - acc: 0.919 - ETA: 1s - loss: 0.2523 - acc: 0.919 - ETA: 1s - loss: 0.2536 - acc: 0.918 - ETA: 1s - loss: 0.2525 - acc: 0.918 - ETA: 0s - loss: 0.2537 - acc: 0.918 - ETA: 0s - loss: 0.2545 - acc: 0.918 - ETA: 0s - loss: 0.2566 - acc: 0.917 - ETA: 0s - loss: 0.2579 - acc: 0.916 - ETA: 0s - loss: 0.2588 - acc: 0.916 - ETA: 0s - loss: 0.2603 - acc: 0.915 - ETA: 0s - loss: 0.2599 - acc: 0.915 - ETA: 0s - loss: 0.2595 - acc: 0.915 - ETA: 0s - loss: 0.2591 - acc: 0.916 - ETA: 0s - loss: 0.2598 - acc: 0.916 - ETA: 0s - loss: 0.2582 - acc: 0.916 - ETA: 0s - loss: 0.2590 - acc: 0.916 - ETA: 0s - loss: 0.2567 - acc: 0.917 - ETA: 0s - loss: 0.2554 - acc: 0.917 - ETA: 0s - loss: 0.2560 - acc: 0.917 - ETA: 0s - loss: 0.2596 - acc: 0.916 - ETA: 0s - loss: 0.2599 - acc: 0.916 - ETA: 0s - loss: 0.2607 - acc: 0.9167Epoch 00002: val_loss improved from 0.70650 to 0.68155, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.2602 - acc: 0.9168 - val_loss: 0.6815 - val_acc: 0.8048\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0622 - acc: 1.000 - ETA: 4s - loss: 0.1250 - acc: 0.960 - ETA: 5s - loss: 0.1070 - acc: 0.968 - ETA: 4s - loss: 0.1154 - acc: 0.975 - ETA: 4s - loss: 0.1277 - acc: 0.968 - ETA: 4s - loss: 0.1260 - acc: 0.965 - ETA: 4s - loss: 0.1342 - acc: 0.960 - ETA: 4s - loss: 0.1429 - acc: 0.953 - ETA: 4s - loss: 0.1380 - acc: 0.956 - ETA: 6s - loss: 0.1386 - acc: 0.956 - ETA: 6s - loss: 0.1385 - acc: 0.955 - ETA: 5s - loss: 0.1491 - acc: 0.955 - ETA: 5s - loss: 0.1519 - acc: 0.953 - ETA: 5s - loss: 0.1476 - acc: 0.956 - ETA: 5s - loss: 0.1511 - acc: 0.954 - ETA: 6s - loss: 0.1547 - acc: 0.952 - ETA: 6s - loss: 0.1625 - acc: 0.949 - ETA: 5s - loss: 0.1615 - acc: 0.950 - ETA: 5s - loss: 0.1626 - acc: 0.950 - ETA: 5s - loss: 0.1678 - acc: 0.949 - ETA: 5s - loss: 0.1672 - acc: 0.949 - ETA: 5s - loss: 0.1693 - acc: 0.948 - ETA: 5s - loss: 0.1689 - acc: 0.948 - ETA: 5s - loss: 0.1682 - acc: 0.948 - ETA: 5s - loss: 0.1654 - acc: 0.949 - ETA: 5s - loss: 0.1625 - acc: 0.950 - ETA: 5s - loss: 0.1624 - acc: 0.949 - ETA: 5s - loss: 0.1625 - acc: 0.948 - ETA: 5s - loss: 0.1639 - acc: 0.948 - ETA: 5s - loss: 0.1626 - acc: 0.948 - ETA: 5s - loss: 0.1628 - acc: 0.948 - ETA: 5s - loss: 0.1627 - acc: 0.948 - ETA: 5s - loss: 0.1660 - acc: 0.947 - ETA: 5s - loss: 0.1693 - acc: 0.945 - ETA: 5s - loss: 0.1685 - acc: 0.945 - ETA: 5s - loss: 0.1671 - acc: 0.946 - ETA: 4s - loss: 0.1664 - acc: 0.946 - ETA: 4s - loss: 0.1644 - acc: 0.947 - ETA: 4s - loss: 0.1650 - acc: 0.946 - ETA: 4s - loss: 0.1695 - acc: 0.945 - ETA: 4s - loss: 0.1734 - acc: 0.944 - ETA: 4s - loss: 0.1712 - acc: 0.944 - ETA: 4s - loss: 0.1711 - acc: 0.944 - ETA: 4s - loss: 0.1699 - acc: 0.944 - ETA: 3s - loss: 0.1723 - acc: 0.943 - ETA: 3s - loss: 0.1732 - acc: 0.943 - ETA: 3s - loss: 0.1743 - acc: 0.942 - ETA: 3s - loss: 0.1759 - acc: 0.941 - ETA: 3s - loss: 0.1740 - acc: 0.942 - ETA: 3s - loss: 0.1727 - acc: 0.942 - ETA: 3s - loss: 0.1715 - acc: 0.942 - ETA: 3s - loss: 0.1705 - acc: 0.942 - ETA: 3s - loss: 0.1711 - acc: 0.942 - ETA: 3s - loss: 0.1727 - acc: 0.942 - ETA: 3s - loss: 0.1703 - acc: 0.943 - ETA: 3s - loss: 0.1700 - acc: 0.943 - ETA: 3s - loss: 0.1703 - acc: 0.943 - ETA: 3s - loss: 0.1711 - acc: 0.943 - ETA: 3s - loss: 0.1722 - acc: 0.943 - ETA: 2s - loss: 0.1712 - acc: 0.943 - ETA: 2s - loss: 0.1706 - acc: 0.943 - ETA: 2s - loss: 0.1718 - acc: 0.943 - ETA: 2s - loss: 0.1709 - acc: 0.944 - ETA: 2s - loss: 0.1696 - acc: 0.945 - ETA: 2s - loss: 0.1694 - acc: 0.945 - ETA: 2s - loss: 0.1686 - acc: 0.945 - ETA: 2s - loss: 0.1681 - acc: 0.945 - ETA: 2s - loss: 0.1674 - acc: 0.946 - ETA: 2s - loss: 0.1662 - acc: 0.946 - ETA: 2s - loss: 0.1676 - acc: 0.945 - ETA: 2s - loss: 0.1682 - acc: 0.945 - ETA: 2s - loss: 0.1691 - acc: 0.945 - ETA: 2s - loss: 0.1703 - acc: 0.944 - ETA: 2s - loss: 0.1707 - acc: 0.944 - ETA: 2s - loss: 0.1701 - acc: 0.944 - ETA: 1s - loss: 0.1716 - acc: 0.944 - ETA: 1s - loss: 0.1719 - acc: 0.944 - ETA: 1s - loss: 0.1725 - acc: 0.943 - ETA: 1s - loss: 0.1734 - acc: 0.944 - ETA: 1s - loss: 0.1729 - acc: 0.944 - ETA: 1s - loss: 0.1730 - acc: 0.944 - ETA: 1s - loss: 0.1739 - acc: 0.944 - ETA: 1s - loss: 0.1735 - acc: 0.944 - ETA: 1s - loss: 0.1734 - acc: 0.944 - ETA: 1s - loss: 0.1735 - acc: 0.944 - ETA: 1s - loss: 0.1756 - acc: 0.943 - ETA: 1s - loss: 0.1760 - acc: 0.943 - ETA: 1s - loss: 0.1756 - acc: 0.943 - ETA: 1s - loss: 0.1760 - acc: 0.943 - ETA: 1s - loss: 0.1772 - acc: 0.942 - ETA: 1s - loss: 0.1770 - acc: 0.943 - ETA: 1s - loss: 0.1765 - acc: 0.943 - ETA: 1s - loss: 0.1770 - acc: 0.942 - ETA: 0s - loss: 0.1775 - acc: 0.942 - ETA: 0s - loss: 0.1778 - acc: 0.942 - ETA: 0s - loss: 0.1794 - acc: 0.941 - ETA: 0s - loss: 0.1802 - acc: 0.941 - ETA: 0s - loss: 0.1798 - acc: 0.941 - ETA: 0s - loss: 0.1807 - acc: 0.941 - ETA: 0s - loss: 0.1821 - acc: 0.940 - ETA: 0s - loss: 0.1819 - acc: 0.940 - ETA: 0s - loss: 0.1810 - acc: 0.940 - ETA: 0s - loss: 0.1795 - acc: 0.941 - ETA: 0s - loss: 0.1794 - acc: 0.941 - ETA: 0s - loss: 0.1793 - acc: 0.941 - ETA: 0s - loss: 0.1784 - acc: 0.941 - ETA: 0s - loss: 0.1784 - acc: 0.941 - ETA: 0s - loss: 0.1794 - acc: 0.9417Epoch 00003: val_loss improved from 0.68155 to 0.67430, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.1788 - acc: 0.9419 - val_loss: 0.6743 - val_acc: 0.8096\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 18s - loss: 0.2763 - acc: 0.95 - ETA: 28s - loss: 0.2167 - acc: 0.95 - ETA: 24s - loss: 0.1961 - acc: 0.95 - ETA: 13s - loss: 0.1990 - acc: 0.94 - ETA: 9s - loss: 0.1917 - acc: 0.9409 - ETA: 8s - loss: 0.1709 - acc: 0.950 - ETA: 6s - loss: 0.1680 - acc: 0.947 - ETA: 6s - loss: 0.1572 - acc: 0.950 - ETA: 5s - loss: 0.1493 - acc: 0.950 - ETA: 5s - loss: 0.1386 - acc: 0.954 - ETA: 4s - loss: 0.1411 - acc: 0.953 - ETA: 5s - loss: 0.1432 - acc: 0.953 - ETA: 5s - loss: 0.1402 - acc: 0.954 - ETA: 5s - loss: 0.1335 - acc: 0.957 - ETA: 5s - loss: 0.1279 - acc: 0.959 - ETA: 5s - loss: 0.1252 - acc: 0.960 - ETA: 5s - loss: 0.1241 - acc: 0.960 - ETA: 4s - loss: 0.1270 - acc: 0.960 - ETA: 4s - loss: 0.1228 - acc: 0.962 - ETA: 4s - loss: 0.1279 - acc: 0.961 - ETA: 4s - loss: 0.1331 - acc: 0.960 - ETA: 4s - loss: 0.1358 - acc: 0.958 - ETA: 4s - loss: 0.1345 - acc: 0.959 - ETA: 4s - loss: 0.1318 - acc: 0.961 - ETA: 4s - loss: 0.1318 - acc: 0.961 - ETA: 4s - loss: 0.1323 - acc: 0.961 - ETA: 4s - loss: 0.1310 - acc: 0.961 - ETA: 4s - loss: 0.1284 - acc: 0.962 - ETA: 4s - loss: 0.1251 - acc: 0.964 - ETA: 4s - loss: 0.1230 - acc: 0.965 - ETA: 4s - loss: 0.1220 - acc: 0.965 - ETA: 4s - loss: 0.1218 - acc: 0.965 - ETA: 4s - loss: 0.1221 - acc: 0.964 - ETA: 4s - loss: 0.1205 - acc: 0.965 - ETA: 4s - loss: 0.1238 - acc: 0.964 - ETA: 3s - loss: 0.1274 - acc: 0.962 - ETA: 3s - loss: 0.1290 - acc: 0.961 - ETA: 3s - loss: 0.1277 - acc: 0.961 - ETA: 3s - loss: 0.1267 - acc: 0.962 - ETA: 3s - loss: 0.1248 - acc: 0.963 - ETA: 3s - loss: 0.1242 - acc: 0.963 - ETA: 3s - loss: 0.1236 - acc: 0.963 - ETA: 3s - loss: 0.1218 - acc: 0.964 - ETA: 3s - loss: 0.1205 - acc: 0.964 - ETA: 3s - loss: 0.1204 - acc: 0.964 - ETA: 3s - loss: 0.1204 - acc: 0.964 - ETA: 3s - loss: 0.1199 - acc: 0.964 - ETA: 3s - loss: 0.1206 - acc: 0.964 - ETA: 3s - loss: 0.1230 - acc: 0.964 - ETA: 2s - loss: 0.1252 - acc: 0.963 - ETA: 2s - loss: 0.1253 - acc: 0.962 - ETA: 2s - loss: 0.1268 - acc: 0.962 - ETA: 2s - loss: 0.1279 - acc: 0.961 - ETA: 2s - loss: 0.1278 - acc: 0.961 - ETA: 2s - loss: 0.1272 - acc: 0.961 - ETA: 2s - loss: 0.1272 - acc: 0.961 - ETA: 2s - loss: 0.1258 - acc: 0.961 - ETA: 2s - loss: 0.1246 - acc: 0.962 - ETA: 2s - loss: 0.1237 - acc: 0.962 - ETA: 2s - loss: 0.1220 - acc: 0.962 - ETA: 2s - loss: 0.1210 - acc: 0.963 - ETA: 2s - loss: 0.1241 - acc: 0.963 - ETA: 1s - loss: 0.1241 - acc: 0.963 - ETA: 1s - loss: 0.1239 - acc: 0.962 - ETA: 1s - loss: 0.1241 - acc: 0.962 - ETA: 1s - loss: 0.1237 - acc: 0.962 - ETA: 1s - loss: 0.1229 - acc: 0.963 - ETA: 1s - loss: 0.1259 - acc: 0.962 - ETA: 1s - loss: 0.1250 - acc: 0.962 - ETA: 1s - loss: 0.1257 - acc: 0.962 - ETA: 1s - loss: 0.1252 - acc: 0.962 - ETA: 1s - loss: 0.1255 - acc: 0.962 - ETA: 1s - loss: 0.1249 - acc: 0.962 - ETA: 1s - loss: 0.1246 - acc: 0.962 - ETA: 1s - loss: 0.1240 - acc: 0.962 - ETA: 1s - loss: 0.1237 - acc: 0.962 - ETA: 1s - loss: 0.1244 - acc: 0.962 - ETA: 1s - loss: 0.1249 - acc: 0.962 - ETA: 1s - loss: 0.1249 - acc: 0.962 - ETA: 1s - loss: 0.1251 - acc: 0.962 - ETA: 0s - loss: 0.1250 - acc: 0.962 - ETA: 0s - loss: 0.1247 - acc: 0.962 - ETA: 0s - loss: 0.1251 - acc: 0.962 - ETA: 0s - loss: 0.1250 - acc: 0.962 - ETA: 0s - loss: 0.1250 - acc: 0.962 - ETA: 0s - loss: 0.1257 - acc: 0.961 - ETA: 0s - loss: 0.1258 - acc: 0.961 - ETA: 0s - loss: 0.1281 - acc: 0.961 - ETA: 0s - loss: 0.1273 - acc: 0.961 - ETA: 0s - loss: 0.1271 - acc: 0.961 - ETA: 0s - loss: 0.1278 - acc: 0.961 - ETA: 0s - loss: 0.1280 - acc: 0.961 - ETA: 0s - loss: 0.1288 - acc: 0.961 - ETA: 0s - loss: 0.1282 - acc: 0.9613Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.1281 - acc: 0.9612 - val_loss: 0.6878 - val_acc: 0.8108\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0543 - acc: 1.000 - ETA: 4s - loss: 0.1047 - acc: 0.980 - ETA: 4s - loss: 0.0742 - acc: 0.987 - ETA: 8s - loss: 0.0884 - acc: 0.975 - ETA: 7s - loss: 0.0755 - acc: 0.978 - ETA: 6s - loss: 0.0744 - acc: 0.977 - ETA: 5s - loss: 0.0724 - acc: 0.974 - ETA: 5s - loss: 0.0853 - acc: 0.970 - ETA: 6s - loss: 0.0908 - acc: 0.969 - ETA: 6s - loss: 0.0911 - acc: 0.968 - ETA: 7s - loss: 0.0914 - acc: 0.969 - ETA: 7s - loss: 0.0920 - acc: 0.969 - ETA: 6s - loss: 0.0848 - acc: 0.972 - ETA: 6s - loss: 0.0846 - acc: 0.971 - ETA: 6s - loss: 0.0826 - acc: 0.972 - ETA: 6s - loss: 0.0813 - acc: 0.972 - ETA: 6s - loss: 0.0789 - acc: 0.973 - ETA: 5s - loss: 0.0775 - acc: 0.973 - ETA: 5s - loss: 0.0773 - acc: 0.974 - ETA: 5s - loss: 0.0809 - acc: 0.974 - ETA: 5s - loss: 0.0797 - acc: 0.974 - ETA: 5s - loss: 0.0798 - acc: 0.974 - ETA: 5s - loss: 0.0797 - acc: 0.973 - ETA: 5s - loss: 0.0793 - acc: 0.974 - ETA: 5s - loss: 0.0777 - acc: 0.974 - ETA: 5s - loss: 0.0801 - acc: 0.975 - ETA: 4s - loss: 0.0792 - acc: 0.975 - ETA: 4s - loss: 0.0811 - acc: 0.974 - ETA: 4s - loss: 0.0799 - acc: 0.974 - ETA: 4s - loss: 0.0788 - acc: 0.975 - ETA: 4s - loss: 0.0808 - acc: 0.973 - ETA: 4s - loss: 0.0810 - acc: 0.973 - ETA: 4s - loss: 0.0801 - acc: 0.974 - ETA: 4s - loss: 0.0804 - acc: 0.973 - ETA: 4s - loss: 0.0823 - acc: 0.973 - ETA: 3s - loss: 0.0817 - acc: 0.973 - ETA: 3s - loss: 0.0802 - acc: 0.974 - ETA: 3s - loss: 0.0810 - acc: 0.973 - ETA: 3s - loss: 0.0796 - acc: 0.974 - ETA: 3s - loss: 0.0803 - acc: 0.974 - ETA: 3s - loss: 0.0808 - acc: 0.974 - ETA: 3s - loss: 0.0816 - acc: 0.974 - ETA: 3s - loss: 0.0805 - acc: 0.974 - ETA: 3s - loss: 0.0814 - acc: 0.974 - ETA: 3s - loss: 0.0811 - acc: 0.974 - ETA: 3s - loss: 0.0800 - acc: 0.974 - ETA: 3s - loss: 0.0803 - acc: 0.974 - ETA: 3s - loss: 0.0800 - acc: 0.974 - ETA: 2s - loss: 0.0793 - acc: 0.974 - ETA: 2s - loss: 0.0824 - acc: 0.973 - ETA: 2s - loss: 0.0823 - acc: 0.974 - ETA: 2s - loss: 0.0823 - acc: 0.973 - ETA: 2s - loss: 0.0823 - acc: 0.973 - ETA: 2s - loss: 0.0829 - acc: 0.973 - ETA: 2s - loss: 0.0847 - acc: 0.973 - ETA: 2s - loss: 0.0868 - acc: 0.972 - ETA: 2s - loss: 0.0867 - acc: 0.972 - ETA: 2s - loss: 0.0864 - acc: 0.973 - ETA: 2s - loss: 0.0877 - acc: 0.972 - ETA: 2s - loss: 0.0868 - acc: 0.973 - ETA: 2s - loss: 0.0860 - acc: 0.973 - ETA: 2s - loss: 0.0862 - acc: 0.973 - ETA: 1s - loss: 0.0858 - acc: 0.973 - ETA: 1s - loss: 0.0860 - acc: 0.973 - ETA: 1s - loss: 0.0861 - acc: 0.972 - ETA: 1s - loss: 0.0858 - acc: 0.973 - ETA: 2s - loss: 0.0859 - acc: 0.972 - ETA: 2s - loss: 0.0859 - acc: 0.972 - ETA: 2s - loss: 0.0861 - acc: 0.972 - ETA: 2s - loss: 0.0858 - acc: 0.972 - ETA: 2s - loss: 0.0859 - acc: 0.972 - ETA: 2s - loss: 0.0858 - acc: 0.972 - ETA: 1s - loss: 0.0862 - acc: 0.972 - ETA: 1s - loss: 0.0862 - acc: 0.972 - ETA: 2s - loss: 0.0869 - acc: 0.972 - ETA: 2s - loss: 0.0872 - acc: 0.972 - ETA: 1s - loss: 0.0871 - acc: 0.972 - ETA: 1s - loss: 0.0879 - acc: 0.971 - ETA: 1s - loss: 0.0875 - acc: 0.972 - ETA: 1s - loss: 0.0877 - acc: 0.972 - ETA: 1s - loss: 0.0875 - acc: 0.972 - ETA: 1s - loss: 0.0870 - acc: 0.972 - ETA: 1s - loss: 0.0886 - acc: 0.972 - ETA: 1s - loss: 0.0881 - acc: 0.972 - ETA: 1s - loss: 0.0875 - acc: 0.972 - ETA: 1s - loss: 0.0872 - acc: 0.972 - ETA: 1s - loss: 0.0875 - acc: 0.972 - ETA: 1s - loss: 0.0871 - acc: 0.972 - ETA: 1s - loss: 0.0871 - acc: 0.972 - ETA: 1s - loss: 0.0870 - acc: 0.972 - ETA: 1s - loss: 0.0867 - acc: 0.972 - ETA: 1s - loss: 0.0861 - acc: 0.972 - ETA: 1s - loss: 0.0861 - acc: 0.972 - ETA: 0s - loss: 0.0859 - acc: 0.973 - ETA: 0s - loss: 0.0867 - acc: 0.972 - ETA: 0s - loss: 0.0870 - acc: 0.972 - ETA: 0s - loss: 0.0873 - acc: 0.972 - ETA: 0s - loss: 0.0867 - acc: 0.972 - ETA: 0s - loss: 0.0867 - acc: 0.973 - ETA: 0s - loss: 0.0862 - acc: 0.973 - ETA: 0s - loss: 0.0856 - acc: 0.973 - ETA: 0s - loss: 0.0855 - acc: 0.973 - ETA: 0s - loss: 0.0863 - acc: 0.973 - ETA: 0s - loss: 0.0861 - acc: 0.973 - ETA: 0s - loss: 0.0875 - acc: 0.973 - ETA: 0s - loss: 0.0882 - acc: 0.9730Epoch 00005: val_loss improved from 0.67430 to 0.64357, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.0879 - acc: 0.9731 - val_loss: 0.6436 - val_acc: 0.8216\n",
      "Epoch 7/20\n",
      "  20/6680 [..............................] - ETA: 15s - loss: 0.0269 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MLUSER\\AppData\\Local\\conda\\conda\\envs\\python_tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.118999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 31s - loss: 0.1360 - acc: 0.97 - ETA: 16s - loss: 0.0724 - acc: 0.98 - ETA: 11s - loss: 0.0941 - acc: 0.96 - ETA: 12s - loss: 0.0864 - acc: 0.96 - ETA: 17s - loss: 0.0791 - acc: 0.96 - ETA: 18s - loss: 0.0781 - acc: 0.96 - ETA: 16s - loss: 0.0763 - acc: 0.97 - ETA: 14s - loss: 0.0762 - acc: 0.96 - ETA: 12s - loss: 0.0665 - acc: 0.97 - ETA: 11s - loss: 0.0663 - acc: 0.97 - ETA: 10s - loss: 0.0619 - acc: 0.97 - ETA: 9s - loss: 0.0604 - acc: 0.9781 - ETA: 9s - loss: 0.0584 - acc: 0.977 - ETA: 8s - loss: 0.0569 - acc: 0.978 - ETA: 8s - loss: 0.0618 - acc: 0.977 - ETA: 7s - loss: 0.0657 - acc: 0.976 - ETA: 7s - loss: 0.0632 - acc: 0.977 - ETA: 7s - loss: 0.0669 - acc: 0.976 - ETA: 7s - loss: 0.0640 - acc: 0.977 - ETA: 7s - loss: 0.0626 - acc: 0.978 - ETA: 7s - loss: 0.0634 - acc: 0.978 - ETA: 6s - loss: 0.0624 - acc: 0.978 - ETA: 6s - loss: 0.0628 - acc: 0.978 - ETA: 6s - loss: 0.0617 - acc: 0.979 - ETA: 6s - loss: 0.0602 - acc: 0.979 - ETA: 6s - loss: 0.0597 - acc: 0.979 - ETA: 6s - loss: 0.0642 - acc: 0.979 - ETA: 5s - loss: 0.0642 - acc: 0.978 - ETA: 5s - loss: 0.0642 - acc: 0.978 - ETA: 5s - loss: 0.0633 - acc: 0.978 - ETA: 5s - loss: 0.0629 - acc: 0.978 - ETA: 5s - loss: 0.0625 - acc: 0.979 - ETA: 5s - loss: 0.0649 - acc: 0.977 - ETA: 5s - loss: 0.0656 - acc: 0.977 - ETA: 4s - loss: 0.0650 - acc: 0.978 - ETA: 4s - loss: 0.0654 - acc: 0.977 - ETA: 4s - loss: 0.0658 - acc: 0.978 - ETA: 4s - loss: 0.0648 - acc: 0.978 - ETA: 4s - loss: 0.0648 - acc: 0.978 - ETA: 4s - loss: 0.0656 - acc: 0.978 - ETA: 4s - loss: 0.0649 - acc: 0.978 - ETA: 4s - loss: 0.0642 - acc: 0.978 - ETA: 4s - loss: 0.0642 - acc: 0.978 - ETA: 4s - loss: 0.0641 - acc: 0.978 - ETA: 4s - loss: 0.0638 - acc: 0.978 - ETA: 4s - loss: 0.0634 - acc: 0.979 - ETA: 4s - loss: 0.0631 - acc: 0.979 - ETA: 4s - loss: 0.0632 - acc: 0.978 - ETA: 3s - loss: 0.0637 - acc: 0.978 - ETA: 3s - loss: 0.0629 - acc: 0.979 - ETA: 3s - loss: 0.0630 - acc: 0.979 - ETA: 3s - loss: 0.0623 - acc: 0.979 - ETA: 3s - loss: 0.0617 - acc: 0.979 - ETA: 3s - loss: 0.0630 - acc: 0.979 - ETA: 3s - loss: 0.0631 - acc: 0.979 - ETA: 3s - loss: 0.0630 - acc: 0.979 - ETA: 3s - loss: 0.0639 - acc: 0.979 - ETA: 3s - loss: 0.0637 - acc: 0.979 - ETA: 3s - loss: 0.0625 - acc: 0.979 - ETA: 3s - loss: 0.0620 - acc: 0.980 - ETA: 3s - loss: 0.0618 - acc: 0.980 - ETA: 3s - loss: 0.0626 - acc: 0.979 - ETA: 3s - loss: 0.0620 - acc: 0.979 - ETA: 2s - loss: 0.0618 - acc: 0.979 - ETA: 2s - loss: 0.0615 - acc: 0.980 - ETA: 2s - loss: 0.0625 - acc: 0.979 - ETA: 2s - loss: 0.0622 - acc: 0.980 - ETA: 2s - loss: 0.0620 - acc: 0.980 - ETA: 2s - loss: 0.0622 - acc: 0.979 - ETA: 2s - loss: 0.0623 - acc: 0.979 - ETA: 2s - loss: 0.0616 - acc: 0.980 - ETA: 2s - loss: 0.0613 - acc: 0.980 - ETA: 2s - loss: 0.0617 - acc: 0.979 - ETA: 2s - loss: 0.0616 - acc: 0.979 - ETA: 2s - loss: 0.0610 - acc: 0.980 - ETA: 1s - loss: 0.0609 - acc: 0.980 - ETA: 1s - loss: 0.0604 - acc: 0.980 - ETA: 1s - loss: 0.0600 - acc: 0.980 - ETA: 1s - loss: 0.0595 - acc: 0.981 - ETA: 1s - loss: 0.0609 - acc: 0.980 - ETA: 1s - loss: 0.0614 - acc: 0.980 - ETA: 1s - loss: 0.0618 - acc: 0.980 - ETA: 1s - loss: 0.0623 - acc: 0.980 - ETA: 1s - loss: 0.0631 - acc: 0.980 - ETA: 1s - loss: 0.0630 - acc: 0.980 - ETA: 1s - loss: 0.0628 - acc: 0.980 - ETA: 1s - loss: 0.0635 - acc: 0.980 - ETA: 0s - loss: 0.0630 - acc: 0.980 - ETA: 0s - loss: 0.0629 - acc: 0.980 - ETA: 0s - loss: 0.0627 - acc: 0.980 - ETA: 0s - loss: 0.0625 - acc: 0.980 - ETA: 0s - loss: 0.0632 - acc: 0.980 - ETA: 0s - loss: 0.0643 - acc: 0.980 - ETA: 0s - loss: 0.0637 - acc: 0.980 - ETA: 0s - loss: 0.0634 - acc: 0.980 - ETA: 0s - loss: 0.0633 - acc: 0.980 - ETA: 0s - loss: 0.0628 - acc: 0.981 - ETA: 0s - loss: 0.0642 - acc: 0.981 - ETA: 0s - loss: 0.0640 - acc: 0.981 - ETA: 0s - loss: 0.0640 - acc: 0.981 - ETA: 0s - loss: 0.0639 - acc: 0.981 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0644 - acc: 0.980 - ETA: 0s - loss: 0.0644 - acc: 0.9809Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0646 - acc: 0.9808 - val_loss: 0.7142 - val_acc: 0.8192\n",
      "Epoch 8/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.1347 - acc: 0.900 - ETA: 4s - loss: 0.1372 - acc: 0.950 - ETA: 4s - loss: 0.1016 - acc: 0.966 - ETA: 4s - loss: 0.0778 - acc: 0.976 - ETA: 4s - loss: 0.0675 - acc: 0.979 - ETA: 4s - loss: 0.0614 - acc: 0.983 - ETA: 4s - loss: 0.0532 - acc: 0.986 - ETA: 4s - loss: 0.0557 - acc: 0.982 - ETA: 4s - loss: 0.0527 - acc: 0.984 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0477 - acc: 0.986 - ETA: 4s - loss: 0.0477 - acc: 0.986 - ETA: 4s - loss: 0.0463 - acc: 0.986 - ETA: 4s - loss: 0.0489 - acc: 0.986 - ETA: 4s - loss: 0.0474 - acc: 0.986 - ETA: 4s - loss: 0.0477 - acc: 0.985 - ETA: 5s - loss: 0.0489 - acc: 0.983 - ETA: 5s - loss: 0.0490 - acc: 0.984 - ETA: 5s - loss: 0.0545 - acc: 0.984 - ETA: 5s - loss: 0.0531 - acc: 0.985 - ETA: 4s - loss: 0.0512 - acc: 0.986 - ETA: 4s - loss: 0.0532 - acc: 0.985 - ETA: 4s - loss: 0.0518 - acc: 0.986 - ETA: 4s - loss: 0.0507 - acc: 0.987 - ETA: 4s - loss: 0.0487 - acc: 0.987 - ETA: 4s - loss: 0.0476 - acc: 0.987 - ETA: 4s - loss: 0.0486 - acc: 0.986 - ETA: 4s - loss: 0.0482 - acc: 0.986 - ETA: 4s - loss: 0.0479 - acc: 0.986 - ETA: 4s - loss: 0.0473 - acc: 0.987 - ETA: 3s - loss: 0.0460 - acc: 0.987 - ETA: 3s - loss: 0.0450 - acc: 0.987 - ETA: 3s - loss: 0.0443 - acc: 0.988 - ETA: 4s - loss: 0.0441 - acc: 0.988 - ETA: 4s - loss: 0.0441 - acc: 0.988 - ETA: 4s - loss: 0.0435 - acc: 0.988 - ETA: 4s - loss: 0.0430 - acc: 0.988 - ETA: 3s - loss: 0.0441 - acc: 0.988 - ETA: 3s - loss: 0.0443 - acc: 0.988 - ETA: 3s - loss: 0.0437 - acc: 0.988 - ETA: 3s - loss: 0.0465 - acc: 0.988 - ETA: 3s - loss: 0.0475 - acc: 0.988 - ETA: 3s - loss: 0.0466 - acc: 0.988 - ETA: 3s - loss: 0.0479 - acc: 0.988 - ETA: 3s - loss: 0.0480 - acc: 0.988 - ETA: 3s - loss: 0.0470 - acc: 0.988 - ETA: 3s - loss: 0.0472 - acc: 0.988 - ETA: 2s - loss: 0.0470 - acc: 0.988 - ETA: 2s - loss: 0.0477 - acc: 0.987 - ETA: 2s - loss: 0.0481 - acc: 0.987 - ETA: 2s - loss: 0.0488 - acc: 0.987 - ETA: 2s - loss: 0.0497 - acc: 0.987 - ETA: 2s - loss: 0.0497 - acc: 0.987 - ETA: 2s - loss: 0.0500 - acc: 0.987 - ETA: 2s - loss: 0.0500 - acc: 0.987 - ETA: 2s - loss: 0.0496 - acc: 0.987 - ETA: 2s - loss: 0.0523 - acc: 0.987 - ETA: 2s - loss: 0.0524 - acc: 0.986 - ETA: 2s - loss: 0.0522 - acc: 0.987 - ETA: 2s - loss: 0.0519 - acc: 0.987 - ETA: 2s - loss: 0.0526 - acc: 0.986 - ETA: 2s - loss: 0.0519 - acc: 0.987 - ETA: 2s - loss: 0.0516 - acc: 0.987 - ETA: 1s - loss: 0.0516 - acc: 0.987 - ETA: 1s - loss: 0.0510 - acc: 0.987 - ETA: 1s - loss: 0.0508 - acc: 0.987 - ETA: 1s - loss: 0.0504 - acc: 0.988 - ETA: 1s - loss: 0.0506 - acc: 0.988 - ETA: 1s - loss: 0.0502 - acc: 0.988 - ETA: 1s - loss: 0.0498 - acc: 0.988 - ETA: 1s - loss: 0.0495 - acc: 0.988 - ETA: 1s - loss: 0.0497 - acc: 0.988 - ETA: 1s - loss: 0.0496 - acc: 0.988 - ETA: 1s - loss: 0.0499 - acc: 0.987 - ETA: 1s - loss: 0.0502 - acc: 0.987 - ETA: 1s - loss: 0.0505 - acc: 0.987 - ETA: 1s - loss: 0.0503 - acc: 0.987 - ETA: 1s - loss: 0.0503 - acc: 0.987 - ETA: 1s - loss: 0.0501 - acc: 0.987 - ETA: 1s - loss: 0.0499 - acc: 0.987 - ETA: 0s - loss: 0.0497 - acc: 0.987 - ETA: 0s - loss: 0.0507 - acc: 0.987 - ETA: 0s - loss: 0.0511 - acc: 0.986 - ETA: 0s - loss: 0.0515 - acc: 0.986 - ETA: 0s - loss: 0.0512 - acc: 0.986 - ETA: 0s - loss: 0.0509 - acc: 0.987 - ETA: 0s - loss: 0.0505 - acc: 0.987 - ETA: 0s - loss: 0.0504 - acc: 0.987 - ETA: 0s - loss: 0.0507 - acc: 0.987 - ETA: 0s - loss: 0.0505 - acc: 0.987 - ETA: 0s - loss: 0.0504 - acc: 0.987 - ETA: 0s - loss: 0.0512 - acc: 0.986 - ETA: 0s - loss: 0.0508 - acc: 0.986 - ETA: 0s - loss: 0.0504 - acc: 0.986 - ETA: 0s - loss: 0.0503 - acc: 0.986 - ETA: 0s - loss: 0.0501 - acc: 0.987 - ETA: 0s - loss: 0.0498 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0495 - acc: 0.9873 - val_loss: 0.7063 - val_acc: 0.8251\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0116 - acc: 1.000 - ETA: 4s - loss: 0.0141 - acc: 1.000 - ETA: 4s - loss: 0.0188 - acc: 1.000 - ETA: 4s - loss: 0.0189 - acc: 0.996 - ETA: 4s - loss: 0.0188 - acc: 0.994 - ETA: 4s - loss: 0.0211 - acc: 0.993 - ETA: 4s - loss: 0.0193 - acc: 0.994 - ETA: 4s - loss: 0.0277 - acc: 0.993 - ETA: 3s - loss: 0.0252 - acc: 0.994 - ETA: 3s - loss: 0.0240 - acc: 0.994 - ETA: 4s - loss: 0.0241 - acc: 0.995 - ETA: 4s - loss: 0.0253 - acc: 0.993 - ETA: 4s - loss: 0.0246 - acc: 0.994 - ETA: 4s - loss: 0.0242 - acc: 0.993 - ETA: 4s - loss: 0.0238 - acc: 0.994 - ETA: 4s - loss: 0.0275 - acc: 0.993 - ETA: 3s - loss: 0.0272 - acc: 0.993 - ETA: 3s - loss: 0.0259 - acc: 0.993 - ETA: 3s - loss: 0.0277 - acc: 0.992 - ETA: 3s - loss: 0.0274 - acc: 0.993 - ETA: 3s - loss: 0.0264 - acc: 0.993 - ETA: 3s - loss: 0.0278 - acc: 0.993 - ETA: 3s - loss: 0.0274 - acc: 0.993 - ETA: 3s - loss: 0.0266 - acc: 0.993 - ETA: 3s - loss: 0.0276 - acc: 0.993 - ETA: 3s - loss: 0.0271 - acc: 0.993 - ETA: 3s - loss: 0.0264 - acc: 0.993 - ETA: 3s - loss: 0.0277 - acc: 0.993 - ETA: 3s - loss: 0.0272 - acc: 0.993 - ETA: 3s - loss: 0.0279 - acc: 0.993 - ETA: 3s - loss: 0.0278 - acc: 0.993 - ETA: 3s - loss: 0.0273 - acc: 0.993 - ETA: 3s - loss: 0.0273 - acc: 0.993 - ETA: 3s - loss: 0.0273 - acc: 0.993 - ETA: 3s - loss: 0.0269 - acc: 0.993 - ETA: 3s - loss: 0.0280 - acc: 0.993 - ETA: 3s - loss: 0.0286 - acc: 0.993 - ETA: 3s - loss: 0.0282 - acc: 0.993 - ETA: 2s - loss: 0.0282 - acc: 0.993 - ETA: 2s - loss: 0.0310 - acc: 0.991 - ETA: 2s - loss: 0.0306 - acc: 0.991 - ETA: 2s - loss: 0.0312 - acc: 0.991 - ETA: 2s - loss: 0.0309 - acc: 0.991 - ETA: 2s - loss: 0.0308 - acc: 0.991 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 2s - loss: 0.0320 - acc: 0.990 - ETA: 2s - loss: 0.0332 - acc: 0.990 - ETA: 2s - loss: 0.0331 - acc: 0.990 - ETA: 2s - loss: 0.0330 - acc: 0.990 - ETA: 2s - loss: 0.0325 - acc: 0.990 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 1s - loss: 0.0329 - acc: 0.990 - ETA: 1s - loss: 0.0325 - acc: 0.990 - ETA: 1s - loss: 0.0339 - acc: 0.990 - ETA: 1s - loss: 0.0334 - acc: 0.990 - ETA: 1s - loss: 0.0334 - acc: 0.991 - ETA: 1s - loss: 0.0333 - acc: 0.990 - ETA: 1s - loss: 0.0331 - acc: 0.990 - ETA: 1s - loss: 0.0330 - acc: 0.991 - ETA: 1s - loss: 0.0326 - acc: 0.991 - ETA: 1s - loss: 0.0323 - acc: 0.991 - ETA: 1s - loss: 0.0323 - acc: 0.991 - ETA: 1s - loss: 0.0323 - acc: 0.991 - ETA: 1s - loss: 0.0339 - acc: 0.990 - ETA: 1s - loss: 0.0340 - acc: 0.990 - ETA: 1s - loss: 0.0336 - acc: 0.990 - ETA: 1s - loss: 0.0336 - acc: 0.990 - ETA: 0s - loss: 0.0333 - acc: 0.990 - ETA: 0s - loss: 0.0360 - acc: 0.990 - ETA: 0s - loss: 0.0357 - acc: 0.990 - ETA: 0s - loss: 0.0354 - acc: 0.990 - ETA: 0s - loss: 0.0356 - acc: 0.990 - ETA: 0s - loss: 0.0353 - acc: 0.990 - ETA: 0s - loss: 0.0361 - acc: 0.990 - ETA: 0s - loss: 0.0360 - acc: 0.990 - ETA: 0s - loss: 0.0358 - acc: 0.990 - ETA: 0s - loss: 0.0364 - acc: 0.989 - ETA: 0s - loss: 0.0360 - acc: 0.990 - ETA: 0s - loss: 0.0359 - acc: 0.990 - ETA: 0s - loss: 0.0360 - acc: 0.990 - ETA: 0s - loss: 0.0361 - acc: 0.990 - ETA: 0s - loss: 0.0361 - acc: 0.989 - ETA: 0s - loss: 0.0364 - acc: 0.9897Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0371 - acc: 0.9897 - val_loss: 0.6869 - val_acc: 0.8347\n",
      "Epoch 10/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0096 - acc: 1.000 - ETA: 3s - loss: 0.0063 - acc: 1.000 - ETA: 3s - loss: 0.0080 - acc: 1.000 - ETA: 3s - loss: 0.0100 - acc: 1.000 - ETA: 3s - loss: 0.0111 - acc: 1.000 - ETA: 3s - loss: 0.0105 - acc: 1.000 - ETA: 3s - loss: 0.0107 - acc: 1.000 - ETA: 3s - loss: 0.0111 - acc: 1.000 - ETA: 3s - loss: 0.0132 - acc: 0.997 - ETA: 3s - loss: 0.0139 - acc: 0.997 - ETA: 3s - loss: 0.0134 - acc: 0.998 - ETA: 3s - loss: 0.0167 - acc: 0.997 - ETA: 3s - loss: 0.0193 - acc: 0.995 - ETA: 3s - loss: 0.0184 - acc: 0.996 - ETA: 3s - loss: 0.0207 - acc: 0.995 - ETA: 3s - loss: 0.0210 - acc: 0.995 - ETA: 3s - loss: 0.0202 - acc: 0.995 - ETA: 2s - loss: 0.0212 - acc: 0.995 - ETA: 2s - loss: 0.0211 - acc: 0.995 - ETA: 2s - loss: 0.0213 - acc: 0.994 - ETA: 2s - loss: 0.0221 - acc: 0.994 - ETA: 2s - loss: 0.0219 - acc: 0.994 - ETA: 2s - loss: 0.0219 - acc: 0.994 - ETA: 2s - loss: 0.0222 - acc: 0.994 - ETA: 2s - loss: 0.0223 - acc: 0.994 - ETA: 2s - loss: 0.0218 - acc: 0.994 - ETA: 2s - loss: 0.0227 - acc: 0.994 - ETA: 2s - loss: 0.0222 - acc: 0.994 - ETA: 2s - loss: 0.0224 - acc: 0.994 - ETA: 2s - loss: 0.0217 - acc: 0.994 - ETA: 2s - loss: 0.0216 - acc: 0.994 - ETA: 2s - loss: 0.0214 - acc: 0.994 - ETA: 2s - loss: 0.0209 - acc: 0.995 - ETA: 2s - loss: 0.0217 - acc: 0.994 - ETA: 2s - loss: 0.0222 - acc: 0.994 - ETA: 2s - loss: 0.0217 - acc: 0.994 - ETA: 2s - loss: 0.0216 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0221 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.994 - ETA: 1s - loss: 0.0232 - acc: 0.994 - ETA: 1s - loss: 0.0231 - acc: 0.994 - ETA: 1s - loss: 0.0230 - acc: 0.994 - ETA: 1s - loss: 0.0227 - acc: 0.994 - ETA: 1s - loss: 0.0225 - acc: 0.994 - ETA: 1s - loss: 0.0224 - acc: 0.994 - ETA: 1s - loss: 0.0225 - acc: 0.994 - ETA: 1s - loss: 0.0243 - acc: 0.994 - ETA: 1s - loss: 0.0241 - acc: 0.994 - ETA: 1s - loss: 0.0243 - acc: 0.994 - ETA: 1s - loss: 0.0245 - acc: 0.994 - ETA: 1s - loss: 0.0245 - acc: 0.994 - ETA: 1s - loss: 0.0251 - acc: 0.994 - ETA: 0s - loss: 0.0268 - acc: 0.994 - ETA: 0s - loss: 0.0270 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0265 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0268 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0270 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0274 - acc: 0.993 - ETA: 0s - loss: 0.0276 - acc: 0.992 - ETA: 0s - loss: 0.0276 - acc: 0.992 - ETA: 0s - loss: 0.0274 - acc: 0.992 - ETA: 0s - loss: 0.0271 - acc: 0.992 - ETA: 0s - loss: 0.0273 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.9925Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0271 - acc: 0.9925 - val_loss: 0.7420 - val_acc: 0.8240\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.0016 - acc: 1.000 - ETA: 6s - loss: 0.0069 - acc: 1.000 - ETA: 6s - loss: 0.0095 - acc: 1.000 - ETA: 5s - loss: 0.0176 - acc: 0.995 - ETA: 5s - loss: 0.0193 - acc: 0.993 - ETA: 6s - loss: 0.0182 - acc: 0.993 - ETA: 6s - loss: 0.0183 - acc: 0.995 - ETA: 5s - loss: 0.0171 - acc: 0.995 - ETA: 5s - loss: 0.0169 - acc: 0.996 - ETA: 5s - loss: 0.0163 - acc: 0.996 - ETA: 5s - loss: 0.0237 - acc: 0.994 - ETA: 4s - loss: 0.0219 - acc: 0.995 - ETA: 4s - loss: 0.0197 - acc: 0.995 - ETA: 4s - loss: 0.0186 - acc: 0.995 - ETA: 4s - loss: 0.0177 - acc: 0.996 - ETA: 4s - loss: 0.0209 - acc: 0.995 - ETA: 4s - loss: 0.0203 - acc: 0.995 - ETA: 4s - loss: 0.0195 - acc: 0.996 - ETA: 4s - loss: 0.0189 - acc: 0.996 - ETA: 3s - loss: 0.0180 - acc: 0.996 - ETA: 3s - loss: 0.0175 - acc: 0.996 - ETA: 3s - loss: 0.0174 - acc: 0.996 - ETA: 3s - loss: 0.0169 - acc: 0.997 - ETA: 3s - loss: 0.0167 - acc: 0.997 - ETA: 3s - loss: 0.0181 - acc: 0.996 - ETA: 3s - loss: 0.0174 - acc: 0.996 - ETA: 3s - loss: 0.0170 - acc: 0.997 - ETA: 3s - loss: 0.0166 - acc: 0.997 - ETA: 3s - loss: 0.0179 - acc: 0.996 - ETA: 3s - loss: 0.0200 - acc: 0.995 - ETA: 2s - loss: 0.0198 - acc: 0.995 - ETA: 2s - loss: 0.0204 - acc: 0.995 - ETA: 2s - loss: 0.0209 - acc: 0.995 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0206 - acc: 0.995 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0213 - acc: 0.994 - ETA: 2s - loss: 0.0219 - acc: 0.994 - ETA: 2s - loss: 0.0217 - acc: 0.994 - ETA: 2s - loss: 0.0214 - acc: 0.994 - ETA: 2s - loss: 0.0213 - acc: 0.994 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0208 - acc: 0.994 - ETA: 2s - loss: 0.0205 - acc: 0.994 - ETA: 2s - loss: 0.0202 - acc: 0.994 - ETA: 2s - loss: 0.0207 - acc: 0.994 - ETA: 2s - loss: 0.0208 - acc: 0.994 - ETA: 2s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.994 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0201 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0200 - acc: 0.994 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0213 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0205 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0204 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0214 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0214 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0222 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0221 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0231 - acc: 0.994 - ETA: 0s - loss: 0.0230 - acc: 0.994 - ETA: 0s - loss: 0.0229 - acc: 0.994 - ETA: 0s - loss: 0.0229 - acc: 0.9941Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0227 - acc: 0.9942 - val_loss: 0.7658 - val_acc: 0.8096\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 16s - loss: 0.0106 - acc: 1.00 - ETA: 17s - loss: 0.0256 - acc: 0.97 - ETA: 21s - loss: 0.0228 - acc: 0.98 - ETA: 27s - loss: 0.0220 - acc: 0.98 - ETA: 21s - loss: 0.0167 - acc: 0.99 - ETA: 16s - loss: 0.0120 - acc: 0.99 - ETA: 13s - loss: 0.0149 - acc: 0.99 - ETA: 12s - loss: 0.0181 - acc: 0.99 - ETA: 11s - loss: 0.0180 - acc: 0.99 - ETA: 10s - loss: 0.0154 - acc: 0.99 - ETA: 9s - loss: 0.0135 - acc: 0.9940 - ETA: 8s - loss: 0.0120 - acc: 0.994 - ETA: 7s - loss: 0.0143 - acc: 0.993 - ETA: 7s - loss: 0.0137 - acc: 0.994 - ETA: 6s - loss: 0.0127 - acc: 0.995 - ETA: 6s - loss: 0.0126 - acc: 0.995 - ETA: 6s - loss: 0.0124 - acc: 0.995 - ETA: 7s - loss: 0.0124 - acc: 0.995 - ETA: 7s - loss: 0.0120 - acc: 0.996 - ETA: 6s - loss: 0.0117 - acc: 0.996 - ETA: 6s - loss: 0.0112 - acc: 0.996 - ETA: 6s - loss: 0.0108 - acc: 0.996 - ETA: 6s - loss: 0.0106 - acc: 0.997 - ETA: 6s - loss: 0.0102 - acc: 0.997 - ETA: 5s - loss: 0.0101 - acc: 0.997 - ETA: 5s - loss: 0.0104 - acc: 0.997 - ETA: 5s - loss: 0.0104 - acc: 0.997 - ETA: 5s - loss: 0.0121 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.997 - ETA: 4s - loss: 0.0117 - acc: 0.997 - ETA: 4s - loss: 0.0115 - acc: 0.997 - ETA: 4s - loss: 0.0111 - acc: 0.997 - ETA: 4s - loss: 0.0121 - acc: 0.996 - ETA: 4s - loss: 0.0158 - acc: 0.996 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0152 - acc: 0.996 - ETA: 3s - loss: 0.0152 - acc: 0.996 - ETA: 3s - loss: 0.0150 - acc: 0.996 - ETA: 3s - loss: 0.0151 - acc: 0.996 - ETA: 3s - loss: 0.0160 - acc: 0.996 - ETA: 2s - loss: 0.0157 - acc: 0.996 - ETA: 2s - loss: 0.0155 - acc: 0.996 - ETA: 2s - loss: 0.0152 - acc: 0.996 - ETA: 2s - loss: 0.0162 - acc: 0.996 - ETA: 2s - loss: 0.0161 - acc: 0.996 - ETA: 2s - loss: 0.0163 - acc: 0.995 - ETA: 2s - loss: 0.0201 - acc: 0.995 - ETA: 2s - loss: 0.0200 - acc: 0.995 - ETA: 2s - loss: 0.0204 - acc: 0.995 - ETA: 2s - loss: 0.0201 - acc: 0.995 - ETA: 2s - loss: 0.0198 - acc: 0.995 - ETA: 2s - loss: 0.0199 - acc: 0.995 - ETA: 2s - loss: 0.0196 - acc: 0.995 - ETA: 2s - loss: 0.0195 - acc: 0.995 - ETA: 1s - loss: 0.0200 - acc: 0.995 - ETA: 1s - loss: 0.0205 - acc: 0.994 - ETA: 1s - loss: 0.0201 - acc: 0.995 - ETA: 1s - loss: 0.0198 - acc: 0.995 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0215 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 1s - loss: 0.0211 - acc: 0.994 - ETA: 1s - loss: 0.0209 - acc: 0.994 - ETA: 1s - loss: 0.0208 - acc: 0.994 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.995 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0193 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0193 - acc: 0.995 - ETA: 0s - loss: 0.0192 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.9950Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0194 - acc: 0.9951 - val_loss: 0.7853 - val_acc: 0.8156\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0033 - acc: 1.000 - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 0.996 - ETA: 3s - loss: 0.0051 - acc: 0.997 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0044 - acc: 0.998 - ETA: 3s - loss: 0.0089 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.996 - ETA: 3s - loss: 0.0105 - acc: 0.996 - ETA: 3s - loss: 0.0097 - acc: 0.997 - ETA: 3s - loss: 0.0122 - acc: 0.996 - ETA: 3s - loss: 0.0154 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0139 - acc: 0.996 - ETA: 3s - loss: 0.0134 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0126 - acc: 0.997 - ETA: 3s - loss: 0.0139 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.995 - ETA: 2s - loss: 0.0140 - acc: 0.995 - ETA: 3s - loss: 0.0138 - acc: 0.996 - ETA: 3s - loss: 0.0136 - acc: 0.996 - ETA: 3s - loss: 0.0136 - acc: 0.996 - ETA: 3s - loss: 0.0136 - acc: 0.996 - ETA: 3s - loss: 0.0134 - acc: 0.996 - ETA: 2s - loss: 0.0130 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.996 - ETA: 2s - loss: 0.0133 - acc: 0.996 - ETA: 2s - loss: 0.0130 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0126 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0127 - acc: 0.996 - ETA: 2s - loss: 0.0126 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.996 - ETA: 2s - loss: 0.0121 - acc: 0.996 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.996 - ETA: 2s - loss: 0.0120 - acc: 0.996 - ETA: 2s - loss: 0.0130 - acc: 0.996 - ETA: 2s - loss: 0.0132 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0130 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.9964Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0160 - acc: 0.9964 - val_loss: 0.8346 - val_acc: 0.8228\n",
      "Epoch 14/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 3.7652e-04 - acc: 1.000 - ETA: 8s - loss: 0.0042 - acc: 1.0000    - ETA: 7s - loss: 0.0030 - acc: 1.000 - ETA: 7s - loss: 0.0026 - acc: 1.000 - ETA: 6s - loss: 0.0148 - acc: 0.995 - ETA: 5s - loss: 0.0118 - acc: 0.996 - ETA: 5s - loss: 0.0097 - acc: 0.997 - ETA: 5s - loss: 0.0080 - acc: 0.998 - ETA: 4s - loss: 0.0072 - acc: 0.998 - ETA: 4s - loss: 0.0068 - acc: 0.998 - ETA: 4s - loss: 0.0068 - acc: 0.998 - ETA: 4s - loss: 0.0167 - acc: 0.997 - ETA: 4s - loss: 0.0156 - acc: 0.997 - ETA: 3s - loss: 0.0144 - acc: 0.998 - ETA: 3s - loss: 0.0140 - acc: 0.998 - ETA: 3s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0119 - acc: 0.998 - ETA: 3s - loss: 0.0113 - acc: 0.998 - ETA: 3s - loss: 0.0110 - acc: 0.998 - ETA: 3s - loss: 0.0105 - acc: 0.998 - ETA: 3s - loss: 0.0101 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0113 - acc: 0.997 - ETA: 3s - loss: 0.0134 - acc: 0.997 - ETA: 3s - loss: 0.0131 - acc: 0.997 - ETA: 3s - loss: 0.0129 - acc: 0.997 - ETA: 3s - loss: 0.0126 - acc: 0.997 - ETA: 3s - loss: 0.0124 - acc: 0.997 - ETA: 3s - loss: 0.0124 - acc: 0.997 - ETA: 3s - loss: 0.0123 - acc: 0.997 - ETA: 3s - loss: 0.0122 - acc: 0.998 - ETA: 3s - loss: 0.0120 - acc: 0.998 - ETA: 3s - loss: 0.0133 - acc: 0.997 - ETA: 3s - loss: 0.0130 - acc: 0.997 - ETA: 3s - loss: 0.0128 - acc: 0.997 - ETA: 3s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0118 - acc: 0.997 - ETA: 2s - loss: 0.0117 - acc: 0.997 - ETA: 2s - loss: 0.0117 - acc: 0.997 - ETA: 2s - loss: 0.0116 - acc: 0.997 - ETA: 2s - loss: 0.0115 - acc: 0.997 - ETA: 2s - loss: 0.0113 - acc: 0.997 - ETA: 2s - loss: 0.0114 - acc: 0.997 - ETA: 2s - loss: 0.0112 - acc: 0.997 - ETA: 2s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0102 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0108 - acc: 0.997 - ETA: 0s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0108 - acc: 0.9974Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0107 - acc: 0.9975 - val_loss: 0.8179 - val_acc: 0.8228\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 4s - loss: 0.0024 - acc: 1.000 - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 5s - loss: 0.0024 - acc: 1.000 - ETA: 4s - loss: 0.0026 - acc: 1.000 - ETA: 4s - loss: 0.0034 - acc: 1.000 - ETA: 4s - loss: 0.0038 - acc: 1.000 - ETA: 4s - loss: 0.0038 - acc: 1.000 - ETA: 4s - loss: 0.0037 - acc: 1.000 - ETA: 4s - loss: 0.0035 - acc: 1.000 - ETA: 4s - loss: 0.0032 - acc: 1.000 - ETA: 4s - loss: 0.0033 - acc: 1.000 - ETA: 4s - loss: 0.0031 - acc: 1.000 - ETA: 4s - loss: 0.0030 - acc: 1.000 - ETA: 4s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0047 - acc: 0.999 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0044 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0042 - acc: 0.999 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0044 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 3s - loss: 0.0060 - acc: 0.999 - ETA: 3s - loss: 0.0059 - acc: 0.999 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0095 - acc: 0.998 - ETA: 3s - loss: 0.0096 - acc: 0.998 - ETA: 3s - loss: 0.0094 - acc: 0.998 - ETA: 3s - loss: 0.0092 - acc: 0.998 - ETA: 3s - loss: 0.0090 - acc: 0.998 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0085 - acc: 0.998 - ETA: 2s - loss: 0.0083 - acc: 0.998 - ETA: 2s - loss: 0.0082 - acc: 0.998 - ETA: 2s - loss: 0.0082 - acc: 0.998 - ETA: 2s - loss: 0.0081 - acc: 0.998 - ETA: 2s - loss: 0.0080 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0077 - acc: 0.998 - ETA: 2s - loss: 0.0080 - acc: 0.998 - ETA: 2s - loss: 0.0080 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0081 - acc: 0.998 - ETA: 2s - loss: 0.0082 - acc: 0.998 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0095 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.9980Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0097 - acc: 0.9981 - val_loss: 0.8796 - val_acc: 0.8263\n",
      "Epoch 16/20\n",
      "6660/6680 [============================>.] - ETA: 5s - loss: 0.0023 - acc: 1.000 - ETA: 5s - loss: 0.0154 - acc: 0.987 - ETA: 4s - loss: 0.0075 - acc: 0.994 - ETA: 4s - loss: 0.0053 - acc: 0.996 - ETA: 4s - loss: 0.0047 - acc: 0.997 - ETA: 4s - loss: 0.0044 - acc: 0.997 - ETA: 4s - loss: 0.0039 - acc: 0.998 - ETA: 4s - loss: 0.0036 - acc: 0.998 - ETA: 4s - loss: 0.0034 - acc: 0.998 - ETA: 4s - loss: 0.0034 - acc: 0.998 - ETA: 4s - loss: 0.0031 - acc: 0.998 - ETA: 4s - loss: 0.0032 - acc: 0.998 - ETA: 4s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0053 - acc: 0.998 - ETA: 4s - loss: 0.0051 - acc: 0.998 - ETA: 4s - loss: 0.0054 - acc: 0.998 - ETA: 4s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0092 - acc: 0.996 - ETA: 3s - loss: 0.0090 - acc: 0.996 - ETA: 3s - loss: 0.0087 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.997 - ETA: 3s - loss: 0.0084 - acc: 0.997 - ETA: 3s - loss: 0.0082 - acc: 0.997 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0075 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0073 - acc: 0.997 - ETA: 3s - loss: 0.0070 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0074 - acc: 0.998 - ETA: 2s - loss: 0.0073 - acc: 0.998 - ETA: 2s - loss: 0.0072 - acc: 0.998 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0075 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.998 - ETA: 2s - loss: 0.0073 - acc: 0.998 - ETA: 2s - loss: 0.0072 - acc: 0.998 - ETA: 2s - loss: 0.0080 - acc: 0.997 - ETA: 2s - loss: 0.0078 - acc: 0.997 - ETA: 2s - loss: 0.0078 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0091 - acc: 0.998 - ETA: 1s - loss: 0.0090 - acc: 0.998 - ETA: 1s - loss: 0.0089 - acc: 0.998 - ETA: 1s - loss: 0.0088 - acc: 0.998 - ETA: 1s - loss: 0.0101 - acc: 0.998 - ETA: 1s - loss: 0.0101 - acc: 0.998 - ETA: 1s - loss: 0.0100 - acc: 0.998 - ETA: 1s - loss: 0.0100 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0097 - acc: 0.998 - ETA: 0s - loss: 0.0095 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0090 - acc: 0.998 - ETA: 0s - loss: 0.0089 - acc: 0.9982Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0089 - acc: 0.9982 - val_loss: 0.8443 - val_acc: 0.8311\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 12s - loss: 0.0013 - acc: 1.00 - ETA: 5s - loss: 4.5508e-04 - acc: 1.000 - ETA: 4s - loss: 8.6070e-04 - acc: 1.000 - ETA: 4s - loss: 7.6561e-04 - acc: 1.000 - ETA: 4s - loss: 8.3730e-04 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.0000    - ETA: 4s - loss: 0.0024 - acc: 1.000 - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 4s - loss: 0.0059 - acc: 0.998 - ETA: 4s - loss: 0.0056 - acc: 0.998 - ETA: 4s - loss: 0.0052 - acc: 0.998 - ETA: 4s - loss: 0.0053 - acc: 0.998 - ETA: 4s - loss: 0.0051 - acc: 0.998 - ETA: 4s - loss: 0.0048 - acc: 0.998 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 4s - loss: 0.0047 - acc: 0.999 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 4s - loss: 0.0042 - acc: 0.999 - ETA: 4s - loss: 0.0070 - acc: 0.998 - ETA: 4s - loss: 0.0066 - acc: 0.998 - ETA: 4s - loss: 0.0075 - acc: 0.998 - ETA: 4s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 3s - loss: 0.0093 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0089 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.998 - ETA: 3s - loss: 0.0084 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0083 - acc: 0.998 - ETA: 3s - loss: 0.0080 - acc: 0.998 - ETA: 3s - loss: 0.0078 - acc: 0.998 - ETA: 3s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 2s - loss: 0.0055 - acc: 0.999 - ETA: 2s - loss: 0.0054 - acc: 0.999 - ETA: 2s - loss: 0.0053 - acc: 0.999 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0082 - acc: 0.998 - ETA: 0s - loss: 0.0082 - acc: 0.998 - ETA: 0s - loss: 0.0082 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0082 - acc: 0.9980Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0081 - acc: 0.9981 - val_loss: 0.8914 - val_acc: 0.8228\n",
      "Epoch 18/20\n",
      "6660/6680 [============================>.] - ETA: 13s - loss: 4.4889e-04 - acc: 1.00 - ETA: 11s - loss: 2.8000e-04 - acc: 1.00 - ETA: 10s - loss: 2.0690e-04 - acc: 1.00 - ETA: 6s - loss: 3.6955e-04 - acc: 1.0000 - ETA: 6s - loss: 3.3906e-04 - acc: 1.000 - ETA: 5s - loss: 0.0012 - acc: 1.0000    - ETA: 5s - loss: 0.0011 - acc: 1.000 - ETA: 5s - loss: 0.0030 - acc: 0.998 - ETA: 5s - loss: 0.0027 - acc: 0.998 - ETA: 5s - loss: 0.0028 - acc: 0.998 - ETA: 5s - loss: 0.0029 - acc: 0.998 - ETA: 5s - loss: 0.0028 - acc: 0.998 - ETA: 4s - loss: 0.0027 - acc: 0.998 - ETA: 4s - loss: 0.0026 - acc: 0.998 - ETA: 4s - loss: 0.0026 - acc: 0.999 - ETA: 4s - loss: 0.0024 - acc: 0.999 - ETA: 4s - loss: 0.0023 - acc: 0.999 - ETA: 4s - loss: 0.0023 - acc: 0.999 - ETA: 5s - loss: 0.0023 - acc: 0.999 - ETA: 5s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0046 - acc: 0.998 - ETA: 5s - loss: 0.0044 - acc: 0.998 - ETA: 5s - loss: 0.0043 - acc: 0.998 - ETA: 4s - loss: 0.0041 - acc: 0.998 - ETA: 4s - loss: 0.0066 - acc: 0.998 - ETA: 4s - loss: 0.0065 - acc: 0.998 - ETA: 4s - loss: 0.0084 - acc: 0.997 - ETA: 4s - loss: 0.0081 - acc: 0.997 - ETA: 4s - loss: 0.0078 - acc: 0.997 - ETA: 4s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0072 - acc: 0.997 - ETA: 4s - loss: 0.0070 - acc: 0.997 - ETA: 4s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.997 - ETA: 3s - loss: 0.0066 - acc: 0.997 - ETA: 3s - loss: 0.0078 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0072 - acc: 0.997 - ETA: 3s - loss: 0.0070 - acc: 0.997 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 3s - loss: 0.0065 - acc: 0.998 - ETA: 3s - loss: 0.0064 - acc: 0.998 - ETA: 3s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0056 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.9985Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0061 - acc: 0.9984 - val_loss: 0.8759 - val_acc: 0.8299\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 5.3551e-04 - acc: 1.000 - ETA: 5s - loss: 0.0012 - acc: 1.0000    - ETA: 5s - loss: 0.0013 - acc: 1.000 - ETA: 5s - loss: 0.0011 - acc: 1.000 - ETA: 5s - loss: 0.0010 - acc: 1.000 - ETA: 4s - loss: 0.0010 - acc: 1.000 - ETA: 4s - loss: 9.2518e-04 - acc: 1.000 - ETA: 5s - loss: 0.0036 - acc: 0.9980    - ETA: 5s - loss: 0.0033 - acc: 0.998 - ETA: 5s - loss: 0.0030 - acc: 0.998 - ETA: 4s - loss: 0.0026 - acc: 0.998 - ETA: 4s - loss: 0.0023 - acc: 0.998 - ETA: 4s - loss: 0.0024 - acc: 0.998 - ETA: 4s - loss: 0.0023 - acc: 0.999 - ETA: 4s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0021 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0023 - acc: 0.999 - ETA: 3s - loss: 0.0023 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0021 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0021 - acc: 0.999 - ETA: 3s - loss: 0.0021 - acc: 0.999 - ETA: 3s - loss: 0.0025 - acc: 0.999 - ETA: 3s - loss: 0.0025 - acc: 0.999 - ETA: 3s - loss: 0.0027 - acc: 0.999 - ETA: 3s - loss: 0.0041 - acc: 0.999 - ETA: 3s - loss: 0.0040 - acc: 0.999 - ETA: 3s - loss: 0.0040 - acc: 0.999 - ETA: 3s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.998 - ETA: 2s - loss: 0.0039 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0056 - acc: 0.998 - ETA: 2s - loss: 0.0056 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.9983Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.9050 - val_acc: 0.8156\n",
      "Epoch 20/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 4.4758e-04 - acc: 1.000 - ETA: 4s - loss: 3.3571e-04 - acc: 1.000 - ETA: 4s - loss: 5.5254e-04 - acc: 1.000 - ETA: 3s - loss: 4.8891e-04 - acc: 1.000 - ETA: 4s - loss: 4.3084e-04 - acc: 1.000 - ETA: 4s - loss: 3.8569e-04 - acc: 1.000 - ETA: 4s - loss: 3.5262e-04 - acc: 1.000 - ETA: 4s - loss: 3.5297e-04 - acc: 1.000 - ETA: 4s - loss: 3.9319e-04 - acc: 1.000 - ETA: 4s - loss: 3.9926e-04 - acc: 1.000 - ETA: 4s - loss: 0.0030 - acc: 0.9987    - ETA: 4s - loss: 0.0029 - acc: 0.998 - ETA: 4s - loss: 0.0038 - acc: 0.998 - ETA: 3s - loss: 0.0035 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.998 - ETA: 3s - loss: 0.0035 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.998 - ETA: 3s - loss: 0.0032 - acc: 0.998 - ETA: 3s - loss: 0.0031 - acc: 0.998 - ETA: 3s - loss: 0.0029 - acc: 0.998 - ETA: 3s - loss: 0.0028 - acc: 0.998 - ETA: 3s - loss: 0.0027 - acc: 0.998 - ETA: 3s - loss: 0.0026 - acc: 0.998 - ETA: 3s - loss: 0.0025 - acc: 0.998 - ETA: 3s - loss: 0.0024 - acc: 0.999 - ETA: 3s - loss: 0.0024 - acc: 0.999 - ETA: 3s - loss: 0.0023 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 3s - loss: 0.0022 - acc: 0.999 - ETA: 2s - loss: 0.0021 - acc: 0.999 - ETA: 2s - loss: 0.0021 - acc: 0.999 - ETA: 2s - loss: 0.0020 - acc: 0.999 - ETA: 2s - loss: 0.0027 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.999 - ETA: 2s - loss: 0.0024 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0026 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0025 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0032 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0030 - acc: 0.998 - ETA: 2s - loss: 0.0030 - acc: 0.998 - ETA: 2s - loss: 0.0030 - acc: 0.998 - ETA: 1s - loss: 0.0029 - acc: 0.998 - ETA: 1s - loss: 0.0029 - acc: 0.998 - ETA: 1s - loss: 0.0029 - acc: 0.998 - ETA: 1s - loss: 0.0028 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0032 - acc: 0.998 - ETA: 1s - loss: 0.0031 - acc: 0.998 - ETA: 1s - loss: 0.0031 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.9977Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0064 - acc: 0.9978 - val_loss: 0.9242 - val_acc: 0.8263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e6249a9908>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Resnet50_model.fit(train_DogResnet50, train_targets, \n",
    "          validation_data=(valid_DogResnet50, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "Resnet50_model.load_weights('saved_models/weights.best.ResNet50.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.9809%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "def dog_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    \n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Resnet50_model.predict(bottleneck_feature)\n",
    "    \n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "92602368/94653016 [============================>.] - ETA: 5136 - ETA: 2573 - ETA: 2571 - ETA: 2207 - ETA: 1638 - ETA: 1583 - ETA: 1361 - ETA: 1168 - ETA: 1009 - ETA: 906 - ETA: 74 - ETA: 77 - ETA: 73 - ETA: 66 - ETA: 59 - ETA: 60 - ETA: 52 - ETA: 53 - ETA: 47 - ETA: 48 - ETA: 44 - ETA: 45 - ETA: 40 - ETA: 41 - ETA: 37 - ETA: 37 - ETA: 34 - ETA: 34 - ETA: 31 - ETA: 32 - ETA: 29 - ETA: 29 - ETA: 27 - ETA: 27 - ETA: 25 - ETA: 25 - ETA: 23 - ETA: 23 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 95s - ETA: 95 - ETA: 88 - ETA: 88 - ETA: 81 - ETA: 75 - ETA: 75 - ETA: 69 - ETA: 69 - ETA: 64 - ETA: 64 - ETA: 59 - ETA: 59 - ETA: 54 - ETA: 54 - ETA: 49 - ETA: 49 - ETA: 45 - ETA: 45 - ETA: 42 - ETA: 42 - ETA: 38 - ETA: 38 - ETA: 35 - ETA: 35 - ETA: 32 - ETA: 32 - ETA: 30 - ETA: 30 - ETA: 28 - ETA: 27 - ETA: 25 - ETA: 25 - ETA: 23 - ETA: 23 - ETA: 21 - ETA: 21 - ETA: 19 - ETA: 19 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 12 - ETA: 12 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA: 0s"
     ]
    }
   ],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def dog_breed_detector(img_path):\n",
    "    breed = dog_breed(img_path) \n",
    "    \n",
    "    # Display the image\n",
    "    img = cv2.imread(img_path)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "    # Detect what it is\n",
    "    if dog_detector(img_path):\n",
    "        print(\"That's a dog. Breed: \" + str(breed))\n",
    "    elif face_detector(img_path):\n",
    "        print(\"That's a human, but it looks like a \" + str(breed))\n",
    "    else:\n",
    "        print(\"I can't detect anything!\")\n",
    "\n",
    "dog_breed_detector(train_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
