{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.1: Understanding our dataset\n",
    "--\n",
    "- Parse the data\n",
    "- Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_table('C:\\Users\\MLUSER\\Documents\\GitHub\\Udacity\\Naive Bayes Tutorial/SMSSpamCollection',\n",
    "                  sep='\\t',\n",
    "                  header=None,\n",
    "                  names=['label','sms_message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2: Data Preprocessing\n",
    "--\n",
    "   - convert labels(str) to binary var(int)\n",
    "   - 0 -> 'ham',not spam\n",
    "   - 1 -> 'spam'\n",
    "- why: beacuse scikit-learn only deal with numerical values\n",
    "\n",
    "-----\n",
    "```python\n",
    "lambda argument: manipulate(argument)\n",
    "map(function_to_apply, list_of_inputs)\n",
    "map(int, [\"12\", \"37\", \"999\"])\n",
    "[12, 37, 999]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.1: Bag of words\n",
    "--\n",
    "   - BoW concepts: take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter.\n",
    "    \n",
    "Step 2.2: Implementing Bag of Words from scratch\n",
    "--\n",
    "\n",
    "```python\n",
    "class sklearn.feature_extraction.text.CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Convert all strings to their lower case form.\n",
    "- Let's say we have a document set:\n",
    "```python\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "doc = ['Hello, how are you!',\n",
    "           'Win money, win from home.',\n",
    "           'Call me now.',\n",
    "           'Hello, Call hello you tomorrow?']\n",
    "doc_lower = []\n",
    "\n",
    "for x in range(len(doc)):\n",
    "    doc_lower.append(doc[x].lower())\n",
    "\n",
    "print doc_lower\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Removing all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello  how are you ', 'win money  win from home ', 'call me now ', 'hello  call hello you tomorrow ']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "sans_doc_punc = []\n",
    "for i in doc_lower:\n",
    "    #sans_doc_punc.append(i.translate(string.maketrans('', '', string.punctuation)))\n",
    "    sans_doc_punc.append(i.translate(string.maketrans(',!?.','    ')))\n",
    "print sans_doc_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "doc_pre = []\n",
    "for i in sans_doc_punc:\n",
    "    doc_pre.append(i.split())\n",
    "    \n",
    "print doc_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Count frequencies\n",
    "```python\n",
    "#Counter dict subclass for counting hashable objects\n",
    "#Tally occurrences of words in a list\n",
    "cnt = Counter()\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "cnt[word] += 1\n",
    "cnt\n",
    "Counter({'blue': 3, 'red': 2, 'green': 1})\n",
    "```\n",
    "https://docs.python.org/2/library/collections.html?highlight=counter\n",
    "\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'how': 1, 'you': 1, 'hello': 1, 'are': 1})\n",
      "\n",
      "\n",
      "Counter({'win': 2, 'home': 1, 'from': 1, 'money': 1})\n",
      "\n",
      "\n",
      "Counter({'me': 1, 'now': 1, 'call': 1})\n",
      "\n",
      "\n",
      "Counter({'hello': 2, 'you': 1, 'call': 1, 'tomorrow': 1})\n",
      "\n",
      "\n",
      "[Counter({'how': 1, 'you': 1, 'hello': 1, 'are': 1}), Counter({'win': 2, 'home': 1, 'from': 1, 'money': 1}), Counter({'me': 1, 'now': 1, 'call': 1}), Counter({'hello': 2, 'you': 1, 'call': 1, 'tomorrow': 1})]\n",
      "\n",
      "\n",
      "<class 'collections.Counter'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "import collections as colt\n",
    "freq_list = []\n",
    "for i in doc_pre:\n",
    "    freq_count = colt.Counter(i)\n",
    "    freq_list.append(freq_count)\n",
    "    print freq_count\n",
    "    print '\\n'\n",
    "    \n",
    "print freq_list\n",
    "print '\\n'\n",
    "print type(freq_count)\n",
    "print type(freq_list)\n",
    "\n",
    "\n",
    "#freq_count = colt.Counter()\n",
    "#for i in doc_pre:\n",
    "#    for j in i:\n",
    "#        freq_count[j] +=1\n",
    "#print freq_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.3: Implementing Bag of Words in scikit-learn\n",
    "--\n",
    "\n",
    "- Instructions: Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'.\n",
    "- Instructions: Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words which have been categorized as features using the get_feature_names() method.\n",
    "\n",
    "#### Data preprocessing with CountVectorizer()\n",
    "In Step 2.2, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:\n",
    "##### lowercase = True\n",
    "The lowercase parameter has a default value of True which converts all of our text to its lower case form.\n",
    "##### token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b\n",
    "The token_pattern parameter has a default regular expression value of (?u)\\\\b\\\\w\\\\w+\\\\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
    "##### stop_words\n",
    "The stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "print count_vector\n",
    "\n",
    "print type(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'are',\n",
       " u'call',\n",
       " u'from',\n",
       " u'hello',\n",
       " u'home',\n",
       " u'how',\n",
       " u'me',\n",
       " u'money',\n",
       " u'now',\n",
       " u'tomorrow',\n",
       " u'win',\n",
       " u'you']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "doc_array = count_vector.transform(documents).toarray()\n",
    "print doc_array\n",
    "print type(doc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frq_matrix = pd.DataFrame(doc_array,index=None,columns = count_vector.get_feature_names())\n",
    "\n",
    "frq_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1: Training and testing sets\n",
    "--\n",
    "Now that we have understood how to deal with the Bag of Words problem we can get back to our dataset and proceed with our analysis. Our first step in this regard would be to split our dataset into a training and testing set so we can test our model later.\n",
    "\n",
    "#### Instructions: Split the dataset into a training and testing set by using the train_test_split method in sklearn. Split the data using the following variables:\n",
    "- X_train is our training data for the 'sms_message' column.\n",
    "- y_train is our training data for the 'label' column\n",
    "- X_test is our testing data for the 'sms_message' column.\n",
    "- y_test is our testing data for the 'label' column Print out the number of   rows we have in each our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 5572\n",
      "Number of rows in the training set: 4179\n",
      "Number of rows in the test set: 1393\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NOTE: sklearn.cross_validation will be deprecated soon to sklearn.model_selection \n",
    "'''\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['sms_message'],df['label'],random_state=1)\n",
    "\n",
    "print'Number of rows in the total set: {}'.format(df.shape[0])\n",
    "print'Number of rows in the training set: {}'.format(x_train.shape[0])\n",
    "print'Number of rows in the test set: {}'.format(x_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(x_train)\n",
    "count_vector.fit(x_test)\n",
    "training_data = count_vector.transform(x_train).toarray()\n",
    "testing_data = count_vector.transform(x_test).toarray()\n",
    "\n",
    "print training_data\n",
    "print testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Solution\n",
    "\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.1: Bayes Theorem implementation from scratch\n",
    "--\n",
    "新訊息出現後的A概率 = A概率 x 新訊息帶來的調整\n",
    "\n",
    "條件機率表示為P（A|B），讀作「在B條件下A的機率」。\n",
    " A 與 B 為樣本空間 Ω 中的兩個事件，其中 P(B)>0。那麼在事件 B 發生的條件下，事件 A 發生的條件機率為：\n",
    " P(A|B)= P(A\\cap B)/P(B)\n",
    "條件機率有時候也稱為：後驗機率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a positive test result P(Pos) is:  0.108\n",
      "The probability of getting a disease, given that postive result P(D|Pos) is:  0.0833333333333\n",
      "The probability of no disease, given that postive result P(~D|Pos) is:  0.916666666667\n"
     ]
    }
   ],
   "source": [
    "#P(D) - disease(Known)\n",
    "p_dis = 0.01\n",
    "\n",
    "#P(~D) - well(Known)\n",
    "p_no_dis = 0.99\n",
    "\n",
    "#P(Pos|D) - disease, Pos (Known)\n",
    "p_pos_dis = 0.9\n",
    "\n",
    "#P(Neg|~D) - well, Neg (Known)\n",
    "p_neg_no_dis = 0.9\n",
    "\n",
    "#P(Pos) - Pos  = P(D)P(Pos|D) + P(~D)P(1-P(Neg|~D))\n",
    "p_pos = (p_dis * p_pos_dis) + (p_no_dis * (1 - p_neg_no_dis))\n",
    "print 'The probability of getting a positive test result P(Pos) is: '\\\n",
    ",format(p_pos)\n",
    "\n",
    "#P(D|Pos) - P(D)P(Pos|D)/P(Pos)\n",
    "p_dis_pos = (p_dis * p_pos_dis) / p_pos\n",
    "print 'The probability of getting a disease, given that postive result P(D|Pos) is: '\\\n",
    ",format(p_dis_pos)\n",
    "\n",
    "#P(Pos|~D) = 1 - P(Neg|~D) = 0.1\n",
    "p_pos_no_dis = 0.1\n",
    "\n",
    "\n",
    "#P(~D|Pos) = P(~D)P(Pos|~D)/P(Pos)\n",
    "p_no_dis_pos = (p_no_dis * p_pos_no_dis)/p_pos\n",
    "print 'The probability of no disease, given that postive result P(~D|Pos) is: '\\\n",
    ",format(p_no_dis_pos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented Bayes theorem from scratch. Your analysis shows that even if you get a positive test result, there is only a 8.3% chance that you actually have diabetes and a 91.67% chance that you do not have diabetes. This is of course assuming that only 1% of the entire population has diabetes which of course is only an assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### What does the term 'Naive' in 'Naive Bayes' mean ?\n",
    " Naive Bayes' is an extension of Bayes' theorem that assumes that all the features are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.2: Naive Bayes implementation from scratch\n",
    "--\n",
    "+ Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)\n",
    "+ Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)\n",
    "+ Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)\n",
    "+ Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)\n",
    "+ Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)\n",
    "+ Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G)\n",
    "+ Probablility of Jill Stein giving a speech, P(J) is 0.5 and the same for Gary Johnson, P(G) = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Probability of words freedom and immigration being said are: ', '0.075')\n",
      "('The probability of Jill Stein saying the words Freedom and Immigration: ', '0.0666666666667')\n",
      "('The probability of Gary Johnson saying the words Freedom and Immigration: ', '0.933333333333')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 1\n",
    "'''\n",
    "#P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "#P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "#P(F|J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "#P(I|J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "#P(E|J)\n",
    "p_j_e = 0.8\n",
    "\n",
    "#P(F|G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "#P(I|G) \n",
    "p_g_i = 0.2\n",
    "\n",
    "#P(E|G) \n",
    "p_g_e = 0.1\n",
    "\n",
    "\n",
    "'''\n",
    "Step 2\n",
    "'''\n",
    "#P(F,I|J) = P(J)P(I|J)P(F|J)\n",
    "p_j_if = p_j * p_j_i * p_j_f\n",
    "\n",
    "#P(F,I|G) = P(G)P(I|G)P(F|G)\n",
    "p_g_if = p_g * p_g_i * p_g_f\n",
    "\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "#P(F,I) = P(F,I|J) + P(F,I|G)\n",
    "p_if = p_j_if + p_g_if\n",
    "\n",
    "print('Probability of words freedom and immigration being said are: '\\\n",
    "      , format(p_if))\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "#P(J|F,I) = P(J)P(F|J)P(I|J)/P(F,I)\n",
    "p_if_j = (p_j * p_j_f * p_j_i)/p_if\n",
    "\n",
    "#P(G|F,I) = P(G)P(F|G)P(I|G)/P(F,I)\n",
    "p_if_g = (p_g * p_g_f * p_g_i)/p_if\n",
    "\n",
    "\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: '\\\n",
    "      , format(p_if_j))\n",
    "print ('The probability of Gary Johnson saying the words Freedom and Immigration: '\\\n",
    ", format(p_if_g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Naive Bayes implementation using scikit-learn\n",
    "--\n",
    "Specifically, we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Evaluating our model\n",
    "--\n",
    "- __Accuracy__ measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "- __Precision__ tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "[True Positives/(True Positives + False Positives)]\n",
    "\n",
    "- __Recall(sensitivity)__ tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the \n",
    "words that were actually spam, in other words it is the ratio of\n",
    "[True Positives/(True Positives + False Negatives)]\n",
    "\n",
    "- For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the __F1 score__ which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score: ', '0.977027997128')\n",
      "('Precision score: ', '0.880597014925')\n",
      "('Recall score: ', '0.956756756757')\n",
      "('F1 score: ', '0.917098445596')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, prediction)))\n",
    "print('Precision score: ', format(precision_score(y_test, prediction)))\n",
    "print('Recall score: ', format(recall_score(y_test, prediction)))\n",
    "print('F1 score: ', format(f1_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
